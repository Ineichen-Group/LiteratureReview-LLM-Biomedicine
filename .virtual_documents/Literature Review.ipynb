import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


import numpy as np
from webcolors import hex_to_rgb
%matplotlib inline

from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

init_notebook_mode(connected=True)  





def load_raw_data():
    df_pubmed = pd.read_excel("data/Scoping_review_text-analysis_extraction_2024.xlsx", sheet_name="Harmonized Information Extracti")
    df_pubmed['publication_type'] = "biomedical_journal"
    df_acl = pd.read_excel("data/Scoping_review_text-analysis_extraction_2024.xlsx", sheet_name="ACLEMNLP Google Scholar")
    df_acl['publication_type'] = "nlp_venue"
    return df_pubmed, df_acl


df_pubmed, df_acl = load_raw_data()


def load_raw_data_included_all():
    df_pubmed, df_acl = load_raw_data()
    df_included = df_pubmed[df_pubmed['Included'] == 1.0]
    df_acl_included = df_acl[df_acl['Included'] == 1.0]
    df = pd.concat([df_included, df_acl_included], ignore_index=True)
    df['year'] = df['year'].astype(int)
    print(df_pubmed.shape, df_included.shape, df_acl.shape, df_acl_included.shape)
    return df


df_2024 = pd.read_excel("data/Scoping_review_text-analysis_extraction_2024.xlsx", sheet_name="SED Version Sijing Extractions ")
## ALIGN WITH PREVIOUS NAMES
# Replace 'Association for Computational Linguistics' with 'ACL'
df_2024['journal'] = df_2024['journal'].str.replace('Association for Computational Linguistics', 'ACL', regex=False)
df_2024['journal'] = df_2024['journal'].str.replace('BMC bioinformatics', 'BMC Bioinformatics', regex=False)
df_2024['journal'] = df_2024['journal'].str.replace('Frontiers in psychiatry', 'Frontiers in Psychiatry', regex=False)

# Replace 'ACL Findings' followed by any characters with 'ACL/Findings'
df_2024['journal'] = df_2024['journal'].str.replace(r'ACL Findings.*', 'ACL/ Findings', regex=True)
df_2024['journal'] = df_2024['journal'].str.replace(r'Findings of the ACL.*', 'ACL/ Findings', regex=True)

df_2024['journal'] = df_2024['journal'].str.replace(r'EMNLP.*', 'EMNLP', regex=True)

df_2024['publication_type'] = np.where(
    df_2024['journal'].str.contains('EMNLP|ACL'), 
    'nlp_venue', 
    'biomedical_journal'
)


df_2024_included = df_2024[df_2024['Included'] == 1.0]
df_2024_included.loc[:, 'year'] = df_2024_included['year'].astype(int)



df_2024_included['year'].values


df_initial = load_raw_data_included_all()


# Creating sets of lowercased titles for each DataFrame
titles_df1 = set(df_initial['title'].str.lower())
titles_df2 = set(df_2024_included['title'].str.lower())

# Finding the intersection of the two sets
overlap_titles = titles_df1.intersection(titles_df2)

# Counting the number of overlaps
number_of_overlaps = len(overlap_titles)


overlap_titles


df_initial['Target application'] = df_initial['Target application'].replace('Machine Reading Coprehension', 'Machine Reading Comprehension')
df_2024_included['Target application'] = df_2024_included['Target application'].replace('Machine Reading Coprehension', 'Machine Reading Comprehension')



all_reviewed_articles = pd.concat([df_pubmed, df_acl, df_2024], ignore_index=True)


all_reviewed_articles.groupby(['publication_type']).size()


df = pd.concat([df_initial, df_2024_included], ignore_index=True)


df.shape


df.groupby("publication_type")["title"].count()


df.head(3)





# Sample 3 random rows from df_included
df_included_sampled = df_pubmed[df_pubmed['Included'] == 1.0].sample(n=3, random_state=42)

# Sample 2 random rows from df_acl_included
df_acl_included_sampled = df_acl[df_acl['Included'] == 1.0].sample(n=2, random_state=42)

# Combine the sampled DataFrames
combined_df = pd.concat([df_included_sampled, df_acl_included_sampled])
combined_df_first_cols = combined_df.iloc[:, :6]

# Display the combined DataFrame
combined_df.head()


combined_df_first_cols.to_csv("sample_to_test_new_extractor.csv")


combined_df.to_csv("full_sample_to_test_new_extractor.csv")





df_2024_included.groupby("publication_type")["title"].count()





df_bio_excluded = df_pubmed[df_pubmed['Included'] == 0]
df_acl_excluded = df_acl[df_acl['Included'] == 0]
df_2024_excluded = df_2024[df_2024['Included'] == 0]

len(df_bio_excluded), len(df_acl_excluded), len(df_2024_excluded)


df_excluded = pd.concat([df_bio_excluded, df_acl_excluded, df_2024_excluded], ignore_index=True)
df_excluded['year'] = df_excluded['year'].astype(int)


df_excluded = df_excluded[["Reason for exclusion", "year", "publication_type"]]


df_excluded.head(5)


df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'^No biomedical focus.*$', 'Different focus', regex=True)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'^Focus on.*$', 'Different focus', regex=True)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'^Not focused on model.*$', 'Different focus', regex=True)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'^Not English.*$', 'Not English', regex=True)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'Review Paper', 'Review', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'Survey', 'Review', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'No LLMs, Review', 'Review', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'Protocol, not the actual work', 'Review', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'Includes vision', 'Different focus', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'Not text', 'Different focus', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'DUPLICATE with Simona ACL papers', 'DUPLICATE', regex=False)
df_excluded["Reason for exclusion"] = df_excluded["Reason for exclusion"].str.replace(
    r'No biomedical domain', 'Different focus', regex=False)

df_excluded.groupby(['Reason for exclusion']).size().sort_values()





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)
#df = df[df['year'] != 2024]

df.head(3)


df.groupby(['publication_type']).size()


import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import pandas as pd

# Assuming df and category_counts are your DataFrames
# You would need to have these DataFrames prepared with the necessary data
label_threshold = 5
# Create a larger figure and subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))

df['year'] = df['year'].replace(2024, 2023)

# First plot
# Group by year and publication type, then count the occurrences
counts = df.groupby(['year', 'publication_type']).size().unstack(fill_value=0)

# Plot the stacked bar chart on the first subplot
bars = counts.plot(kind='bar', stacked=True, color=["#0072B2", "#E69F00"], width=0.8, ax=axes[0], zorder=2)
# Add horizontal grid lines for better readability in the second plot
axes[0].grid(axis='y', linestyle='--', alpha=0.6, zorder=1)

# Add count values as annotations for the first plot
# (This code is adjusted to work with subplots)
for i, year in enumerate(counts.index):
    for pub_type, count in zip(counts.columns, counts.loc[year]):
        if pub_type == "biomedical_journal":
            total_count = counts.sum(axis=1)[year]
            print(total_count, year)
            if total_count == 1:
                continue
            else:
                axes[0].text(i, count/2, str(count), color='black' if count < counts.sum(axis=1).max() else 'black', ha='center', va='center', fontsize=15)
        else:
            total_count = counts.sum(axis=1)[year]
            print(total_count, year)
            va = 'center' if count < 10 else 'top' if count >= label_threshold else 'bottom'
            if total_count == 1:
                continue
            else:
                axes[0].text(i, total_count - 4, str(count), color='black' if count < counts.sum(axis=1).max() else 'black', ha='center', va=va, fontsize=15)

# Add total count values on top of each bar for the first plot
for i, total_count in enumerate(counts.sum(axis=1)):
    axes[0].text(i, total_count, str(total_count), color='black', ha='center', va='bottom', fontsize=15)
    

# Customize the first plot
axes[0].set_ylim(0, 100)
axes[0].set_xticks(range(len(counts.index)))
axes[0].set_xticklabels(counts.index, rotation=360, fontsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[0].set_xlabel('Year', fontsize=15)
#axes[0].set_ylabel('Number of Articles', fontsize=15)
axes[0].set_title('Number of Included Articles over Time', fontsize=16)
axes[0].legend(['Biomedical Journal', 'NLP Venue'], fontsize=15)

# SECOND PLOT
# Get the top 10 categories and sort in descending order
category_counts = df.groupby(['journal', 'publication_type']).size()
# Modify the value with "\n" added
category_counts = category_counts.rename("count")
category_counts = category_counts.reset_index()
category_counts['journal'] = category_counts['journal'].replace({
    "Database: The Journal of Biological Databases and Curation": "Database: The Journal of\nBiological Databases and Curation",
    "BMC Medical Informatics & Decision Making": "BMC Medical Informatics &\nDecision Making",
    "Association for Computational Linguistics/ Findings": "ACL/ Findings",
    "Association for Computational Linguistics/ BioNLP": "ACL/ BioNLP",
    "Association for Computational Linguistics/ NLP-COVID19": "ACL/ NLP-COVID19",
    "AMIA Summits on Translational Science Proceedings": "AMIA Summits on Translational\nScience Proceedings",
    "AMIA ... Annual Symposium Proceedings/AMIA Symposium":"AMIA Symposium"
})
top_10_categories = category_counts.groupby(['journal', 'publication_type'])['count'].sum().reset_index()
top_10_categories = top_10_categories.sort_values(by='count', ascending=False).head(10)

# Assign colors based on the publication type
colors = ["#0072B2" if pub_type == 'biomedical_journal' else "#E69F00" for pub_type in top_10_categories['publication_type']]
colors = colors[::-1]  # Reverse the order of colors

# Create a horizontal bar chart with different colors for the second subplot
bar_plot = axes[1].barh(top_10_categories['journal'][::-1], top_10_categories['count'][::-1], color=colors, zorder=2)

# Customize the second plot
axes[1].set_xlabel('Count', fontsize=15)
#axes[1].set_ylabel('Venue/ Journal Name', fontsize=15)
axes[1].set_title('Top 10 Number of Included Articles per Source', fontsize=16)
axes[1].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed
axes[1].tick_params(axis='x', labelsize=15)  # Adjust y-axis label size

# Add count values as annotations for the second plot
for bar, count in zip(bar_plot, top_10_categories['count'][::-1]):
    width = bar.get_width()
    axes[1].text(width, bar.get_y() + bar.get_height() / 2, str(count), va='center', fontsize=15)

# Add horizontal grid lines for better readability in the second plot
axes[1].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

# Create legend handles for each category and place it outside the plot for the second plot
legend_handles = [Patch(color="#0072B2", label='Biomedical Journal'), Patch(color="#E69F00", label='NLP Venue')]
axes[1].legend(handles=legend_handles, fontsize=15, loc='lower right')

plt.tight_layout()
plt.savefig('viz/overview_years_and_journal_count_2024.pdf')
plt.show()



import pandas as pd
import matplotlib.pyplot as plt

# Create a larger figure
plt.figure(figsize=(10, 8))

# Group by year and publication type, then count the occurrences
counts = df.groupby(['year', 'publication_type']).size().unstack(fill_value=0)

# Plot the stacked bar chart
bars = counts.plot(kind='bar', stacked=True, color=['skyblue', 'darkblue'], width=0.8)

# Define a threshold for label adjustment (e.g., 5)
label_threshold = 5

# Add count values as annotations for each bar
for i, year in enumerate(counts.index):
    for pub_type, count in zip(counts.columns, counts.loc[year]):
        if pub_type == "biomedical_journal":
            plt.text(i, count/2 , str(count), color='white' if count < counts.sum(axis=1).max() else 'black', ha='center', va='center', fontsize=12)
        else:
            total_count = counts.sum(axis=1)[year]
            va = 'center' if count < 10 else 'top' if count >= label_threshold else 'bottom'
            plt.text(i, total_count - 4, str(count), color='white' if count < counts.sum(axis=1).max() else 'black', ha='center', va=va, fontsize=12)

# Add total count values on top of each bar
for i, total_count in enumerate(counts.sum(axis=1)):
    plt.text(i, total_count, str(total_count), color='black', ha='center', va='bottom', fontsize=12)
    
# Set the y-axis limit to ensure it reaches up to 45
plt.ylim(0, 100)

# Customize the plot
plt.xticks(range(len(counts.index)), counts.index, rotation=360, fontsize=14)
plt.tick_params(axis='y', labelsize=14)
plt.xlabel('Year', fontsize=15)
plt.ylabel('Number of Articles', fontsize=15)
#plt.title('Number Articles Over Publication Year', fontsize=16)
plt.legend(['Biomedical Journal', 'NLP Venue'], fontsize=12)

plt.tight_layout()
plt.savefig('viz/years_count_stacked_with_labels_2024.pdf')
# Display the chart
plt.show()



top_10_categories


import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch

# Assuming df is your DataFrame

# Count the frequency of each category
category_counts = df.groupby(['journal', 'publication_type']).size()

# Modify the value with "\n" added
category_counts = category_counts.rename("count")
category_counts = category_counts.reset_index()
category_counts['journal'] = category_counts['journal'].replace({
    "Database: The Journal of Biological Databases and Curation": "Database: The Journal of\nBiological Databases and Curation",
    "BMC Medical Informatics & Decision Making": "BMC Medical Informatics &\nDecision Making",
    "Association for Computational Linguistics/ Findings": "ACL/ Findings",
    "Association for Computational Linguistics/ BioNLP": "ACL/ BioNLP",
    "Association for Computational Linguistics/ NLP-COVID19": "ACL/ NLP-COVID19",
    "AMIA Summits on Translational Science Proceedings": "AMIA Summits on Translational\nScience Proceedings",
    "AMIA ... Annual Symposium Proceedings/AMIA Symposium":"AMIA Symposium"
})

# Get the top 10 categories and sort in descending order
top_10_categories = category_counts.groupby(['journal', 'publication_type'])['count'].sum().reset_index()
top_10_categories = top_10_categories.sort_values(by='count', ascending=False).head(10)

# Transpose the DataFrame
top_10_categories = pd.DataFrame(top_10_categories)

# Assign colors based on the publication type
colors = ['skyblue' if pub_type == 'biomedical_journal' else 'darkblue' for pub_type in top_10_categories['publication_type']]

# Reverse the order of colors to correspond with reversed bars
colors = colors[::-1]

# Create a larger figure
plt.figure(figsize=(10, 6))

# Create a horizontal bar chart with different colors for each publication type
bar_plot = plt.barh(top_10_categories['journal'][::-1], top_10_categories['count'][::-1], color=colors, zorder=2)

# Add labels and title
plt.xlabel('Count', fontsize=14)
plt.ylabel('Venue/ Journal Name', fontsize=14)
plt.title('Top 10 Number of Included Articles per Source', fontsize=16)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Add count values as annotations
for bar, count in zip(bar_plot, top_10_categories['count'][::-1]):
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, str(count), va='center', fontsize=12)

# Add horizontal grid lines for better readability
plt.grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

# Create legend handles for each category and place it outside the plot
legend_handles = [Patch(color='skyblue', label='Biomedical Journal'), Patch(color='darkblue', label='NLP Venue')]
plt.legend(handles=legend_handles, fontsize=12, loc='lower right')

plt.tick_params(axis='y', labelsize=14)
plt.tick_params(axis='x', labelsize=14)

# Save the plot to a local folder
plt.savefig('viz/top_10_journals_count_2024.pdf', bbox_inches='tight')

# Display the chart
plt.show()



top_10_categories





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)
# Replace semicolons with commas in the 'Target database' column
df['Target database'] = df['Target database'].str.replace(';', ',', regex=False)

df.head(2)


df.shape


df['Target database'] = df['Target database'].replace("PubMed articles (also financial, lifestyle, recreation, technology, writing and science data)", "PubMed")

# Create a new column to store the flattened data types
df['Flattened_Data_Types'] = df['Target database'].str.split(', ')

# Explode the flattened data types into separate rows
df_exploded = df.explode('Flattened_Data_Types')

# Replace "PubMed articles" with "PubMed"
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("PubMed articles", "PubMed/ MEDLINE")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Pubmed", "PubMed/ MEDLINE")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("PubMed", "PubMed/ MEDLINE")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("MEDLINE", "PubMed/ MEDLINE")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("MedLine", "PubMed/ MEDLINE")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Embase", "PubMed/ MEDLINE")

df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("DrugBank database", "DrugBank")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Drugbank", "EMBASE")

df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("MedRxiv", "medRxiv")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("BioRxiv", "bioRxiv")

df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("EuropePMC", "Europe PMC")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("CINAHL (Ebsco)", "CINAHL")

# Replace "electronic health records" and "Electronic Health Records" with "Clinical texts"
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace(["clinical notes", "electronic health records", "Electronic Health Records"], "Clinical texts")

# Count the frequency of each data type
data_source_counts = df_exploded['Flattened_Data_Types'].value_counts()

# Print the frequency counts
# Adjust the maximum display options to show the full list
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Print the full list of data type counts
print(data_source_counts)


import pandas as pd
import matplotlib.pyplot as plt

# Assuming df_exploded is your DataFrame with the exploded data types and data_type_counts is the counts
# If not, you can adjust accordingly based on your DataFrame structure

# Get the top 10 data types and their counts, in descending order
top_10_data_types = data_source_counts.nlargest(10)

# Reverse the order
top_10_data_types = top_10_data_types[::-1]

# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_data_types.index, top_10_data_types, color='skyblue')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('Data Sources', fontsize=15)
plt.title('Top 10 Data Sources by Frequency', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/top_10_data_types_frequency_2024.png')

# Display the chart
plt.show()



10/sum(top_10_data_types)


df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)
df['Data type'] = df['Data type'].str.replace(';', ',', regex=False)

#df.head(2)


# Create a new column to store the flattened data types
df['Flattened_Data_Types'] = df['Data type'].str.split(', ')

# Explode the flattened data types into separate rows
df_exploded = df.explode('Flattened_Data_Types')
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.title()
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Abstract", "Abstracts")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Abstract", "Abstracts")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Full-Texts", "Full-Text")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Discharge Summaries", "Patient Discharge Summaries")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Freetext Radiology Reports", "Free-Text Radiology Reports")
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].replace("Free-Text Radiology Reports", "Radiology Reports",)

# Count the frequency of each data type
data_type_counts = df_exploded['Flattened_Data_Types'].value_counts()

# Print the frequency counts
# Adjust the maximum display options to show the full list
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Print the full list of data type counts
print(data_type_counts)


import pandas as pd
import matplotlib.pyplot as plt

# Assuming df_exploded is your DataFrame with the exploded data types and data_type_counts is the counts
# If not, you can adjust accordingly based on your DataFrame structure

# Get the top 10 data types and their counts, in descending order
top_10_data_types = data_type_counts.nlargest(10)

# Reverse the order
top_10_data_types = top_10_data_types[::-1]

# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_data_types.index, top_10_data_types, color='skyblue')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('Data Types', fontsize=15)
plt.title('Top 10 Data Types by Frequency', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/top_10_data_types_details_frequency_2024.png')

# Display the chart
plt.show()



17/sum(top_10_data_types)





import pandas as pd
import matplotlib.pyplot as plt

# Assuming df_exploded is your DataFrame with the exploded data types and data_type_counts is the counts
# If not, you can adjust accordingly based on your DataFrame structure

# Get the top 10 data sources and their counts, in descending order
# This assumes you have a similar way to calculate top data sources as for data types
# Adjust this part according to your data source counts calculation
top_10_data_sources = data_source_counts.nlargest(10)

# Reverse the order for plotting
#top_10_data_types = top_10_data_types[::-1]
top_10_data_sources = top_10_data_sources[::-1]

# Create a larger figure to hold both subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))

# FIRST plot - Top 10 Data Sources by Frequency
bars = axes[0].barh(top_10_data_sources.index, top_10_data_sources, color='lightgrey',zorder=2)
# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    axes[0].text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

axes[0].tick_params(axis='y', labelsize=15)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].set_xlabel('Frequency', fontsize=15)
#axes[0].set_ylabel('Data Sources', fontsize=15)
axes[0].set_title('Top 10 Data Sources by Frequency', fontsize=16)
axes[0].set_xlim(0, 140)  # Set x-axis limit up to 125
axes[0].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

# SECOND plot - Top 10 Data Types by Frequency
bars = axes[1].barh(top_10_data_types.index, top_10_data_types, color='lightgrey',zorder=2)
# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    axes[1].text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

axes[1].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].set_xlabel('Frequency', fontsize=15)
#axes[1].set_ylabel('Data Types', fontsize=15)
axes[1].set_title('Top 10 Data Types by Frequency', fontsize=16)
axes[1].set_xlim(0, 125)  # Set x-axis limit up to 125
axes[1].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

plt.tight_layout()
# Save the combined plot to a local folder
plt.savefig('viz/top_10_data_types_and_sources_frequency_2024.pdf')
# Display the combined chart
plt.show()


import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame df_exploded
# Group by 'year' and count the occurrences of each 'Flattened_Data_Types' value
grouped = df_exploded.groupby(['year', 'Flattened_Data_Types']).size().reset_index(name='count')

# Sort the data by year and count in descending order
grouped = grouped.sort_values(by=['year', 'count'], ascending=[True, False])

# Get the top 5 most frequent values for each year
top5_values = grouped.groupby('year').head(5)

# Create a pivot table to make it easier for plotting
pivot_table = top5_values.pivot(index='year', columns='Flattened_Data_Types', values='count')

# Plot the data in a line chart
pivot_table.plot(kind='line', marker='o', figsize=(12, 6))
plt.xlabel('Year')
plt.ylabel('Count')
plt.title('Top 5 Flattened_Data_Types Over Each Year')
plt.legend(title='Flattened_Data_Types', bbox_to_anchor=(1.05, 1), loc='upper left')

plt.show()






df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)

df.head(2)


#df = df_2024_included.copy()


# Create a new column to store the flattened data types
df['Fine-tuning corpus data/task'] = df['Fine-tuning corpus data/task'].str.replace(',', ';', regex=False)
df['Flattened_Data_Types'] = df['Fine-tuning corpus data/task'].str.split('; ?')

# Explode the flattened data types into separate rows
df_exploded = df.explode('Flattened_Data_Types')
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.strip()
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"BC5CDR-Dis", "BC5CDR-disease", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"BC5CDR-Chem", "BC5CDR-chem", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"ChemProt", "CHEMPROT", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"NCBI-Dis", "NCBI-disease", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"NCBI-diseaseease", "NCBI-disease", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"TREC-Covid", "TREC-COVID", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"HOC dataset", "HoC", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"SCIERC", "SciERC", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"GAD-10", "GAD", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"AGAC corpus", "AGAC", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"BioCreative-LitCovid", "LitCovid", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"Genia Event 2013", "GENIA Event 2013", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"THYME(?! Clinical Notes)", "THYME Clinical Notes", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"ChemDNER", "CHEMDNER", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"DementiaBank English Pitt Corpus", "DementiaBank Pitt Corpus", regex=True)
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"DrugBank DB", "DrugBank", regex=True)


# Convert the Flattened_Data_Types column to strings
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].astype(str)

# Filter out rows containing 'custom' or 'self' in the Flattened_Data_Types column
df_exploded = df_exploded[~df_exploded['Flattened_Data_Types'].str.contains(r'custom|self', case=False)]


#df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].str.replace(r"BC5CDR.*", "BC5CDR", regex=True)


# Count the frequency of each data type
data_type_counts = df_exploded['Flattened_Data_Types'].value_counts()

# Print the frequency counts
# Adjust the maximum display options to show the full list
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Print the full list of data type counts
data_type_counts





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['Fine-tuning corpus data/task'] = df['Fine-tuning corpus data/task'].str.replace(',', ';', regex=False)
df['Flattened_Data_Types'] = df['Fine-tuning corpus data/task'].str.split('; ?')
df_exploded = df.explode('Flattened_Data_Types')
df_exploded['Flattened_Data_Types'] = df_exploded['Flattened_Data_Types'].astype(str)



df_exploded.head()


selected_rows = df_exploded[df_exploded["Flattened_Data_Types"].str.contains("custom|self", case=False, na=False)]
custom_datasets = selected_rows[['title', 'Domain of automated approach', 'journal', 'Data type', 'Target application', 'year', 'Data used availability', 'Flattened_Data_Types', 'publication_type', 'New annotations']]
#custom_datasets[['Biomedical Domain', 'Domain Details']] = custom_datasets['Domain of automated approach'].str.split(':', 1, expand=True)
#df[['Biomedical Domain', 'Domain Details']] = df['Domain of automated approach'].str.split(':', 1, expand=True)

#custom_datasets[['Biomedical Domain Main', 'Biomedical Domain Additional']] = custom_datasets['Biomedical Domain'].str.split('/', 1, expand=True)
selected_rows.shape, custom_datasets.shape


custom_datasets_classification = custom_datasets[custom_datasets["Target application"].str.contains("classification", case=False, na=False)]
custom_datasets_classification.sort_values(by="Flattened_Data_Types")


custom_datasets_classification.shape


custom_datasets['Target application'].value_counts()



sum(custom_datasets['Target application'].value_counts())/196


# Assuming df_exploded is your DataFrame with the exploded data types and data_type_counts is the counts
# If not, you can adjust accordingly based on your DataFrame structure

# Get the top 10 data types and their counts, in descending order
top_10_data_types = custom_datasets['Target application'].value_counts()

# Reverse the order
top_10_data_types = top_10_data_types[::-1]

# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_data_types.index, top_10_data_types, color='lightgrey', zorder=2)

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
#plt.ylabel('NLP Task', fontsize=15)
plt.grid(axis='x', linestyle='--', alpha=0.6, zorder=1)
plt.title('Custom Datasets Applications', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/custom_dataset_nlp_task_2024.pdf')

# Display the chart
plt.show()


category_counts


import pandas as pd
import matplotlib.pyplot as plt

# Assuming custom_datasets is defined and loaded
# Count the frequency of each category
category_counts = custom_datasets.groupby(['Target application', 'publication_type']).size()
category_counts = category_counts.rename("count")
category_counts = category_counts.reset_index()

# Pivot the data for a stacked bar chart
pivoted_data = category_counts.pivot(index='Target application', columns='publication_type', values='count')

# Fill NaN with 0 for stacking
pivoted_data = pivoted_data.fillna(0)

# Sort by the total count of each category
pivoted_data['total'] = pivoted_data.sum(axis=1)
pivoted_data = pivoted_data.sort_values('total', ascending=False)

# Store the sorted target application names for later use
sorted_target_applications = pivoted_data.index.tolist()

# Drop the total column used for sorting
pivoted_data = pivoted_data.drop(columns='total')

# Create a larger figure
plt.figure(figsize=(10, 7))

# Plotting stacked horizontal bars
bar_plot = pivoted_data.plot(kind='barh', stacked=True, color=['skyblue', 'darkgrey'], ax=plt.gca())

# Reverse the order of the y-axis to match the sorted categories
plt.gca().invert_yaxis()

# Add labels and title
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('', fontsize=15)
plt.title('Custom Datasets Frequency Developed per NLP Task', fontsize=16)
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)

# Add count values as annotations in the center of each bar segment
for i, bar in enumerate(bar_plot.containers):
    for rect in bar:
        # Calculate width and center position for the annotation
        width = rect.get_width()
        center = rect.get_x() + width / 2
        if width > 0:  # Only annotate non-zero values
            plt.text(center, rect.get_y() + rect.get_height() / 2, int(width), ha='center', va='center', color='black', fontsize=12)

# Add color legend
plt.legend(['Biomedical Journal', 'NLP Venue'], fontsize=15)

# Save and show the plot
plt.savefig('viz/custom_dataset_nlp_task_stacked_2024.pdf', bbox_inches='tight')
plt.show()



custom_datasets['Data type'].value_counts()



custom_datasets['Biomedical Domain Main'].value_counts()



custom_datasets['publication_type'].value_counts()



41/bio_papers_count, 16/nlp_papers_count


custom_datasets['Data used availability'].value_counts()



len(custom_datasets_classification)


custom_datasets_retrieval = custom_datasets[custom_datasets["Target application"].str.contains("retrieval", case=False, na=False)]
custom_datasets_retrieval.sort_values(by="Flattened_Data_Types")


custom_datasets_extraction = custom_datasets[custom_datasets["Target application"].str.contains("extraction", case=False, na=False)]
custom_datasets_extraction.sort_values(by="Flattened_Data_Types")



sorted_dataframe = custom_datasets_classification.sort_values(by="Flattened_Data_Types")

# Print the complete text content of "New annotations" column for the first row
print(sorted_dataframe['New annotations'].iloc[9])



custom_datasets['Flattened_Data_Types'].value_counts()





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)


df[['Biomedical Domain', 'Domain Details']] = df['Domain of automated approach'].str.split(':', n=1, expand=True)
replace_values = {
    'Literature-Based Discovery': 'Knowledge Management/Literature-Based Discovery',
    'Literature Screening': 'Knowledge Management/Literature Screening',
    'Biomedical Literature Curation': 'Knowledge Management/Literature Curation',
    'General biomedical text mining': 'General Biomedical Text Mining'
}
df['Biomedical Domain'] = df['Biomedical Domain'].replace(replace_values, regex=True)


df[['Biomedical Domain Level 1', 'Biomedical Domain Level 2']] = df['Biomedical Domain'].str.split('/', n=1, expand=True)
df[['Biomedical Domain Level 2', 'Biomedical Domain Level 3']] = df['Biomedical Domain Level 2'].str.split('/', n=1, expand=True)


replace_values = {
    'Pharma': 'Pharmacology',
    'Evidence-Based Medicine': 'Evidence Synthesis',
    'Biological Functions': 'Biology'
}
df['Biomedical Domain Level 1'] = df['Biomedical Domain Level 1'].replace(replace_values, regex=True)


df[['Biomedical Domain', 'Biomedical Domain Level 1', 'Biomedical Domain Level 2', 'Biomedical Domain Level 3']].head(5)


import textwrap


# Count the frequency of each category
#df['Biomedical Domain Main'] = df['Biomedical Domain Main'].replace("General biomedical text mining", "General biomedical \ntext mining")
#df['Biomedical Domain Main'] = df['Biomedical Domain Main'].replace("Biomedicla Literature Curation", "Biomedicla Literature \nCuration")
#df['Biomedical Domain Main'] = df['Biomedical Domain Main'].replace("Literature-Based Discovery", "Literature-Based \nDiscovery")

category_counts = df['Biomedical Domain Level 1'].value_counts()
top_n = 11

# Get the top 10 categories
top_10_categories = category_counts#.head(top_n) SHOW ALL!

# Sort the categories in descending order
top_10_categories = top_10_categories.sort_values(ascending=True)

# Transpose the DataFrame
top_10_categories = top_10_categories.transpose()


# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_categories.index, top_10_categories, color='lightgrey')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('Biomedical Domain', fontsize=15)
plt.title('Number of Articles per Domain', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/number_of_papers_domain_count_2024.pdf'.format(top_n))

# Display the chart
plt.show()



sum(category_counts)


filtered_df = df[(df['Biomedical Domain Level 1'] == 'Knowledge Management') | (df['Biomedical Domain Level 1'] == 'Pharmacology') | (df['Biomedical Domain Level 1'] == 'Media for Health Care')| (df['Biomedical Domain Level 1'] == 'Clinical')]
filtered_df = filtered_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
filtered_df['Biomedical Domain Level 2'].fillna("missing", inplace=True)


filtered_df[['doi','Biomedical Domain Level 1', 'Biomedical Domain Level 2']]


import plotly.express as px

# Create a sunburst chart
fig = px.sunburst(filtered_df, path=['Biomedical Domain Level 1', 'Biomedical Domain Level 2'])

# Update layout for better readability
fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))

# Show the chart
fig.show()



from collections import defaultdict

category_counts = defaultdict(lambda: defaultdict(int))

# Iterate over the DataFrame rows
for _, row in filtered_df.iterrows():
    # Increment the count of the (Level 1, Level 2) pair
    category_counts[row['Biomedical Domain Level 1']][row['Biomedical Domain Level 2']] += 1

# Convert the defaultdict to a regular dict and sort the subcategories by count, descending
categories = {lvl1: sorted(lvl2.items(), key=lambda item: item[1], reverse=True)
               for lvl1, lvl2 in category_counts.items()}


import matplotlib.pyplot as plt

# Define colors for each main category
category_colors = {'Knowledge Management': 'gray', 'Pharmacology': 'steelblue', 'Clinical': 'mediumaquamarine', 'Media for Health Care': 'orange'}

# Create the figure and the axes with increased figure size
fig, ax = plt.subplots(figsize=(12, 10))

# Combine all subcategories and their values into a single list
all_items = [(subcat, value) for items in categories.values() for subcat, value in items]

# Sort the items by value in ascending order
sorted_items = sorted(all_items, key=lambda x: x[1], reverse=True)

# Loop over the sorted items in reverse order to create the stacked bar chart.
for subcat, value in reversed(sorted_items):
    # Find the category for the current subcategory
    for category, items in categories.items():
        if (subcat, value) in items:
            # Plot the horizontal bar with the corresponding value and color
            bar = ax.barh(subcat, value, color=category_colors[category])
            # Display the value at the end of each bar using the bar width
            bar_width = bar[0].get_width()
            # Use the bar's y-coordinate for text placement
            bar_y = bar[0].get_y() + bar[0].get_height() / 2
            ax.text(bar_width, bar_y, str(value), va='center', color='black', fontsize=15)  # Set the text size to 15

# Set the title of the chart with font size
ax.set_title('Biomedical Application Subdomains', fontsize=15)

# Add a legend for the colors
handles = [plt.Rectangle((0, 0), 1, 1, color=color, label=category) for category, color in category_colors.items()]
ax.legend(handles=handles, loc='lower right', fontsize=15)  # Set the legend font size to 15

# Set the x-axis label
ax.set_xlabel('Frequency', fontsize=15)  # Set the x-axis label and font size

# Show the x-axis labels and title
ax.xaxis.set_visible(True)

# Set the font size for x-axis and y-axis labels
ax.tick_params(axis='both', labelsize=15)

plt.tight_layout()  # Adjust subplot spacing to avoid cutting off labels

plt.savefig('viz/number_of_papers_subdomain_count_2024.pdf'.format(top_n))

# Show the plot
plt.show()






#df = pd.concat([df_initial, df_2024_included], ignore_index=True)
#df['year'] = df['year'].astype(int)


df['Target application'].value_counts()


category_counts = df['Target application'].value_counts()
top_n = len(df['Target application'].value_counts()) # interested in all categories

# Get the top 10 categories
top_10_categories = category_counts#.head(top_n)

# Sort the categories in descending order
top_10_categories = top_10_categories.sort_values(ascending=True)

# Transpose the DataFrame
top_10_categories = top_10_categories.transpose()


# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_categories.index, top_10_categories, color='skyblue')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('NLP Task', fontsize=15)
plt.title('Number of Articles per NLP task', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/target_NLP_application_count_2024.pdf'.format(top_n))

# Display the chart
plt.show()





import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame with information on biomedical domains and target NLP applications

# Get the top counts for Biomedical Domain Level 1 and Target application
top_biomedical_domains = df['Biomedical Domain Level 1'].value_counts()
top_nlp_tasks = df['Target application'].value_counts()

# Reverse the order for plotting to have the largest counts at the top of the plot
top_biomedical_domains_reversed = top_biomedical_domains[::-1]
top_nlp_tasks_reversed = top_nlp_tasks[::-1]

# Create a larger figure to hold both subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))

# Plot for Top Biomedical Domains by Frequency
bars_domains = axes[0].barh(top_biomedical_domains_reversed.index, top_biomedical_domains_reversed, color='lightgrey', zorder=2)
# Add labels to each bar in the domains plot
for bar in bars_domains:
    width = bar.get_width()
    axes[0].text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

axes[0].tick_params(axis='y', labelsize=15)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].set_xlabel('Frequency', fontsize=15)
axes[0].set_title('Biomedical Domains by Frequency', fontsize=16)
axes[0].set_xlim(0, 65)  # Adjust the x-axis limit if needed
axes[0].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

# Plot for Top NLP Tasks by Frequency
bars_tasks = axes[1].barh(top_nlp_tasks_reversed.index, top_nlp_tasks_reversed, color='lightgrey',zorder=2)
# Add labels to each bar in the NLP tasks plot
for bar in bars_tasks:
    width = bar.get_width()
    axes[1].text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

axes[1].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].set_xlabel('Frequency', fontsize=15)
axes[1].set_title('NLP Tasks by Frequency', fontsize=16)
axes[1].set_xlim(0, 60)  # Adjust the x-axis limit if needed
axes[1].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

plt.tight_layout()
# Save the combined plot to a local folder
plt.savefig('viz/domains_and_nlp_tasks_frequency_2024.pdf')
# Display the combined chart
plt.show()






#%pip install pySankey


#%pip install webcolors


# Group by 'Biomedical Domain Level 1' and 'Target application' and count
count_df = df.groupby(['Biomedical Domain Level 1', 'Target application']).size().reset_index(name='Sum of Included')

# Sort the DataFrame based on the count in descending order
count_df = count_df.sort_values(by='Sum of Included', ascending=False).reset_index(drop=True)
count_df.head()


top_5_domains = df['Biomedical Domain Level 1'].value_counts().head(5).index.tolist()
top_5_domains


filtered_count_df = count_df[count_df["Biomedical Domain Level 1"].isin(top_5_domains)]



l1 = filtered_count_df["Biomedical Domain Level 1"].to_list()
l2 = filtered_count_df["Target application"].to_list()
connect_values = count_df["Sum of Included"].to_list()
levels_combined = list(set(l1+l2))
levels_combined


#node_label = ["A1", "A2", "B1", "B2","B3", "C1", "C2"]
node_dict = {y:x for x, y in enumerate(levels_combined)}
node_dict


source = l1 #['A1','A1','A1','A2','A2','A2','B1','B2','B2','B3','B3']
target = l2 #['B1','B2','B3','B1','B2','B3','C1','C1','C2','C1','C2'] 
values = connect_values #[ 10, 5, 15, 5, 20, 45, 15, 20, 5, 30, 30 ]


source_node = [node_dict[x] for x in source]
target_node = [node_dict[x] for x in target]


#source_node


import plotly.graph_objects as go # Import the graphical object

fig = go.Figure( 
    data=[go.Sankey( # The plot we are interest
        # This part is for the node information
        node = dict( 
            label = levels_combined
        ),
        # This part is for the link information
        link = dict(
            source = source_node,
            target = target_node,
            value = values
        ))])

# With this save the plots 
plot(fig,
     image_filename='sankey_plot_1', 
     image='png', 
     image_width=900, 
     image_height=600
)
# And shows the plot
fig.show()





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)

# Create a new column to store the flattened data types
df['Flattened_Models'] = df['LLM Model'].str.split(', ')

# Explode the flattened data types into separate rows
df_exploded = df.explode('Flattened_Models')
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace(r'(?i).*gpt.*', 'GPT', regex=True)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace(r'(?i).*t5.*', 'T5', regex=True)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace('Bio+Clinical BERT', 'BioClinicalBERT', regex=False)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace('Bio+ClinicalBERT', 'BioClinicalBERT', regex=False)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace('BioCinicalBERT', 'BioClinicalBERT', regex=False)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace('Sci-BERT uncased version', 'SciBERT', regex=False)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace('BioBERT (base and large)', 'BioBERT', regex=False)
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.replace('AlBERT', 'ALBERT', regex=False)


# Remove leading and trailing whitespaces from the 'Flattened_Models' column
df_exploded['Flattened_Models'] = df_exploded['Flattened_Models'].str.strip()

# Count the frequency of each data type
data_type_counts = df_exploded['Flattened_Models'].value_counts()

# Print the frequency counts
# Adjust the maximum display options to show the full list
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Print the full list of data type counts
print(data_type_counts)


sum(data_type_counts)


#df_year_model = df_exploded[["year", "Flattened_Models"]]
df_year_model = df_exploded[df_exploded['year'] != 2024][["year", "Flattened_Models"]]

# Group by year and model, and count occurrences
year_model_counts = df_year_model.groupby(['year', 'Flattened_Models']).size().reset_index(name='counts')

# For each year, find the top 5 models
top5_per_year = year_model_counts.groupby('year').apply(lambda x: x.nlargest(5, 'counts')).reset_index(drop=True)

# Pivot this data for plotting
pivot_data = top5_per_year.pivot(index='year', columns='Flattened_Models', values='counts').fillna(0)
pivot_data.head()


pastel_palette = sns.color_palette("Spectral", len(pivot_data.columns))

plt.figure(figsize=(12, 8))
ax = pivot_data.plot(kind='bar', stacked=True, figsize=(10, 6), color=pastel_palette)

# Adding data labels inside the bars
for rect in ax.patches:
    # Find where to place the label
    y_value = rect.get_y() + rect.get_height() / 2
    x_value = rect.get_x() + rect.get_width() / 2

    # Get the value to label
    value = int(rect.get_height())

    # Only label bars with a value
    if value > 0:
        ax.annotate(value, (x_value, y_value), ha='center', va='center', fontsize=12)

# Adding total label on top of each bar
for i, total in enumerate(pivot_data.sum(axis=1)):
    ax.text(i, total, str(int(total)), ha='center', va='bottom', fontsize=12)
    
# Adding a dashed line in the background
ax.axhline(y=0, color='lightgrey', linestyle='--', zorder=1)

plt.title("Top 5 Models per Year (Stacked Bar Chart)",fontsize=15)
plt.xlabel("Year", fontsize=15)
plt.ylabel("Count", fontsize=15)
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xticks(rotation=45)
plt.legend(title="Models", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)
plt.tight_layout()
plt.savefig('viz/LLM_models_top5_per_year_2024.pdf')
plt.show()


# Get the top 10 categories
top_10_categories = data_type_counts.head(10)

# Sort the categories in descending order
top_10_categories = top_10_categories.sort_values(ascending=True)

# Transpose the DataFrame
top_10_categories = top_10_categories.transpose()

# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_categories.index, top_10_categories, color='lightgrey')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('LLM Model Name', fontsize=15)
plt.title('Frequency of Models Use', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/LLM_models_top_10_2024.pdf')

# Display the chart
plt.show()


top_10_categories


sum(top_10_categories)


(302-27)/302


302-27





# Define the extended Okabe-Ito palette with the additional Light Lavender Purple
custom_colors = [
    "#E69F00",  # Orange
    "#56B4E9",  # Sky Blue
    "#009E73",  # Bluish Green
    "#F0E442",  # Yellow
    "#0072B2",  # Blue
    "#D55E00",  # Vermilion
    "#CC79A7",  # Reddish Purple
    "#808080",  # Black
    "#008080"   # teal
]

# Convert the custom colors list to a seaborn palette
okabe_ito_palette = sns.color_palette(custom_colors)
okabe_ito_palette


import pandas as pd
import matplotlib.pyplot as plt

# Create a larger figure to hold both subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))

# FIRST plot 
top_10_categories = data_type_counts.head(10)

# Sort the categories in descending order
top_10_categories = top_10_categories.sort_values(ascending=True)

# Transpose the DataFrame
top_10_categories = top_10_categories.transpose()

# Create a horizontal bar chart for the top 10 data types
#plt.figure(figsize=(10, 6))
bars = axes[0].barh(top_10_categories.index, top_10_categories, color='lightgrey',zorder=2)

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    axes[0].text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

axes[0].tick_params(axis='y', labelsize=15)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].set_xlabel('Frequency', fontsize=15)
#axes[0].set_ylabel('Data Sources', fontsize=15)
axes[0].set_title('Top 10 LLMs by Frequency', fontsize=16)
axes[0].set_xlim(0, 100)  # Set x-axis limit up to 125
axes[0].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)


# SECOND plot - Stacked Bar Chart for Model Frequency over Years
#colors = plt.cm.Pastel1(np.linspace(0, 1, len(models)))
pivot_data.plot(kind='bar', stacked=True, ax=axes[1], color=okabe_ito_palette, figsize=(15, 6),zorder=2)

axes[1].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15, labelrotation=0)
axes[1].set_title('Model Frequency Over Years', fontsize=16)
axes[1].set_xlabel('Year', fontsize=15)

# Adding data labels inside the bars
for p in axes[1].patches:
    width, height = p.get_width(), p.get_height()
    if height > 0:
        axes[1].annotate(f'{int(height)}', (p.get_x() + width/2, p.get_y() + height/2), ha='center', va='center', fontsize=10)

# Adding total label on top of each bar
for i, total in enumerate(pivot_data.sum(axis=1)):
    axes[1].text(i, total, str(int(total)), ha='center', va='bottom', fontsize=12)

# Reverse the order of legend items and change the legend title
handles, labels = axes[1].get_legend_handles_labels()
axes[1].legend(handles[::-1], labels[::-1], title='Model Names', fontsize=10)
axes[1].grid(axis='y', linestyle='--', alpha=0.6, zorder=1)

plt.tight_layout()
# Save the combined plot to a local folder
plt.savefig('viz/LLM_models_and_time_trend_2024.pdf')
# Display the combined chart
plt.show()


import pandas as pd
import matplotlib.pyplot as plt

# Create a larger figure to hold both subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))

# FIRST plot 
top_10_categories = data_type_counts.head(10)

# Sort the categories in descending order
top_10_categories = top_10_categories.sort_values(ascending=True)

# Transpose the DataFrame
top_10_categories = top_10_categories.transpose()

# Create a horizontal bar chart for the top 10 data types
#plt.figure(figsize=(10, 6))
bars = axes[0].barh(top_10_categories.index, top_10_categories, color='skyblue')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    axes[0].text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

axes[0].tick_params(axis='y', labelsize=15)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].set_xlabel('Frequency', fontsize=15)
#axes[0].set_ylabel('Data Sources', fontsize=15)
axes[0].set_title('Top 10 LLMs by Frequency', fontsize=16)
axes[0].set_xlim(0, 100)  # Set x-axis limit up to 125

# SECOND plot - Normalized Stacked Bar Chart for Model Frequency over Years
# Normalize the pivot_data DataFrame to reflect percentages rather than absolute counts
pivot_data_normalized = pivot_data.div(pivot_data.sum(axis=1), axis=0) * 100

pivot_data_normalized.plot(kind='bar', stacked=True, ax=axes[1], color=pastel_palette, figsize=(15, 6))
axes[1].set_title('Model Frequency Over Years (Normalized)', fontsize=16)
axes[1].set_xlabel('Year', fontsize=15)
axes[1].set_ylabel('Proportion (%)', fontsize=15)
axes[1].tick_params(axis='x', labelrotation=0)  # Making x-axis labels horizontal

# Corrected approach for adding percentage labels
for i, (index, row) in enumerate(pivot_data_normalized.iterrows()):
    cumulative_height = 0
    for model in row.index:
        percentage = row[model]
        if percentage > 0:  # Only add labels to non-zero segments
            x = i
            y = cumulative_height + percentage / 2  # Position at the middle of the segment
            label = f'{percentage:.0f}%' if percentage > 5 else ''  # Only label if segment is large enough
            axes[1].annotate(label, (x, y), ha='center', va='center', fontsize=10, rotation=0)
            cumulative_height += percentage

plt.tight_layout()
# Save the combined plot to a local folder
#plt.savefig('viz/LLM_models_and_time_trend_2024.pdf')
# Display the combined chart
plt.show()





df_year_model_list = df[df['year'] != 2024][["year", "Flattened_Models"]]
#df_year_model = df_exploded[df_exploded['year'] != 2024][["year", "Flattened_Models"]]



import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt


# Convert the "year" column to integers
df_year_model_list['year'] = df_year_model_list['year'].astype(int)

# Calculate the number of models used by each paper
df_year_model_list['Num_Models_Used'] = df_year_model_list['Flattened_Models'].apply(lambda x: len(x) if isinstance(x, list) else 0)

# Calculate the average number of models for each year
yearly_avg = df_year_model_list.groupby('year')['Num_Models_Used'].mean()

# Calculate the confidence intervals for the averages
yearly_std = df_year_model_list.groupby('year')['Num_Models_Used'].std()
confidence_interval = stats.norm.interval(0.95, loc=yearly_avg, scale=yearly_std / np.sqrt(df_year_model_list['year'].nunique()))

# Create a DataFrame for the results
result_df = pd.DataFrame({
    "Year": yearly_avg.index,
    "Average_Num_Models": yearly_avg.values,
    "Lower_CI": confidence_interval[0],
    "Upper_CI": confidence_interval[1]
})

# Create a line graph with confidence intervals
plt.figure(figsize=(9, 6))
plt.plot(result_df['Year'], result_df['Average_Num_Models'], marker='o', label='Average Number of Models', color='b')
plt.fill_between(result_df['Year'], result_df['Lower_CI'], result_df['Upper_CI'], color='grey', alpha=0.2, label='95% Confidence Interval')

plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xticks(result_df['Year'])  # Set the x-axis ticks to match the "Year" values
#plt.title("Mean Number of Models Used by Papers Published Each Year", fontsize=15)
plt.xlabel("Year", fontsize=15)
plt.ylabel("Mean (+/- std)", fontsize=15)
plt.legend(fontsize=12)
plt.grid(axis='y')  # Enable grid only on the y-axis

# Annotate each data point with rounded values
for i, row in result_df.iterrows():
    plt.text(row['Year'], round(row['Average_Num_Models'], 2) + 0.05, round(row['Average_Num_Models'], 2), ha='center', va='bottom', fontsize=15)
    
plt.savefig('viz/LLM_number_per_paper_over_time_2024.pdf'.format(top_n))

plt.tight_layout()
plt.show()


result_df





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)


df_technical_setup = df[["Library/Framework Harmonized", "Hardware type", "Programming language" ,"publication_type"]]


df_technical_setup.head()


nan_count = df_technical_setup.isna().sum()

# Print the results
print(nan_count)



nan_counts = df_technical_setup.groupby('publication_type').apply(lambda x: x.isna().sum())
nan_counts


# Get the count of unique values in the "Hardware type" column
hardware_type_counts = df_technical_setup['Hardware type'].value_counts()
hardware_type_counts = hardware_type_counts.sort_values(ascending=True)

# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(hardware_type_counts.index, hardware_type_counts, color='lightgrey')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('Hardware Type', fontsize=15)
plt.title('Frequency of Reporting', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/technical_hardware_used_2024.pdf')

# Display the chart
plt.show()



hardware_type_counts


72/sum(hardware_type_counts)


import pandas as pd
import matplotlib.pyplot as plt

# Split the values in the "Programming language" column by ","
df_technical_setup['Programming language'] = df_technical_setup['Programming language'].str.split(', ')

# Explode the list into separate rows
df_technical_setup = df_technical_setup.explode('Programming language')

# Get the count of unique values in the "Programming language" column
programming_language_counts = df_technical_setup['Programming language'].value_counts()
programming_language_counts = programming_language_counts.sort_values(ascending=True)

# Create a horizontal bar chart for the counts
plt.figure(figsize=(10, 6))
bars = plt.barh(programming_language_counts.index, programming_language_counts, color='lightgrey')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)

plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('Programming language', fontsize=15)
plt.title('Reported Programming Language Count', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/technical_programming_language_count_2024.pdf')

# Display the chart
plt.show()



programming_language_counts


157/sum(programming_language_counts)


# Create a new column to store the flattened data types
df_technical_setup['Flattened_Libraries'] = df_technical_setup['Library/Framework Harmonized'].str.split(', ')

# Explode the flattened data types into separate rows
df_exploded = df_technical_setup.explode('Flattened_Libraries')
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].str.lower()

df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("nltk", "NLTK")
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("transformers", "HuggingFace")
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("transformer", "HuggingFace")
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("huggingface", "HuggingFace")
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("tensorflow", "TensorFlow")
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("pytorch", "PyTorch")
df_exploded['Flattened_Libraries'] = df_exploded['Flattened_Libraries'].replace("stanford corenlp", "CoreNLP ")
 

# Count the frequency of each data type
library_counts_flat = df_exploded['Flattened_Libraries'].value_counts()


# Get the top 10 categories
top_10_libraries = library_counts_flat.head(10)

# Sort the categories in descending order
top_10_libraries = top_10_libraries.sort_values(ascending=True)
top_10_libraries


30/sum(top_10_libraries)


#library_counts = df['Library/Framework Harmonized'].value_counts()
#library_counts = library_counts.sort_values(ascending=True)

# Create a horizontal bar chart for the top 10 data types
plt.figure(figsize=(10, 6))
bars = plt.barh(top_10_libraries.index, top_10_libraries, color='skyblue')

# Add labels to each bar
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width}', va='center', fontsize=12)
    
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=15)
plt.xlabel('Frequency', fontsize=15)
plt.ylabel('Code Library', fontsize=15)
plt.title('Reported Library Usage Count', fontsize=15)
plt.tight_layout()

# Save the plot to a local folder
plt.savefig('viz/technical_code_library_count_2024.pdf')

# Display the chart
plt.show()


library_counts_flat


top_10_libraries.replace('Stanford CoreNLP', 'CoreNLP')





import matplotlib.pyplot as plt
import pandas as pd

# Assuming df_technical_setup and top_10_libraries are already defined with the necessary data

# Setup for subplot arrangement
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))

# Plot for "Hardware type"
axes[0].barh(hardware_type_counts.index, hardware_type_counts, color='lightgrey',zorder=2)
for i, v in enumerate(hardware_type_counts):
    axes[0].text(v, i, f"{v}", va='center', fontsize=12)

# Plot for "Programming language"
axes[1].barh(programming_language_counts.index, programming_language_counts, color='lightgrey',zorder=2)
for i, v in enumerate(programming_language_counts):
    axes[1].text(v, i, f"{v}", va='center', fontsize=12)

# Plot for "Library/Framework Harmonized"
# Assuming top_10_libraries data is prepared similarly to the previous examples
axes[2].barh(top_10_libraries.index, top_10_libraries, color='lightgrey',zorder=2)
for i, v in enumerate(top_10_libraries):
    axes[2].text(v, i, f"{v}", va='center', fontsize=12)

# Setting labels and titles for each subplot
axes[0].set_xlabel('Frequency', fontsize=15)
axes[0].set_title('Hardware Type Reporting', fontsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

axes[1].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].set_xlabel('Frequency', fontsize=15)
axes[1].set_title('Programming Language Reporting', fontsize=15)
axes[1].set_xlim(0, 170) 
axes[1].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)

axes[2].tick_params(axis='y', labelsize=15)
axes[2].tick_params(axis='x', labelsize=15)
axes[2].set_xlabel('Frequency', fontsize=15)
axes[2].set_title('Top 10 Library/Framework Usage', fontsize=15)
axes[2].grid(axis='x', linestyle='--', alpha=0.6, zorder=1)


# Adjust subplot parameters to reduce white space
plt.subplots_adjust(wspace=0.01)  # Adjust the wspace parameter as needed to reduce the white space

plt.tight_layout()


# Save the combined plot to a local folder
plt.savefig('viz/technical_summary_2024.pdf')

# Display the combined chart
plt.show()






df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)


df_transparency = df[["year", "Source code availability", "Data used availability", "Hosted Application for End-users", "publication_type"]]
df_transparency.fillna("no", inplace=True)


df_transparency['Hosted Application for End-users'] = df_transparency['Hosted Application for End-users'].str.split(':|,', n=1).str[0]



df_transparency.head(2)


# Group by year and calculate the percentages for each category
result_by_year = df_transparency.groupby("year").agg({
    "Source code availability": lambda x: (x == "yes").mean() * 100,
    "Data used availability": lambda x: (x == "yes").mean() * 100,
    "Hosted Application for End-users": lambda x: (x == "yes").mean() * 100
}).reset_index()

# Rename columns for clarity
result_by_year.columns = ["Year", "Source code availability (%)", "Data used availability (%)", "Hosted Application for End-users (%)"]
result_by_year


result = df_transparency.groupby("publication_type").agg({
    "Source code availability": lambda x: (x == "yes").mean() * 100,
    "Data used availability": lambda x: (x == "yes").mean() * 100,
    "Hosted Application for End-users": lambda x: (x == "yes").mean() * 100
}).reset_index()

# Rename columns for clarity
result.columns = ["publication_type", "Source code availability", "Data used availability", "Application for End-users"]
result


def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        plt.annotate(f'{height:.0f}%',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 3),  # 3 points vertical offset
                     textcoords="offset points",
                     ha='center', va='bottom', fontsize=15)


from matplotlib.patches import Patch

# Adjusted code

fig, ax = plt.subplots(figsize=(10, 6))  # Increased plot size

# The x position for each bar
bar_width = 0.35
index = np.arange(len(result.columns) - 1)

bar1 = ax.bar(index, result.iloc[0, 1:], bar_width, label="Biomedical Journal", color="#0072B2",zorder=2)
bar2 = ax.bar(index + bar_width, result.iloc[1, 1:], bar_width, label="NLP Venue", color="#E69F00",zorder=2)

#ax.set_xlabel('Category', fontsize=15)
#ax.set_ylabel('Percentage', fontsize=15)
ax.set_title('Transparency of Methods', fontsize=15)
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels(result.columns[1:], fontsize=15)
ax.tick_params(axis='y', labelsize=15)

# Adding a dashed line in the background
plt.grid(axis='y', linestyle='--', alpha=0.6, zorder=1)

ax.set_ylim(0, 100)

# Adding percentage labels on the bars
add_labels(bar1)
add_labels(bar2)

# Create legend handles for each category
legend_handles = [Patch(color="#0072B2", label='Biomedical Journal'), Patch(color="#E69F00", label='NLP Venue')]

# Add color legend
plt.legend(handles=legend_handles, fontsize=15)
plt.savefig('viz/rs_transparency_overview_2024.pdf')

plt.show()



df_overview = df_transparency.groupby(['publication_type', 'New dataset developed']).size().reset_index(name='counts')
df_overview


df_overview = df_transparency.groupby(['publication_type', 'Source code availability']).size().reset_index(name='counts')
df_overview


# Group by "publication_type" and "Source code availability" and calculate the percentage
df_grouped = df_transparency.groupby(['publication_type', 'Source code availability']).size().unstack(fill_value=0)
df_grouped_percentage = df_grouped.div(df_grouped.sum(axis=1), axis=0) * 100
df_grouped_percentage


df_grouped = df_transparency.groupby(['publication_type', 'Data used availability']).size().unstack(fill_value=0)
df_grouped_percentage = df_grouped.div(df_grouped.sum(axis=1), axis=0) * 100
df_grouped_percentage


# Create a larger figure
plt.figure(figsize=(20, 3))

# Group by "Source code availability" and "publication_type", then count the occurrences
counts = df_transparency.groupby(['Source code availability', 'publication_type']).size().unstack(fill_value=0)

# Plot the stacked bar chart
bars = counts.plot(kind='bar', stacked=True, color=['skyblue', 'darkblue'], width=0.8)

# Add count values as annotations for each bar
for i, source_avail in enumerate(counts.index):
    for pub_type, count in zip(counts.columns, counts.loc[source_avail]):
        total_count = counts.sum(axis=1)[source_avail]
        plt.text(i, total_count - 13, str(count), color='white' if count < counts.sum(axis=1).max() else 'black', ha='center', va='center', fontsize=12)

# Add total count values on top of each bar
for i, total_count in enumerate(counts.sum(axis=1)):
    plt.text(i, total_count, str(total_count), color='black', ha='center', va='bottom', fontsize=12)
    
# Customize the plot
plt.xticks(range(len(counts.index)), counts.index, rotation=0, fontsize=15)
plt.tick_params(axis='y', labelsize=15)
plt.xlabel('Source code availability', fontsize=15)
plt.ylabel('Counts', fontsize=15)
plt.legend(['Biomedical Journal', 'NLP Venue'], title='Publication Type', fontsize=12)

# Set the y-axis limit to ensure it reaches up to 100
plt.ylim(0, 100)

# Tight layout for better visualization
plt.tight_layout()

# Display the chart
plt.show()





df = pd.concat([df_initial, df_2024_included], ignore_index=True)
df['year'] = df['year'].astype(int)


df.columns


df_rob = df[["Internal validity 1", "Internal validity 2", "External validity" ,"External validity.1"]]
# Define the new column names
new_column_names = {
    "Internal validity 1": "data_split_reported",
    "Internal validity 2": "overfitting_avoided",
    "External validity": "compared_to_others",
    "External validity.1": "external_validity"
}
# Rename the columns
df_rob = df_rob.rename(columns=new_column_names)

# Convert column names to lowercase
df_rob = df_rob.applymap(lambda x: x.lower() if isinstance(x, str) else x)

# Replace NaNs with "no"
df_rob = df_rob.fillna('no')

# Replace "yes" with "yes" in 'overfitting_avoided' column
df_rob = df_rob.replace(r'^yes.*', 'yes', regex=True)
df_rob = df_rob.replace(r'^partial.*', 'yes', regex=True)

df_rob.head(5)


len(df_rob[df_rob["data_split_reported"]=="yes"])/len(df_rob)


len(df_rob[df_rob["overfitting_avoided"]=="yes"])/len(df_rob)


len(df_rob[df_rob["compared_to_others"]=="yes"])/len(df_rob)



