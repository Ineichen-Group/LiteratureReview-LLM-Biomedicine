,author,doi,Link to paper,title,journal,year,Extracted by,Included,Reason for exclusion,Target application,Domain of automated approach,Target database,Data type,Data filter applied,Hosted Application for End-users,Alternative Approach,LLM Model,Models/Resources used with LLM,System Architecture/ Flow,Programming language,Library/Framework,Library/Framework Harmonized,Reported performance metrics,Reported performance metrics Harmonized,Source code availability,Source code link,Data used availability,Preprocessing applied,Pretraining corpus origin,Pretraining corpus size,Fine-tuning corpus data/task,New annotations,Annotations type,New dataset developed,Fine-tuning corpus size,Number of tasks/datasets for performance evaluation,Hardware used for training/validation/fine-tuning,Hardware type,Internal validity 1,Internal validity 2,External validity,External validity.1,Comment,Unnamed: 43,url,label,type,abstract,address,accession,Link,issue,keywords,language,issn,pages,volume,DA,DB,DP,ID,j2,m3,n1,ST,m1,c5,c2,c1,OP,publication_type,Unnamed: 40
219,"Li, D. and Xiong, Y. and Hu, B. and Tang, B. and Peng, W. and Chen, Q.",10.1186/s12911-021-01614-7,https://doi.org/10.1186/s12911-021-01614-7,Drug knowledge discovery via multi-task learning and pre-trained models,BMC Medical Informatics & Decision Making,2021.0,SDO,1,,Information Extraction,Pharma/ Drug Repurposing: Collecting mutation-disease knowledge from PubMed for drug repurposing.,PubMed,Abstracts,,no,,"BERT, NCBI BERT, ClinicalBERT, BioBERT",,"We start by splitting the PubMed abstract into sentences, tagging them as words, and extracting several features, such as POS tags. NER offsets and entity identification are then performed based on the BERT-based approach, and finally the relationship of each potential entity pair is predicted.",Python,,,"Precision, Recall, F1-score","Precision, Recall, F1-Score",no,,yes,WordPiece tokenization,depends on BERT model,depends on BERT model,BC5CDR-disease; NCBI-disease; BC2GM; 2010 i2b2/VA; AGAC corpus,,,no,"# Train 4559; 5423; 12,573; 16,315 # Dev 4580; 922; 2518; – # Test 4796; 939; 5037; 27,626",4,,,yes,,yes,BiLSTM + CRF,,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=med20&AN=34789238https://uzb.swisscovery.slsp.ch/openurl/41SLSP_UZB/41SLSP_UZB:UZB?sid=OVID:medline&id=pmid:34789238&id=doi:10.1186%2Fs12911-021-01614-7&issn=1472-6947&isbn=&volume=21&issue=9&spage=251&pages=251&date=2021&title=BMC+Medical+Informatics+%26+Decision+Making&atitle=Drug+knowledge+discovery+via+multi-task+learning+and+pre-trained+models.&aulast=Li&pid=%3Cauthor%3ELi+D%3BXiong+Y%3BHu+B%3BTang+B%3BPeng+W%3BChen+Q%3C%2Fauthor%3E%3CAN%3E34789238%3C%2FAN%3E%3CDT%3EJournal+Article%3C%2FDT%3E,Li_2021_BMI.DM,JOUR,"BACKGROUND: Drug repurposing is to find new indications of approved drugs, which is essential for investigating new uses for approved or investigational drug efficiency. The active gene annotation corpus (named AGAC) is annotated by human experts, which was developed to support knowledge discovery for drug repurposing. The AGAC track of the BioNLP Open Shared Tasks using this corpus is organized by EMNLP-BioNLP 2019, where the ""Selective annotation"" attribution makes AGAC track more challenging than other traditional sequence labeling tasks. In this work, we show our methods for trigger word detection (Task 1) and its thematic role identification (Task 2) in the AGAC track. As a step forward to drug repurposing research, our work can also be applied to large-scale automatic extraction of medical text knowledge. METHODS: To meet the challenges of the two tasks, we consider Task 1 as the medical name entity recognition (NER), which cultivates molecular phenomena related to gene mutation. And we regard Task 2 as a relation extraction task, which captures the thematic roles between entities. In this work, we exploit pre-trained biomedical language representation models (e.g., BioBERT) in the information extraction pipeline for mutation-disease knowledge collection from PubMed. Moreover, we design the fine-tuning framework by using a multi-task learning technique and extra features. We further investigate different approaches to consolidate and transfer the knowledge from varying sources and illustrate the performance of our model on the AGAC corpus. Our approach is based on fine-tuned BERT, BioBERT, NCBI BERT, and ClinicalBERT using multi-task learning. Further experiments show the effectiveness of knowledge transformation and the ensemble integration of models of two tasks. We conduct a performance comparison of various algorithms. We also do an ablation study on the development set of Task 1 to examine the effectiveness of each component of our method. RESULTS: Compared with competitor methods, our model obtained the highest Precision (0.63), Recall (0.56), and F-score value (0.60) in Task 1, which ranks first place. It outperformed the baseline method provided by the organizers by 0.10 in F-score. The model shared the same encoding layers for the named entity recognition and relation extraction parts. And we obtained a second high F-score (0.25) in Task 2 with a simple but effective framework. CONCLUSIONS: Experimental results on the benchmark annotation of genes with active mutation-centric function changes corpus show that integrating pre-trained biomedical language representation models (i.e., BERT, NCBI BERT, ClinicalBERT, BioBERT) into a pipe of information extraction methods with multi-task learning can improve the ability to collect mutation-disease knowledge from PubMed.","Li, Dongfang. Harbin Institute of Technology (Shenzhen), Shenzhen, China. Xiong, Ying. Harbin Institute of Technology (Shenzhen), Shenzhen, China. Hu, Baotian. Harbin Institute of Technology (Shenzhen), Shenzhen, China. hubaotian@hit.edu.cn. Tang, Buzhou. Harbin Institute of Technology (Shenzhen), Shenzhen, China. Tang, Buzhou. Peng Cheng Laboratory, Shenzhen, China. Peng, Weihua. Baidu, International Technology (Shenzhen) Co., Ltd, Shenzhen, China. Chen, Qingcai. Harbin Institute of Technology (Shenzhen), Shenzhen, China. qingcai.chen@hit.edu.cn. Chen, Qingcai. Peng Cheng Laboratory, Shenzhen, China. qingcai.chen@hit.edu.cn.",34789238.0,,Suppl 9,Algorithms and Humans and Information Storage and Retrieval and Knowledge Discovery and *Natural Language Processing and *Pharmaceutical Preparations and 0 (Pharmaceutical Preparations),English,1472-6947,251,21,11 16,MEDLINE,Ovid Technologies,2897.0,BMC Med Inf Decis Mak,"Research Support, Non-U.S. Gov't","Li, Dongfang Xiong, Ying Hu, Baotian Tang, Buzhou Peng, Weihua Chen, Qingcai and ASReview_relevant",Drug knowledge discovery via multi-task learning and pre-trained models,,,,,,biomedical_journal,
265,"Martenot, V. and Masdeu, V. and Cupe, J. and Gehin, F. and Blanchon, M. and Dauriat, J. and Horst, A. and Renaudin, M. and Girard, P. and Zucker, J. D.",10.1186/s12911-022-02085-0,https://doi.org/10.1186/s12911-022-02085-0,LiSA: an assisted literature search pipeline for detecting serious adverse drug events with deep learning,BMC Medical Informatics & Decision Making,2022.0,SDO,1,,Information Retrieval,"Literature-Based Discovery/ Pharma/ Adverse-Drug-Event: automatically identifying relevant publications mentioning an established link between a Drug and a Serious Adverse Event (includes AE-DRUG relationship classification, NER, Seriousness classification)",PubMed,"Abstracts, Full-text, Sentences",,"yes, but not public",,"BioBERT, BlueBERT, SciBERT, Bio+ClinicalBERT, BERT, PubMedBERT, UMLSBERT","regex-search, keyword search algorithm, rule-based system to calculate a ranking score","documents -> format into 3 tables: documents (metadata and full content), contents (split in different sections or paragraphs), menaing units -> keyword search with Aho-corasick algorithm for drugs and conditions-> 3 NLP tasks run in parallel -> filter and qualify relevant documents",Python,"HuggingFace, PowerBI for visualization","HuggingFace, PowerBI for visualization","precision, recall, F1-score; comparison to keywords search","Precision, Recall, F1-Score, Comparison to Keywords Search",no,,partially,sentence tokenization,depends on BERT,depends on BERT,ADE-Corpus-V2 dataset; Custom-annotated PubMed sentences,"sentences labeled for three categories based on the health impact of an event: “serious”, “important medical event”, “none” ",,yes,20k sentences ; 7776 sentences,3,,,yes,,partially,only LLM models compared,they use PowerBI for visualisation!,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=medl&AN=36550485https://uzb.swisscovery.slsp.ch/openurl/41SLSP_UZB/41SLSP_UZB:UZB?sid=OVID:medline&id=pmid:36550485&id=doi:10.1186%2Fs12911-022-02085-0&issn=1472-6947&isbn=&volume=22&issue=1&spage=338&pages=338&date=2022&title=BMC+Medical+Informatics+%26+Decision+Making&atitle=LiSA%3A+an+assisted+literature+search+pipeline+for+detecting+serious+adverse+drug+events+with+deep+learning.&aulast=Martenot&pid=%3Cauthor%3EMartenot+V%3BMasdeu+V%3BCupe+J%3BGehin+F%3BBlanchon+M%3BDauriat+J%3BHorst+A%3BRenaudin+M%3BGirard+P%3BZucker+JD%3C%2Fauthor%3E%3CAN%3E36550485%3C%2FAN%3E%3CDT%3EJournal+Article%3C%2FDT%3E,Martenot_2022_BMI.DM,JOUR,"INTRODUCTION: Detecting safety signals attributed to a drug in scientific literature is a fundamental issue in pharmacovigilance. The constant increase in the volume of publications requires the automation of this tedious task, in order to find and extract relevant articles from the pack. This task is critical, as serious Adverse Drug Reactions (ADRs) still account for a large number of hospital admissions each year. OBJECTIVES: The aim of this study is to develop an augmented intelligence methodology for automatically identifying relevant publications mentioning an established link between a Drug and a Serious Adverse Event, according to the European Medicines Agency (EMA) definition of seriousness. METHODS: The proposed pipeline, called LiSA (for Literature Search Application), is based on three independent deep learning models supporting a precise detection of safety signals in the biomedical literature. By combining a Bidirectional Encoder Representations from Transformers (BERT) algorithms and a modular architecture, the pipeline achieves a precision of 0.81 and a recall of 0.89 at sentences level in articles extracted from PubMed (either abstract or full-text). We also measured that by using LiSA, a medical reviewer increases by a factor of 2.5 the number of relevant documents it can collect and evaluate compared to a simple keyword search. In the interest of re-usability, emphasis was placed on building a modular pipeline allowing the insertion of other NLP modules to enrich the results provided by the system, and extend it to other use cases. In addition, a lightweight visualization tool was developed to analyze and monitor safety signal results. CONCLUSIONS: Overall, the generic pipeline and the visualization tool proposed in this article allows for efficient and accurate monitoring of serious adverse drug reactions from the literature and can easily be adapted to similar pharmacovigilance use cases. To facilitate reproducibility and benefit other research studies, we also shared a first benchmark dataset for Serious Adverse Drug Events detection.","Martenot, Vincent. Quinten, 8 rue Vernier, 75017, Paris, France. v.martenot@quinten-france.com. Masdeu, Valentin. Quinten, 8 rue Vernier, 75017, Paris, France. Cupe, Jean. Quinten, 8 rue Vernier, 75017, Paris, France. Gehin, Faustine. Quinten, 8 rue Vernier, 75017, Paris, France. Blanchon, Margot. Quinten, 8 rue Vernier, 75017, Paris, France. Dauriat, Julien. Quinten, 8 rue Vernier, 75017, Paris, France. Horst, Alexander. Swiss Agency for Therapeutic Products, Swissmedic, Hallerstrasse 7, 3012, Bern, Switzerland. Renaudin, Michael. Swiss Agency for Therapeutic Products, Swissmedic, Hallerstrasse 7, 3012, Bern, Switzerland. Girard, Philippe. Swiss Agency for Therapeutic Products, Swissmedic, Hallerstrasse 7, 3012, Bern, Switzerland. Zucker, Jean-Daniel. UMMISCO, Sorbonne University, IRD, Bondy, France. Jean-Daniel.zucker@ird.fr.",36550485.0,,1,Humans and *Deep Learning and Reproducibility of Results and Adverse Drug Reaction Reporting Systems and Algorithms and Drug-Related Side Effects and Adverse Reactions/di [Diagnosis] and Drug-Related Side Effects and Adverse Reactions/ep [Epidemiology] and *Drug-Related Side Effects and Adverse Reactions,English,1472-6947,338,22,12 22,MEDLINE,Ovid Technologies,3308.0,BMC Med Inf Decis Mak,"Research Support, Non-U.S. Gov't","Martenot, Vincent Masdeu, Valentin Cupe, Jean Gehin, Faustine Blanchon, Margot Dauriat, Julien Horst, Alexander Renaudin, Michael Girard, Philippe Zucker, Jean-Daniel and ASReview_relevant",LiSA: an assisted literature search pipeline for detecting serious adverse drug events with deep learning,,,,,,biomedical_journal,
166,"Ji, Z. and Wei, Q. and Xu, H.",,https://doi.org/,BERT-based Ranking for Biomedical Entity Normalization,AMIA Summits on Translational Science Proceedings,2020.0,SDO,1,,Entity Normalization/Linking,General biomedical text mining: Alleviate the variation problem for entity linking in biomedical texts.,"Clinical texts, PubMed","Clinical notes, Abstracts",,no,,"BERT, BioBERT, ClinicalBERT",BM25,Mention -> Pre-processing -> Candidate Conpcet Ranking -> Unlinkable Mention Prediction,"Python, Java",CLAMP toolkit,CLAMP toolkit,accuracy,Accuracy,no,,yes,"spelling correction, abbreviation resolution, numeric synonyms resolution, tokenization, punctuation removal, stemming, lower-casing",depends on the pre-trained model,depends on the pre-trained model,ShARe/CLEF; NCBI-disease; TAC-ADR 2017,,,no,see paper table with overview,3,Quadro P6000 GPU,GPU,yes,,yes,"UWM, TaggerOne, CNN-based, Xu et al.’s system, D’Souza & Ng’s system",,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=pmnm5&AN=32477646https://uzb.swisscovery.slsp.ch/openurl/41SLSP_UZB/41SLSP_UZB:UZB?sid=OVID:medline&id=pmid:32477646&id=doi:&issn=2153-4063&isbn=&volume=2020&issue=&spage=269&pages=269-277&date=2020&title=AMIA+Summits+on+Translational+Science+Proceedings&atitle=BERT-based+Ranking+for+Biomedical+Entity+Normalization.&aulast=Ji&pid=%3Cauthor%3EJi+Z%3BWei+Q%3BXu+H%3C%2Fauthor%3E%3CAN%3E32477646%3C%2FAN%3E%3CDT%3EJournal+Article%3C%2FDT%3E,Ji_2020_AMSuonTrScPr,JOUR,"Developing high-performance entity normalization algorithms that can alleviate the term variation problem is of great interest to the biomedical community. Although deep learning-based methods have been successfully applied to biomedical entity normalization, they often depend on traditional context-independent word embeddings. Bidirectional Encoder Representations from Transformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for Clinical Text Mining (ClinicalBERT) were recently introduced to pre-train contextualized word representation models using bidirectional Transformers, advancing the state-of-the-art for many natural language processing tasks. In this study, we proposed an entity normalization architecture by fine-tuning the pre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive experiments to evaluate the effectiveness of the pre-trained models for biomedical entity normalization using three different types of datasets. Our experimental results show that the best fine-tuned models consistently outperformed previous methods and advanced the state-of-the-art for biomedical entity normalization, with up to 1.17% increase in accuracy.","Ji, Zongcheng. School of Biomedical Informatics, The University of Texas Health Science Center at Houston, Houston, TX, USA. Wei, Qiang. School of Biomedical Informatics, The University of Texas Health Science Center at Houston, Houston, TX, USA. Xu, Hua. School of Biomedical Informatics, The University of Texas Health Science Center at Houston, Houston, TX, USA.",32477646.0,,,,English,2153-4063,269-277,2020,,MEDLINE,Ovid Technologies,2280.0,AMIA Summits Transl Sci Proc,,"Ji, Zongcheng Wei, Qiang Xu, Hua and ASReview_relevant",BERT-based Ranking for Biomedical Entity Normalization,,,,,,biomedical_journal,
279,"['Zhang, Y.', 'Zhou, B.', 'Song, K.', 'Sui, X.', 'Zhao, G.']",,https://aclanthology.org/2022.findings-emnlp.144/ https://aclanthology.org/2022.findings-emnlp.144.pdf,PM2F2N: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction,Association for Computational Linguistics/ Findings,2022.0,SDO,1,,Text Classification,Clinical/ Outcome Prediction: clinical outcome prediction based on multi-modal data.,"Clinical texts, sensory patient data",Clinical notes,,no,,BioBERT,"Graph Neural Networks, TF-IDF, Bi-GRU, Med7 model for NER",,Python,,,,"AUROC, AUPRC",yes,https://github.com/ZovanZhou/PM2F2N,yes,,biobert,biobert,MIMIC-III ,,,no,see table 1,4,NVIDIA GTX A6000 ,GPU,yes,,yes,SOTA models,,,,,,"Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common prediction targets: mortality and length of stay. Traditional methods focused on utilizing time series data but ignored clinical notes. With the development of deep learning, natural language processing (NLP) and multi-modal learning methods are exploited to jointly model the time series and clinical notes with different modals. However, the existing methods failed to fuse the multi-modal features of patients from different views. Therefore, we propose the patient multi-view multi-modal feature fusion networks for clinical outcome prediction. Firstly, from patient inner view, we propose to utilize the co-attention module to enhance the fine-grained feature interaction between time series and clinical notes from each patient. Secondly, the patient outer view is the correlation between patients, which can be reflected by the structural knowledge in clinical notes. We exploit the structural information extracted from clinical notes to construct the patient correlation graph, and fuse patients’ multi-modal features by graph neural networks (GNN). The experimental results on MIMIC-III benchmark demonstrate the superiority of our method.",,,,,,,,,,,,,,,,,,,,,,,nlp_venue,
256,"['Lu, Q.', 'Dou, D.', 'Nguyen, T.']",,https://aclanthology.org/2022.findings-emnlp.398/ https://aclanthology.org/2022.findings-emnlp.398.pdf,ClinicalT5: A generative language model for clinical text,Association for Computational Linguistics/ Findings,2022.0,SDO,1,,Multi-Domain,General biomedical text mining: T5-based text-to-text transformer model pre-trained on clinical text for multiple tasks.,Clinical texts,Clinical notes,,no,,SciFive-PubMed-PMC,,,Python,,"PyTorch, HuggingFace",,"Precision, Recall, F1-Score, Accuracy",no,,yes,minimal pre-processing isconducted where unnecessary tokens and characters are removed,MIMIC-III,2 million notes,HOC dataset; NCBI-disease; BC5CDR-disease; MedNLI,,,no,tbr,3,3 Nvidia Tesla V100-32GB GPUs,GPU,yes,,yes,"BART, T5, BioBART, SciFive",,,,,,"In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines.",,,,,,,,,,,,,,,,,,,,,,,nlp_venue,
