TY  - JOUR
AB  - … For example, the encode-limit for a BERT-based metric is 512 tokens. However, real-world summaries and clinical notes may contain more than 512 tokens. For instance, our analysis in …
AU  - Abacha, A. B.
AU  - Yim, W.
AU  - Michalopoulos, G.
DA  - //
PY  - 2023
ST  - An Investigation of Evaluation Methods in Automatic Medical Note Generation
T2  - … Linguistics: ACL 2023
TI  - An Investigation of Evaluation Methods in Automatic Medical Note Generation
UR  - https://aclanthology.org/2023.findings-acl.161/
https://aclanthology.org/2023.findings-acl.161.pdf
ID  - 450
ER  - 

TY  - JOUR
AB  - … is a BERT-based language representation model which is pre-trained on biomedical corpora … It has reported state-of-the-art performance in several NLP related tasks on biomedical text, …
AU  - Afzal, Z.
AU  - Yadav, V.
AU  - Fedorova, O.
AU  - Kandala, V.
DA  - //
N1  - Cited By (since 2020): 1
PY  - 2020
ST  - CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles
T2  - … -19 (Part 2) at EMNLP …
TI  - CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles
UR  - https://aclanthology.org/2020.nlpcovid19-2.2/
https://aclanthology.org/2020.nlpcovid19-2.2.pdf
ID  - 425
ER  - 

TY  - JOUR
AB  - Recent papers have introduced methods to incorporate gazetteer features and entity segmentation techniques in neural named entity recognition models. These papers rely on different …
AU  - Agarwal, O.
AU  - Nenkova, A.
DA  - //
N1  - Cited By (since 2021): 4
PY  - 2021
ST  - The Utility and Interplay of Gazetteers and Entity Segmentation for Named Entity Recognition in English
T2  - … for Computational Linguistics: ACL …
TI  - The Utility and Interplay of Gazetteers and Entity Segmentation for Named Entity Recognition in English
UR  - https://aclanthology.org/2021.findings-acl.349.pdf
ID  - 478
ER  - 

TY  - JOUR
AB  - We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it …
AU  - Ahuja, C.
AU  - Lee, D. W.
AU  - Ishii, R.
DA  - //
N1  - Cited By (since 2020): 37
PY  - 2020
ST  - No gestures left behind: Learning relationships between spoken language and freeform gestures
T2  - … Linguistics: EMNLP 2020
TI  - No gestures left behind: Learning relationships between spoken language and freeform gestures
UR  - https://aclanthology.org/2020.findings-emnlp.170/
https://aclanthology.org/2020.findings-emnlp.170.pdf
ID  - 436
ER  - 

TY  - JOUR
AB  - … 2015) provides annual competitions on biomedical semantic indexing and questionanswering. … Another line of work could encompass combining a transformbased model, like BERT, in …
AU  - Almeida, T.
AU  - Matos, S.
DA  - //
N1  - Cited By (since 2020): 5
PY  - 2020
ST  - Frugal neural reranking: evaluation on the covid-19 literature
T2  - … on NLP for COVID-19 (Part 2) at EMNLP 2020
TI  - Frugal neural reranking: evaluation on the covid-19 literature
UR  - https://openreview.net/forum?id=WYyPpXNXQyC
https://openreview.net/pdf?id=WYyPpXNXQyC
ID  - 417
ER  - 

TY  - JOUR
AB  - … Model We fine-tune a BERT-base model on metadata shaped data for each task, taking the pooled [… , we compare to methods which use the same base LM, BERT-base, and external …
AU  - Arora, S.
AU  - Wu, S.
AU  - Liu, E.
AU  - Ré, C.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - Metadata shaping: A simple approach for knowledge-enhanced language models
T2  - … for Computational Linguistics: ACL …
TI  - Metadata shaping: A simple approach for knowledge-enhanced language models
UR  - https://aclanthology.org/2022.findings-acl.137/
https://aclanthology.org/2022.findings-acl.137.pdf
ID  - 392
ER  - 

TY  - JOUR
AB  - … Inter-annotator agreement and the upper limit on machine performance: Evidence from biomedical natural language processing. In MEDINFO 2017: Precision Healthcare through …
AU  - Aufrant, L.
DA  - //
PY  - 2022
ST  - Is NLP Ready for Standardization?
T2  - … the Association for Computational Linguistics: EMNLP …
TI  - Is NLP Ready for Standardization?
UR  - https://aclanthology.org/2022.findings-emnlp.202/
https://aclanthology.org/2022.findings-emnlp.202.pdf
ID  - 475
ER  - 

TY  - JOUR
AB  - … The results for the two best baseline models are also included, namely the same model using only the BERT document embeddings and only the w2v embeddings over a simple Bi-…
AU  - Azevedo, L.
AU  - D'Aquin, M.
AU  - Davis, B.
DA  - //
N1  - Cited By (since 2021): 6
PY  - 2021
ST  - Lux (linguistic aspects under examination): Discourse analysis for automatic fake news classification
T2  - … Processing (ACL-IJCNLP …
TI  - Lux (linguistic aspects under examination): Discourse analysis for automatic fake news classification
UR  - https://hal.science/hal-03659147/
https://hal.science/hal-03659147/document
ID  - 433
ER  - 

TY  - JOUR
AB  - … BERT encoder the context and an indicator for the answer span location in the context (Fig. 1-(2), BERT … In this paper, we use BERT as the default MRC model, since BERT or its variants …
AU  - Back, S.
AU  - Kedia, A.
AU  - Chinthakindi, S. C.
AU  - Lee, H.
DA  - //
N1  - Cited By (since 2021): 10
PY  - 2021
ST  - Learning to generate questions by learning to recover answer-containing sentences
T2  - … Linguistics: ACL …
TI  - Learning to generate questions by learning to recover answer-containing sentences
UR  - https://aclanthology.org/2021.findings-acl.132.pdf
ID  - 376
ER  - 

TY  - JOUR
AB  - … Then a softmax classifier is to predict intent categories with representations of “[CLS]” in BERT. … We use base BERT as the classifier. The learning rate is set to 2e-5. The batch size is 8. …
AU  - Bai, G.
AU  - He, S.
AU  - Liu, K.
AU  - Zhao, J.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - Incremental intent detection for medical domain with contrast replay networks
T2  - … for Computational Linguistics: ACL …
TI  - Incremental intent detection for medical domain with contrast replay networks
UR  - https://aclanthology.org/2022.findings-acl.280/
https://aclanthology.org/2022.findings-acl.280.pdf
ID  - 367
ER  - 

TY  - JOUR
AB  - … , and the five most likely predictions from BERT. The examples show some random predic… We argue that even though BERT is trained on Wikipedia content, the huge amount of data …
AU  - Beloucif, M.
AU  - Biemann, C.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Probing pre-trained language models for semantic attributes and their values
T2  - … for Computational Linguistics: EMNLP …
TI  - Probing pre-trained language models for semantic attributes and their values
UR  - https://aclanthology.org/2021.findings-emnlp.218/
https://aclanthology.org/2021.findings-emnlp.218.pdf
ID  - 357
ER  - 

TY  - JOUR
AB  - Context guides comprehenders’ expectations during language processing, and informationtheoretic surprisal is commonly used as an index of cognitive processing effort. However, …
AU  - Bhattasali, S.
AU  - Resnik, P.
DA  - //
N1  - Cited By (since 2021): 4
PY  - 2021
ST  - Using surprisal and fMRI to map the neural bases of broad and local contextual prediction during natural language comprehension
T2  - … for Computational Linguistics: ACL …
TI  - Using surprisal and fMRI to map the neural bases of broad and local contextual prediction during natural language comprehension
UR  - https://aclanthology.org/2021.findings-acl.332.pdf
ID  - 477
ER  - 

TY  - JOUR
AB  - … , and to compare RE models with the RE baselines in both biomedical (PMC) and universal (arXiv) domains. To … We first use a fused BERT embedding efd to represent this funder name: …
AU  - Bian, J.
AU  - Huang, L.
AU  - Huang, X.
AU  - Zhou, H.
DA  - //
N1  - Cited By (since 2021): 4
PY  - 2021
ST  - Grantrel: Grant information extraction via joint entity and relation extraction
T2  - … Linguistics: ACL-IJCNLP …
TI  - Grantrel: Grant information extraction via joint entity and relation extraction
UR  - https://aclanthology.org/2021.findings-acl.236.pdf
ID  - 347
ER  - 

TY  - JOUR
AB  - Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive …
AU  - Blloshmi, R.
AU  - Tripodi, R.
AU  - Navigli, R.
DA  - //
N1  - Cited By (since 2020): 50
PY  - 2020
ST  - XL-AMR: Enabling cross-lingual AMR parsing with transfer learning techniques
T2  - … Language Processing (EMNLP)
TI  - XL-AMR: Enabling cross-lingual AMR parsing with transfer learning techniques
UR  - https://aclanthology.org/2020.emnlp-main.195/
https://aclanthology.org/2020.emnlp-main.195.pdf
ID  - 471
ER  - 

TY  - JOUR
AB  - … 2018) is a dataset of papers drawn from the biomedical and life sciences. Unlike PeerRead, … We find that this distinction is captured by the SC measure using BERT. Quantifying seman…
AU  - Bommasani, R.
AU  - Cardie, C.
DA  - //
N1  - Cited By (since 2020): 43
PY  - 2020
ST  - Intrinsic evaluation of summarization datasets
T2  - … Natural Language Processing (EMNLP …
TI  - Intrinsic evaluation of summarization datasets
UR  - https://aclanthology.org/2020.emnlp-main.649/
https://aclanthology.org/2020.emnlp-main.649.pdf
ID  - 412
ER  - 

TY  - JOUR
AB  - … to some of them, we trained four standard BERT classifiers on the missing datasets (ie GoE-… we name GoE-Bert, M-GoE Bert, M-Bert. The classifiers employ the pre-trained BERT-base …
AU  - Bulla, L.
AU  - Gangemi, A.
DA  - //
PY  - 2023
ST  - Towards Distribution-shift Robust Text Classification of Emotional Content
T2  - … for Computational Linguistics: ACL 2023
TI  - Towards Distribution-shift Robust Text Classification of Emotional Content
UR  - https://aclanthology.org/2023.findings-acl.524/
https://aclanthology.org/2023.findings-acl.524.pdf
ID  - 395
ER  - 

TY  - JOUR
AB  - … 2012) is a dataset on the biomedical domain, for which Adverse-Effects from drugs are annotated as pairs of drug and adverse-effect. The dataset provides 10-folds of train and test splits…
AU  - Cabot, P. L. H.
AU  - Navigli, R.
DA  - //
N1  - Cited By (since 2021): 82
PY  - 2021
ST  - REBEL: Relation extraction by end-to-end language generation
T2  - … for Computational Linguistics: EMNLP …
TI  - REBEL: Relation extraction by end-to-end language generation
UR  - https://aclanthology.org/2021.findings-emnlp.204/
https://aclanthology.org/2021.findings-emnlp.204.pdf
ID  - 389
ER  - 

TY  - JOUR
AB  - … We propose Simple-QE, a BERT-based quality estimation (QE) model adapted from prior summarization QE work, and show that it correlates well with human quality judgments. Simple-…
AU  - Callison-Burch, C.
AU  - Li, B.
AU  - Callison-Burch, C.
DA  - //
PY  - 2023
ST  - Understanding Generative Artificial Intelligence and Its Relationship to Copyright
T2  - … Linguistics (ACL 2023 …
TI  - Understanding Generative Artificial Intelligence and Its Relationship to Copyright
UR  - https://www.cis.upenn.edu/~ccb/publications.html
ID  - 482
ER  - 

TY  - JOUR
AB  - Different from other text generation tasks, in product description generation, it is of vital importance to generate faithful descriptions that stick to the product attribute information. However…
AU  - Chan, Z.
AU  - Chen, X.
AU  - Wang, Y.
AU  - Li, J.
AU  - Zhang, Z.
DA  - //
N1  - Cited By (since 2019): 16
PY  - 2019
ST  - Stick to the facts: Learning towards a fidelity-oriented e-commerce product description generation
T2  - … Processing (EMNLP …
TI  - Stick to the facts: Learning towards a fidelity-oriented e-commerce product description generation
UR  - https://aclanthology.org/D19-1501/
https://aclanthology.org/D19-1501.pdf
ID  - 483
ER  - 

TY  - JOUR
AB  - … BERT with its default settings (eg, for BERT-base, we use 12 layers of multi-head attentions with 768 dimensional hidden vectors; for BERT-… the hidden vectors from BERT. For evaluation…
AU  - Chen, G.
AU  - Tian, Y.
AU  - Song, Y.
AU  - Wan, X.
DA  - //
N1  - Cited By (since 2021): 25
PY  - 2021
ST  - Relation extraction with type-aware map memories of word dependencies
T2  - … Computational Linguistics: ACL …
TI  - Relation extraction with type-aware map memories of word dependencies
UR  - https://aclanthology.org/2021.findings-acl.221.pdf
ID  - 374
ER  - 

TY  - JOUR
AB  - … many NLP tasks, our experiment employs BERT (Devlin et al.… For a fair comparison, we take the same BERT-base encoder … For a fair comparison, the BERT used in our model is BERT-…
AU  - Chen, Y.
DA  - //
PY  - 2023
ST  - Incomplete Utterance Rewriting as Sequential Greedy Tagging
T2  - … of the Association for Computational Linguistics: ACL …
TI  - Incomplete Utterance Rewriting as Sequential Greedy Tagging
UR  - https://aclanthology.org/2023.findings-acl.456/
https://aclanthology.org/2023.findings-acl.456.pdf
ID  - 409
ER  - 

TY  - JOUR
AB  - … BERT mainly learns from the masked subset of input tokens, but ELECTRA could predict all … We conduct experiments in both computer science and biomedical domains, SODA …
AU  - Cheng, D.
AU  - Huang, S.
AU  - Liu, J.
AU  - Zhan, Y.
AU  - Sun, H.
DA  - //
PY  - 2022
ST  - Snapshot-Guided Domain Adaptation for ELECTRA
T2  - … Linguistics: EMNLP …
TI  - Snapshot-Guided Domain Adaptation for ELECTRA
UR  - https://aclanthology.org/2022.findings-emnlp.163/
https://aclanthology.org/2022.findings-emnlp.163.pdf
ID  - 322
ER  - 

TY  - JOUR
AB  - … BC5CDR is specially designed for biomedical domain. By contrast, we are the first to analyze … As DGCNN-BERT has been used in the main process of construction, we evaluate other …
AU  - Cheng, Q.
AU  - Liu, J.
AU  - Qu, X.
AU  - Zhao, J.
AU  - Liang, J.
DA  - //
N1  - Cited By (since 2021): 12
PY  - 2021
ST  - HacRED: A large-scale relation extraction dataset toward hard cases in practical applications
T2  - … Linguistics: ACL …
TI  - HacRED: A large-scale relation extraction dataset toward hard cases in practical applications
UR  - https://aclanthology.org/2021.findings-acl.249.pdf
ID  - 371
ER  - 

TY  - JOUR
AB  - … (legal, religious, and biomedical genres) and are annotated … For this metric, we adopted the same versions of BERT Base … Always by using the same BERT Base models, we measured …
AU  - Chersoni, E.
AU  - Hsu, Y. Y.
DA  - //
PY  - 2022
ST  - PolyU-CBS at TSAR-2022: A Simple, Rank-Based Method for Complex Word Substitution in Two Steps
T2  - EMNLP Workshop on Workshop on Text …
TI  - PolyU-CBS at TSAR-2022: A Simple, Rank-Based Method for Complex Word Substitution in Two Steps
UR  - https://www.researchgate.net/profile/Emmanuele-Chersoni/publication/366559314_PolyU-CBS_at_TSAR-2022_A_Simple_Rank-Based_Method_for_Complex_Word_Substitution_in_Two_Steps/links/63a6e086097c7832ca620172/PolyU-CBS-at-TSAR-2022-A-Simple-Rank-Based-Method-for-Complex-Word-Substitution-in-Two-Steps.pdf
ID  - 418
ER  - 

TY  - JOUR
AB  - Welcome to the very first volume of Findings of ACL, a new publication designed to operate as a “companion” to conferences in the ACL stable. This first volume, Findings of ACL: …
AU  - Cohn, T.
AU  - He, Y.
AU  - Liu, Y.
DA  - //
N1  - Cited By (since 2020): 2
PY  - 2020
ST  - Findings of the Association for Computational Linguistics: EMNLP 2020
T2  - … for Computational Linguistics: EMNLP 2020
TI  - Findings of the Association for Computational Linguistics: EMNLP 2020
UR  - https://aclanthology.org/2020.findings-emnlp.0.pdf
ID  - 470
ER  - 

TY  - BOOK
AB  - This paper presents transformer-based models created for the CLPsych 2022 shared task. Using posts from Reddit users over a period of time, we aim to predict changes in mood from …
AU  - Culnan, J.
AU  - Diaz, D. Y. Romero
AU  - Bethard, S.
M1  - Query date: 2023-07-14 10:46:20
N1  - Cited By (since 2022): 1
PB  - repository.arizona.edu
PY  - 2022
ST  - Exploring transformers and time lag features for predicting changes in mood over time
TI  - Exploring transformers and time lag features for predicting changes in mood over time
UR  - https://repository.arizona.edu/handle/10150/666482
https://repository.arizona.edu/bitstream/handle/10150/666482/2022.clpsych-1.21.pdf?sequence=1
ID  - 385
ER  - 

TY  - JOUR
AB  - … The large difference in training and test accuracy for the news, biomedical, and Reddit domains also points to a drift in domains between the training and test data. Thus, both of these is…
AU  - Currey, A.
AU  - Mathur, P.
AU  - Dinu, G.
DA  - //
N1  - Cited By (since 2020): 22
PY  - 2020
ST  - Distilling multiple domains for neural machine translation
T2  - … Language Processing (EMNLP)
TI  - Distilling multiple domains for neural machine translation
UR  - https://aclanthology.org/2020.emnlp-main.364/
https://aclanthology.org/2020.emnlp-main.364.pdf
ID  - 375
ER  - 

TY  - JOUR
AB  - … 2018) (Bidirectional Encoder Representations from Transformers) for biomedical text mining… Bio-BERT outperforms other approaches of embeddings as well as vanilla BERT on clinical …
AU  - Das, D.
AU  - Katyal, Y.
AU  - Verma, J.
AU  - Ranjan, R. K.
DA  - //
N1  - Cited By (since 2020): 22
PY  - 2020
ST  - Information retrieval and extraction on covid-19 clinical articles using graph community detection and bio-bert embeddings
T2  - ACL 2020 Workshop …
TI  - Information retrieval and extraction on covid-19 clinical articles using graph community detection and bio-bert embeddings
UR  - https://openreview.net/forum?id=W3Dzaik1ipL
https://openreview.net/pdf?id=W3Dzaik1ipL
ID  - 327
ER  - 

TY  - JOUR
AB  - … We apply a fine-tuned BERT model to each of the patient’s clinical notes. The resulting embeddings are then combined to obtain the overall embedding for the entire text part of the data…
AU  - Deznabi, I.
AU  - Iyyer, M.
AU  - Fiterau, M.
DA  - //
N1  - Cited By (since 2021): 11
PY  - 2021
ST  - Predicting in-hospital mortality by combining clinical notes with time-series data
T2  - … for computational linguistics: ACL …
TI  - Predicting in-hospital mortality by combining clinical notes with time-series data
UR  - https://aclanthology.org/2021.findings-acl.352.pdf
ID  - 335
ER  - 

TY  - JOUR
AB  - … Domain-specific language model pretraining for biomedical natural language processing. ACM … , we feed the [CLS] representation into the output layer when BERTbase as the classifier (…
AU  - Ding, R.
AU  - Han, X.
AU  - Wang, L.
DA  - //
PY  - 2023
ST  - A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks
T2  - … for Computational Linguistics: ACL 2023
TI  - A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks
UR  - https://aclanthology.org/2023.findings-acl.24/
https://aclanthology.org/2023.findings-acl.24.pdf
ID  - 396
ER  - 

TY  - JOUR
AB  - … Variants of BERT We compare three pretrained models namely, BERT and its successor … First, we observe that both Bio-BERT and RoBERTa outperformed the initial BERT model and …
AU  - Dixit, M.
AU  - Kirchhoff, S. B. K.
DA  - //
N1  - Cited By (since 2020): 2
PY  - 2020
ST  - Robust prediction of punctuation and truecasing for medical asr
T2  - ACL 2020
TI  - Robust prediction of punctuation and truecasing for medical asr
UR  - http://aclanthology.lst.uni-saarland.de/2020.nlpmc-1.pdf#page=65
ID  - 344
ER  - 

TY  - JOUR
AB  - We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic …
AU  - Do, T.
AU  - Nguyen, P.
AU  - Nguyen, M.
DA  - //
PY  - 2023
ST  - StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints
T2  - … for Computational Linguistics: ACL …
TI  - StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints
UR  - https://aclanthology.org/2023.findings-acl.648/
https://aclanthology.org/2023.findings-acl.648.pdf
ID  - 457
ER  - 

TY  - JOUR
AB  - … Recently, pre-trained NLP models, and BERT in particular, are … on active learning techniques for BERT-based classification, addressing … Our results demonstrate that AL can boost BERT …
AU  - Dor, L. E.
AU  - Halfon, A.
AU  - Gera, A.
AU  - Shnarch, E.
DA  - //
N1  - Cited By (since 2020): 128
PY  - 2020
ST  - Active learning for BERT: an empirical study
T2  - … Processing (EMNLP)
TI  - Active learning for BERT: an empirical study
UR  - https://aclanthology.org/2020.emnlp-main.638/
https://aclanthology.org/2020.emnlp-main.638.pdf
ID  - 324
ER  - 

TY  - JOUR
AB  - … BERT tokenizers, we converted them into actual repetitions of words. We remain with only words, punctuation, and pauses for input into the BERT … for multi-task biomedical named entity …
AU  - Duan, J.
AU  - Wei, F.
AU  - Liu, J.
AU  - Li, H.
AU  - Liu, T.
DA  - //
PY  - 2023
ST  - CDA: A Contrastive Data Augmentation Method for Alzheimer's Disease Detection
T2  - … Linguistics: ACL 2023
TI  - CDA: A Contrastive Data Augmentation Method for Alzheimer's Disease Detection
UR  - https://aclanthology.org/2023.findings-acl.114/
https://aclanthology.org/2023.findings-acl.114.pdf
ID  - 380
ER  - 

TY  - JOUR
AB  - Procedural text contains rich anaphoric phenomena, yet has not received much attention in NLP. To fill this gap, we investigate the textual properties of two types of procedural text, …
AU  - Fang, B.
AU  - Baldwin, T.
AU  - Verspoor, K.
DA  - //
N1  - Cited By (since 2022): 6
PY  - 2022
ST  - What does it take to bake a cake? The RecipeRef corpus and anaphora resolution in procedural text
T2  - … Computational Linguistics: ACL …
TI  - What does it take to bake a cake? The RecipeRef corpus and anaphora resolution in procedural text
UR  - https://aclanthology.org/2022.findings-acl.275/
https://aclanthology.org/2022.findings-acl.275.pdf
ID  - 462
ER  - 

TY  - JOUR
AB  - … When we use the state-of-theart language models, such as ELMo and BERT, instead of BiLSTM, we obtain prominent performance gains. This indicates the importance of semantics for …
AU  - Fei, H.
AU  - Ren, Y.
AU  - Ji, D.
DA  - //
N1  - Cited By (since 2020): 18
PY  - 2020
ST  - Improving text understanding via deep syntax-semantics communication
T2  - … for Computational Linguistics: EMNLP 2020
TI  - Improving text understanding via deep syntax-semantics communication
UR  - https://aclanthology.org/2020.findings-emnlp.8/
https://aclanthology.org/2020.findings-emnlp.8.pdf
ID  - 419
ER  - 

TY  - JOUR
AB  - … The fact that BioBERT does not perform better than base BERT suggests that clinical-specific pretraining is crucial and cannot be replaced by pretraining on general biomedical corpora. …
AU  - Feng, J.
AU  - Shaib, C.
AU  - Rudzicz, F.
DA  - //
N1  - Cited By (since 2020): 21
PY  - 2020
ST  - Explainable clinical decision support from text
T2  - … language processing (EMNLP)
TI  - Explainable clinical decision support from text
UR  - https://aclanthology.org/2020.emnlp-main.115/
https://aclanthology.org/2020.emnlp-main.115.pdf
ID  - 332
ER  - 

TY  - JOUR
AB  - … downstream tasks in Twitter and biomedical domains where OOV words often appear, even when the computed OOV embeddings are integrated into a BERT-based strong baseline. …
AU  - Fukuda, N.
AU  - Yoshinaga, N.
DA  - //
N1  - Cited By (since 2020): 8
PY  - 2020
ST  - Robust backed-off estimation of out-of-vocabulary embeddings
T2  - … Linguistics: EMNLP 2020
TI  - Robust backed-off estimation of out-of-vocabulary embeddings
UR  - https://aclanthology.org/2020.findings-emnlp.434/
https://aclanthology.org/2020.findings-emnlp.434.pdf
ID  - 316
ER  - 

TY  - JOUR
AB  - … We focus in this work on biomedical NEL, ie, identifying mentions referring to biomedical … utilizing language-specific BERT models instead of multilingual BERT (eg, swapping m-…
AU  - Galperin, R.
AU  - Schnapp, S.
AU  - Elhadad, M.
DA  - //
N1  - Cited By (since 2022): 2
PY  - 2022
ST  - Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning
T2  - … Linguistics: ACL 2022
TI  - Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning
UR  - https://aclanthology.org/2022.findings-acl.266/
https://aclanthology.org/2022.findings-acl.266.pdf
ID  - 319
ER  - 

TY  - JOUR
AB  - … After explaining how the method works globally (Section 2), we illustrate it with data from two shared tasks from the biomedical domain, one for multi-label classification and another for …
AU  - Gianola, L.
AU  - Boukkouri, H. El
AU  - Grouin, C.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - Differential evaluation: a qualitative analysis of natural language processing system behavior based upon data resistance to processing
T2  - … NLP Systems", EMNLP …
TI  - Differential evaluation: a qualitative analysis of natural language processing system behavior based upon data resistance to processing
UR  - https://hal.science/hal-03432331/
https://hal.science/hal-03432331/document
ID  - 421
ER  - 

TY  - JOUR
AB  - … RIch Self-Supervision (KRISS) for biomedical entity linking, by … standard datasets spanning biomedical literature and clinical … We find that in both BERT and RoBERTa the magnitude of …
AU  - Goldberg, Y.
AU  - Kozareva, Z.
AU  - Zhang, Y.
DA  - //
PY  - 2022
ST  - Findings of the Association for Computational Linguistics: EMNLP 2022
T2  - … Linguistics: EMNLP 2022
TI  - Findings of the Association for Computational Linguistics: EMNLP 2022
UR  - https://aclanthology.org/volumes/2022.findings-emnlp/
ID  - 459
ER  - 

TY  - JOUR
AB  - … Nevertheless, the subword tokenization of BERT-like encoders will split numbers that are … There is inconsistent evidence as to whether BERTbased models are superior to BiLSTMs in …
AU  - Göpfert, J.
AU  - Kuckertz, P.
AU  - Weinand, J.
DA  - //
N1  - Cited By (since 2022): 2
PY  - 2022
ST  - Measurement extraction with natural language processing: a review
T2  - … Linguistics: EMNLP …
TI  - Measurement extraction with natural language processing: a review
UR  - https://aclanthology.org/2022.findings-emnlp.161/
https://aclanthology.org/2022.findings-emnlp.161.pdf
ID  - 442
ER  - 

TY  - JOUR
AB  - … We use BERT as the main building block of our model and perform a comparison of two BERT variants pre-trained on general-purpose text BERTBASE and BERTSMALL, with three …
AU  - Grujicic, D.
AU  - Radevski, G.
AU  - Tuytelaars, T.
DA  - //
N1  - Cited By (since 2020): 5
PY  - 2020
ST  - Self-supervised context-aware COVID-19 document exploration through atlas grounding
T2  - ACL 2020 Workshop on …
TI  - Self-supervised context-aware COVID-19 document exploration through atlas grounding
UR  - https://openreview.net/forum?id=EC1vWkJXpjy
https://openreview.net/pdf?id=EC1vWkJXpjy
ID  - 365
ER  - 

TY  - JOUR
AB  - … BERT-based, ABSA classification model3. The latter model is composed of a HuggingFace implementation of a BERT … BERT post-training for review reading comprehension and aspect-…
AU  - Haber, A.
AU  - Waks, Z.
DA  - //
PY  - 2021
ST  - Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews
T2  - … for Computational Linguistics: EMNLP 2021
TI  - Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews
UR  - https://aclanthology.org/2021.findings-emnlp.217/
https://aclanthology.org/2021.findings-emnlp.217.pdf
ID  - 448
ER  - 

TY  - JOUR
AB  - … We select BioASQ and SearchQA to represent the biomedical and web-search domains as … Specifically, we conduct a secondstep pre-training on BERT for the extractive QA model …
AU  - Han, R.
AU  - Qi, P.
AU  - Zhang, Y.
AU  - Liu, L.
AU  - Burger, J.
DA  - //
PY  - 2023
ST  - RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering
T2  - … Linguistics: ACL …
TI  - RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering
UR  - https://aclanthology.org/2023.findings-acl.263/
https://aclanthology.org/2023.findings-acl.263.pdf
ID  - 370
ER  - 

TY  - JOUR
AB  - … ) shows that training a BERT-base model emits around 1,500 lbs of carbon dioxide and costs … Domain-specific language model pretraining for biomedical natural language processing. …
AU  - Hayet, I.
AU  - Yao, Z.
AU  - Luo, B.
DA  - //
PY  - 2022
ST  - Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings
T2  - … for Computational Linguistics: EMNLP …
TI  - Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings
UR  - https://aclanthology.org/2022.findings-emnlp.368/
https://aclanthology.org/2022.findings-emnlp.368.pdf
ID  - 443
ER  - 

TY  - JOUR
AB  - … The Biomedical domain is the only domain where we see the … and 12.24% on biomedical NER, biomedical relation extraction, … 2021), another BERTBase model targeting the biomedical …
AU  - Hong, Z.
AU  - Ajith, A.
AU  - Pauloski, J.
AU  - Duede, E.
DA  - //
N1  - Cited By (since 2023): 1
PY  - 2023
ST  - The diminishing returns of masked language models to science
T2  - … Linguistics: ACL …
TI  - The diminishing returns of masked language models to science
UR  - https://aclanthology.org/2023.findings-acl.82/
https://aclanthology.org/2023.findings-acl.82.pdf
ID  - 325
ER  - 

TY  - JOUR
AB  - … designed for language models such as BERT, it fits the DP … the original BERT model, our trained DP-BERT model does … In this section, we formulate the problem of training a DP BERT …
AU  - Hoory, S.
AU  - Feder, A.
AU  - Tendler, A.
AU  - Erell, S.
DA  - //
N1  - Cited By (since 2021): 31
PY  - 2021
ST  - Learning and evaluating a differentially private pre-trained language model
T2  - … Linguistics: EMNLP …
TI  - Learning and evaluating a differentially private pre-trained language model
UR  - https://aclanthology.org/2021.findings-emnlp.102/
https://aclanthology.org/2021.findings-emnlp.102.pdf
ID  - 341
ER  - 

TY  - JOUR
AB  - … 1means we replace the base encoder from RoBERTa-Base to BERT-Base, and all models are based on BERT-Base for a fair comparison. Note that perfect R@Y is not possible when …
AU  - Hu, X.
AU  - Wang, S.
AU  - Qin, X.
AU  - Lei, C.
AU  - Shen, Z.
DA  - //
PY  - 2023
ST  - Automatic Table Union Search with Tabular Representation Learning
T2  - … Linguistics: ACL …
TI  - Automatic Table Union Search with Tabular Representation Learning
UR  - https://aclanthology.org/2023.findings-acl.233/
https://aclanthology.org/2023.findings-acl.233.pdf
ID  - 399
ER  - 

TY  - JOUR
AB  - … differs from BERT-MRC in the following ways: (1) Different from BERT-MRC which uses bert-… However, BERTMRC only conducts entity decoding based on the span boundary. (3) We …
AU  - Huang, P.
AU  - Zhao, X.
AU  - Hu, M.
AU  - Fang, Y.
AU  - Li, X.
DA  - //
N1  - Cited By (since 2022): 3
PY  - 2022
ST  - Extract-select: A span selection framework for nested named entity recognition with generative adversarial training
T2  - … Linguistics: ACL 2022
TI  - Extract-select: A span selection framework for nested named entity recognition with generative adversarial training
UR  - https://aclanthology.org/2022.findings-acl.9/
https://aclanthology.org/2022.findings-acl.9.pdf
ID  - 401
ER  - 

TY  - BOOK
AB  - An existing domain taxonomy for normalizing content is often assumed when discussing approaches to information extraction, yet often in real-world scenarios there is none. When one …
AU  - Hungerford, J.
AU  - Chan, Y. S.
AU  - MacBride, J.
AU  - Gyori, B. M.
M1  - Query date: 2023-07-14 10:46:20
PB  - repository.arizona.edu
PY  - 2022
ST  - Taxonomy Builder: a Data-driven and User-centric Tool for Streamlining Taxonomy Construction
TI  - Taxonomy Builder: a Data-driven and User-centric Tool for Streamlining Taxonomy Construction
UR  - https://repository.arizona.edu/handle/10150/666484
https://repository.arizona.edu/bitstream/handle/10150/666484/2022.hcinlp-1.1.pdf?sequence=1
ID  - 481
ER  - 

TY  - JOUR
AB  - … Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for …
AU  - Iki, T.
AU  - Aizawa, A.
DA  - //
PY  - 2020
ST  - Language-Conditioned Feature Pyramids for Visual Selection Tasks
T2  - … for Computational Linguistics: EMNLP 2020
TI  - Language-Conditioned Feature Pyramids for Visual Selection Tasks
UR  - https://aclanthology.org/2020.findings-emnlp.420/
https://aclanthology.org/2020.findings-emnlp.420.pdf
ID  - 479
ER  - 

TY  - JOUR
AB  - Welcome to EMNLP-IJCNLP 2019 in Hong Kong! I hope that this will be a successful conference in the 25-year tradition of EMNLP and IJCNLP, as well as an enjoyable and enriching …
AU  - Inui, K.
AU  - Jiang, J.
AU  - Ng, V.
AU  - Wan, X.
DA  - //
N1  - Cited By (since 2019): 39
PY  - 2019
ST  - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language …
T2  - … Language Processing (EMNLP …
TI  - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language …
UR  - https://aclanthology.org/D19-1000.pdf
ID  - 480
ER  - 

TY  - JOUR
AB  - … The suicidal ideation does not appear in the prediction of BERT (Figure 3a). RoBERTa in … we do not consider the memorization issue of BERT here. While MentalRoBERTa outputs “die” …
AU  - Ji, S.
DA  - //
N1  - Cited By (since 2022): 2
PY  - 2022
ST  - Towards intention understanding in suicidal risk assessment with natural language processing
T2  - … the Association for Computational Linguistics: EMNLP …
TI  - Towards intention understanding in suicidal risk assessment with natural language processing
UR  - https://aclanthology.org/2022.findings-emnlp.297/
https://aclanthology.org/2022.findings-emnlp.297.pdf
ID  - 384
ER  - 

TY  - JOUR
AB  - … encoder representations from Transformers (BERT). We adopt a … We add the gates to the input of BiLSTM or BERT encoder, … information was carefully annotated on biomedical literature. …
AU  - Jiang, T.
AU  - Zhao, T.
AU  - Qin, B.
AU  - Liu, T.
AU  - Chawla, N.
DA  - //
N1  - Cited By (since 2019): 14
PY  - 2019
ST  - Multi-input multi-output sequence labeling for joint extraction of fact and condition tuples from scientific text
T2  - … Processing (EMNLP …
TI  - Multi-input multi-output sequence labeling for joint extraction of fact and condition tuples from scientific text
UR  - https://par.nsf.gov/servlets/purl/10167267
ID  - 372
ER  - 

TY  - JOUR
AB  - … We use the cased multilingual BERTBASE with 12 Transformer blocks, 768 hidden units, 12 self-attention heads, GELU activations, a dropout rate of 0.1 and learned positional em…
AU  - Jin, H.
AU  - Dong, T.
AU  - Hou, L.
AU  - Li, J.
AU  - Chen, H.
AU  - Dai, Z.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?
T2  - … Linguistics: ACL 2022
TI  - How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?
UR  - https://aclanthology.org/2022.findings-acl.243/
https://aclanthology.org/2022.findings-acl.243.pdf
ID  - 444
ER  - 

TY  - JOUR
AB  - … Our finetuned BERT classifier has 60.23% accuracy and 62.31% weighted F1. Detailed … for the BERT model, and 0.22749 for the TextBlob model. We apply the finetuned COVID BERT …
AU  - Jin, Z.
AU  - Peng, Z.
AU  - Vaidhya, T.
AU  - Schoelkopf, B.
DA  - //
N1  - Cited By (since 2021): 9
PY  - 2021
ST  - Mining the cause of political decision-making from social media: A case study of COVID-19 policies across the US states
T2  - … Linguistics: EMNLP …
TI  - Mining the cause of political decision-making from social media: A case study of COVID-19 policies across the US states
UR  - https://aclanthology.org/2021.findings-emnlp.27/
https://aclanthology.org/2021.findings-emnlp.27.pdf
ID  - 393
ER  - 

TY  - JOUR
AB  - … To offset the effects of freezing BERT, we pass the context representations through a … To test our performance in the biomedical domain, we use data from the JNLPBA 2004 shared task …
AU  - Jörke, M.
AU  - Gillick, J.
AU  - Sims, M.
AU  - Bamman, D.
DA  - //
N1  - Cited By (since 2020): 3
PY  - 2020
ST  - Attending to long-distance document context for sequence labeling
T2  - … Linguistics: EMNLP 2020
TI  - Attending to long-distance document context for sequence labeling
UR  - https://par.nsf.gov/biblio/10274011
https://par.nsf.gov/servlets/purl/10274011
ID  - 351
ER  - 

TY  - JOUR
AB  - … from the PubMed repository of biomedical research papers. The … a BERT-based model that contains recurrent layers as well. In the appendix, we explore the effect of fine-tuning BERT …
AU  - Kabbara, J.
AU  - Cheung, J. C. K.
DA  - //
N1  - Cited By (since 2021): 3
PY  - 2021
ST  - Post-editing extractive summaries by definiteness prediction
T2  - … for Computational Linguistics: EMNLP …
TI  - Post-editing extractive summaries by definiteness prediction
UR  - https://aclanthology.org/2021.findings-emnlp.312/
https://aclanthology.org/2021.findings-emnlp.312.pdf
ID  - 369
ER  - 

TY  - JOUR
AB  - … utterance using BERT, ie, [hBERT … BERT model is trained on a general web corpus such as Wikipedia, it might not generalize well to our corpus. Therefore, we further fine tune the BERT …
AU  - Khosla, S.
AU  - Vashishth, S.
AU  - Lehman, J. F.
DA  - //
N1  - Cited By (since 2020): 3
PY  - 2020
ST  - MedFilter: improving extraction of task-relevant utterances through integration of discourse structure and ontological knowledge
T2  - … Processing (EMNLP)
TI  - MedFilter: improving extraction of task-relevant utterances through integration of discourse structure and ontological knowledge
UR  - https://aclanthology.org/2020.emnlp-main.626/
https://aclanthology.org/2020.emnlp-main.626.pdf
ID  - 432
ER  - 

TY  - JOUR
AB  - … This study performs BERT-based analysis, which is a representative contextualized lan… we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT …
AU  - Kim, A.
AU  - Yoon, S.
DA  - //
N1  - Cited By (since 2021): 3
PY  - 2021
ST  - Corporate bankruptcy prediction with domain-adapted BERT
T2  - EMNLP 2021, 3rd Workshop on ECONLP
TI  - Corporate bankruptcy prediction with domain-adapted BERT
UR  - https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4191690
https://aclanthology.org/2021.econlp-1.4.pdf
ID  - 339
ER  - 

TY  - JOUR
AB  - … vestigate biases in biomedical masked language models with manually curated prompts. The experimental results show that BERT is less biased than the biomedical models in race …
AU  - Kim, M.
AU  - Kim, J.
AU  - Johnson, K.
DA  - //
PY  - 2023
ST  - Race, Gender, and Age Biases in Biomedical Masked Language Models
T2  - … for Computational Linguistics: ACL …
TI  - Race, Gender, and Age Biases in Biomedical Masked Language Models
UR  - https://aclanthology.org/2023.findings-acl.749/
https://aclanthology.org/2023.findings-acl.749.pdf
ID  - 309
ER  - 

TY  - JOUR
AB  - … We set LT to be 12,000 words (∼20,000 BERT tokens) for computational and memory constraints, and because less than 10% of the transcripts in the sample are longer than that length…
AU  - Koval, R.
AU  - Andrews, N.
AU  - Yan, X.
DA  - //
PY  - 2023
ST  - Forecasting Earnings Surprises from Conference Call Transcripts
T2  - … for Computational Linguistics: ACL …
TI  - Forecasting Earnings Surprises from Conference Call Transcripts
UR  - https://aclanthology.org/2023.findings-acl.520/
https://aclanthology.org/2023.findings-acl.520.pdf
ID  - 406
ER  - 

TY  - JOUR
AB  - … For both models, we employ the pretrained BERT model (ie, the bert-base-cased version) to encode input texts. Besides, motivated by the language difference between historical and …
AU  - Lai, V. D.
AU  - Nguyen, M. Van
AU  - Kaufman, H.
DA  - //
N1  - Cited By (since 2021): 16
PY  - 2021
ST  - Event extraction from historical texts: A new dataset for black rebellions
T2  - … Linguistics: ACL …
TI  - Event extraction from historical texts: A new dataset for black rebellions
UR  - https://aclanthology.org/2021.findings-acl.211.pdf
ID  - 362
ER  - 

TY  - BOOK
AB  - This paper presents the Source-Free Domain Adaptation shared task held within SemEval-2021. The aim of the task was to explore adaptation of machine-learning models in the face of …
AU  - Laparra, E.
AU  - Su, X.
AU  - Zhao, Y.
AU  - Uzuner, Ö
AU  - Miller, T. A.
M1  - Query date: 2023-07-14 10:46:20
N1  - Cited By (since 2021): 7
PB  - repository.arizona.edu
PY  - 2021
ST  - Semeval-2021 task 10: source-free domain adaptation for semantic processing
TI  - Semeval-2021 task 10: source-free domain adaptation for semantic processing
UR  - https://repository.arizona.edu/handle/10150/667110
https://repository.arizona.edu/bitstream/handle/10150/667110/2021.semeval-1.42.pdf?sequence=1
ID  - 454
ER  - 

TY  - JOUR
AB  - … unannotated biomedical texts such as PubMed abstracts and have been proven useful in biomedical … By fusion of knowledge source and text over BERT-base, the performance is further …
AU  - Li, D.
AU  - Hu, B.
AU  - Chen, Q.
AU  - Peng, W.
DA  - //
N1  - Cited By (since 2020): 24
PY  - 2020
ST  - Towards medical machine reading comprehension with structural knowledge and plain text
T2  - … processing (EMNLP)
TI  - Towards medical machine reading comprehension with structural knowledge and plain text
UR  - https://aclanthology.org/2020.emnlp-main.111/
https://aclanthology.org/2020.emnlp-main.111.pdf
ID  - 346
ER  - 

TY  - JOUR
AB  - … For two tasks, we exploit the pre-trained biomedical language representation model (ie, BERT) in the pipe of information extraction for the collection of mutation-disease knowledge from …
AU  - Li, D.
AU  - Xiong, Y.
AU  - Hu, B.
AU  - Du, H.
AU  - Tang, B.
DA  - //
N1  - Cited By (since 2019): 1
PY  - 2019
ST  - DXHITSZ at BioNLP-OST 2019: Trigger Word Detection and Thematic Role Identification via BERT and Multitask Learning
T2  - EMNLP-IJCNLP 2019
TI  - DXHITSZ at BioNLP-OST 2019: Trigger Word Detection and Thematic Role Identification via BERT and Multitask Learning
UR  - https://aclanthology.org/D19-57.pdf#page=86
ID  - 312
ER  - 

TY  - JOUR
AB  - … Table 3 shows the results on two biomedical datasets. Here… BERT-base model as our encoder for comparing with some baselines. We use the initial learning rate 1e-5 to fine-tune BERT…
AU  - Li, J.
AU  - Xu, K.
AU  - Li, F.
AU  - Fei, H.
AU  - Ren, Y.
AU  - Ji, D.
DA  - //
N1  - Cited By (since 2021): 31
PY  - 2021
ST  - MRN: A locally and globally mention-based reasoning network for document-level relation extraction
T2  - … Linguistics: ACL-IJCNLP 2021
TI  - MRN: A locally and globally mention-based reasoning network for document-level relation extraction
UR  - https://aclanthology.org/2021.findings-acl.117.pdf
ID  - 355
ER  - 

TY  - JOUR
AB  - … The basic BERT model incorrectly predicts the polarity of the example. It might be confused of BERT … Fine-grained information extraction from biomedical literature based on knowledge-…
AU  - Li, S.
AU  - Wang, Z.
AU  - Jiang, X.
AU  - Zhou, G.
DA  - //
PY  - 2022
ST  - Cross-Domain Sentiment Classification using Semantic Representation
T2  - … Linguistics: EMNLP 2022
TI  - Cross-Domain Sentiment Classification using Semantic Representation
UR  - https://aclanthology.org/2022.findings-emnlp.22/
https://aclanthology.org/2022.findings-emnlp.22.pdf
ID  - 377
ER  - 

TY  - JOUR
AB  - … Finally, we see that HT-bert, as well as the other BERT-based models, gives mediocre … a candidate in the list of words predicted by HT-bert ‘fill_mask’. The dataset is split into train, val…
AU  - Li, Y.
AU  - Nair, P.
AU  - Pelrine, K.
AU  - Rabbany, R.
DA  - //
N1  - Cited By (since 2022): 2
PY  - 2022
ST  - Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking
T2  - … Linguistics: ACL 2022
TI  - Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking
UR  - https://aclanthology.org/2022.findings-acl.225/
https://aclanthology.org/2022.findings-acl.225.pdf
ID  - 414
ER  - 

TY  - JOUR
AB  - … For the teacher model, we also consider BERT-large and BERT-base. See Table 4… Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten …
AU  - Limkonchotiwat, P.
AU  - Ponwitayarat, W.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation
T2  - … Linguistics: EMNLP …
TI  - ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation
UR  - https://aclanthology.org/2022.findings-emnlp.483/
https://aclanthology.org/2022.findings-emnlp.483.pdf
ID  - 348
ER  - 

TY  - BOOK
AB  - … with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along … -centric task in the biomedical domain. The language addressed in this work is English. …
AU  - Lin, C.
AU  - Miller, T.
AU  - Dligach, D.
AU  - Bethard, S.
AU  - Savova, G.
M1  - Query date: 2023-07-14 10:46:20
N1  - Cited By (since 2021): 26
PB  - repository.arizona.edu
PY  - 2021
ST  - EntityBERT: Entity-centric masking strategy for model pretraining for the clinical domain
TI  - EntityBERT: Entity-centric masking strategy for model pretraining for the clinical domain
UR  - https://repository.arizona.edu/handle/10150/663579
https://repository.arizona.edu/bitstream/handle/10150/663579/2021_bionlp_1_21.pdf?sequence=1
ID  - 315
ER  - 

TY  - BOOK
AB  - … by taking advantage of the rich BERT representations. However, that … (3) We use the same BERT encoding of the input instance for … 2019), we can encode every entity, Ei, by its BERT …
AU  - Lin, C.
AU  - Miller, T.
AU  - Dligach, D.
AU  - Sadeque, F.
AU  - Bethard, S.
M1  - Query date: 2023-07-14 10:46:20
N1  - Cited By (since 2020): 18
PB  - repository.arizona.edu
PY  - 2020
ST  - A BERT-based one-pass multi-task model for clinical temporal relation extraction
TI  - A BERT-based one-pass multi-task model for clinical temporal relation extraction
UR  - https://repository.arizona.edu/handle/10150/657415
https://repository.arizona.edu/bitstream/handle/10150/657415/2020.bionlp-1.7.pdf?sequence=1
ID  - 326
ER  - 

TY  - JOUR
AB  - … further introduce BERT into models by replacing the word embedding with BERT representa… Biomedical named entity recognition using conditional random fields and rich feature sets. …
AU  - Lin, H.
AU  - Lu, Y.
AU  - Han, X.
AU  - Sun, L.
AU  - Dong, B.
DA  - //
N1  - Cited By (since 2019): 21
PY  - 2019
ST  - Gazetteer-enhanced attentive neural networks for named entity recognition
T2  - … Processing (EMNLP …
TI  - Gazetteer-enhanced attentive neural networks for named entity recognition
UR  - https://aclanthology.org/D19-1646/
https://aclanthology.org/D19-1646.pdf
ID  - 386
ER  - 

TY  - JOUR
AB  - … In this work, we identify and quantify two bias modes in BERT’s prompt-based predictions, leveraging the availability bias and the framing effect on biomedical drug-drug interaction …
AU  - Lin, R.
AU  - Ng, H. T.
DA  - //
PY  - 2023
ST  - Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting
T2  - … the Association for Computational Linguistics: ACL …
TI  - Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting
UR  - https://aclanthology.org/2023.findings-acl.324/
https://aclanthology.org/2023.findings-acl.324.pdf
ID  - 352
ER  - 

TY  - JOUR
AB  - … The distributed features are those generated by BERT. For words being split into multiple … The GENIA corpus is the primary collection of biomedical abstracts, whose texts exhibit a lower …
AU  - Ling, Z.
AU  - Zheng, X.
AU  - Xu, J.
AU  - Lin, J.
AU  - Chang, K. W.
DA  - //
PY  - 2023
ST  - Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations
T2  - … Linguistics: ACL …
TI  - Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations
UR  - https://aclanthology.org/2023.findings-acl.726/
https://aclanthology.org/2023.findings-acl.726.pdf
ID  - 407
ER  - 

TY  - JOUR
AB  - … training to continuously train BERT for better reasoning ability towards the COVID-FACT. For the claim and evidence<c, e>, we optimize BERT model with supervisions from SCIFACT: …
AU  - Liu, Z.
AU  - Xiong, C.
AU  - Dai, Z.
AU  - Sun, S.
AU  - Sun, M.
DA  - //
N1  - Cited By (since 2020): 12
PY  - 2020
ST  - Adapting open domain fact extraction and verification to COVID-FACT through in-domain language modeling
T2  - … Linguistics: EMNLP 2020
TI  - Adapting open domain fact extraction and verification to COVID-FACT through in-domain language modeling
UR  - https://aclanthology.org/2020.findings-emnlp.216/
https://aclanthology.org/2020.findings-emnlp.216.pdf
ID  - 349
ER  - 

TY  - JOUR
AB  - … Second, the impact of using a BERTbased classifier. Labeled training data is expensive to acquire as it requires domain expertise. However, as policy investigators review Benefit Rules…
AU  - Lopez, V.
AU  - Yadav, N.
AU  - Picco, G.
AU  - Vejsbjerg, I.
DA  - //
PY  - 2021
ST  - Towards Protecting Vital Healthcare Programs by Extracting Actionable Knowledge from Policy
T2  - … Linguistics: ACL …
TI  - Towards Protecting Vital Healthcare Programs by Extracting Actionable Knowledge from Policy
UR  - https://aclanthology.org/2021.findings-acl.308.pdf
ID  - 438
ER  - 

TY  - JOUR
AB  - Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of …
AU  - Lorenzo, A. C. M.
AU  - Cabot, P. L. H.
AU  - Navigli, R.
DA  - //
PY  - 2023
ST  - Cross-lingual AMR Aligner: Paying Attention to Cross-Attention
T2  - … Linguistics: ACL 2023
TI  - Cross-lingual AMR Aligner: Paying Attention to Cross-Attention
UR  - https://aclanthology.org/2023.findings-acl.109/
https://aclanthology.org/2023.findings-acl.109.pdf
ID  - 473
ER  - 

TY  - JOUR
AB  - … Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and Shared …
AU  - Lu, Q.
AU  - Dou, D.
AU  - Nguyen, T.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - ClinicalT5: A generative language model for clinical text
T2  - … for Computational Linguistics: EMNLP …
TI  - ClinicalT5: A generative language model for clinical text
UR  - https://aclanthology.org/2022.findings-emnlp.398/
https://aclanthology.org/2022.findings-emnlp.398.pdf
ID  - 318
ER  - 

TY  - JOUR
AB  - … biomedical tasks, as reflected in DAKI-BERT, DAKI-RoBERTa and DAKI-ALBERT, demonstrating the effectiveness of the architecture. Moreover, although DAKIBERT outperforms BERT …
AU  - Lu, Q.
AU  - Dou, D.
AU  - Nguyen, T. H.
DA  - //
N1  - Cited By (since 2021): 12
PY  - 2021
ST  - Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models
T2  - … for Computational Linguistics: EMNLP …
TI  - Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models
UR  - https://aclanthology.org/2021.findings-emnlp.325/
https://aclanthology.org/2021.findings-emnlp.325.pdf
ID  - 308
ER  - 

TY  - JOUR
AB  - … In particular, Fusion uses an LSTM network to stack the segments of Bert embeddings from truncated clinical notes as iputs, and takes an attention-based soft-pooling approach to …
AU  - Luo, J.
AU  - Xiao, C.
AU  - Glass, L.
AU  - Sun, J.
AU  - Ma, F.
DA  - //
N1  - Cited By (since 2021): 13
PY  - 2021
ST  - Fusion: towards automated ICD coding via feature compression
T2  - … Linguistics: ACL-IJCNLP …
TI  - Fusion: towards automated ICD coding via feature compression
UR  - https://aclanthology.org/2021.findings-acl.184.pdf
ID  - 429
ER  - 

TY  - JOUR
AB  - … Additionally, Figure 4 shows that the LM Loss is 6.9x larger than the initial BERT model, and we … biomedical field, published in the English language, and had both a title and an abstract. …
AU  - Lv, B.
AU  - Liu, X.
AU  - Dai, S.
AU  - Liu, N.
AU  - Yang, F.
AU  - Luo, P.
DA  - //
PY  - 2023
ST  - DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction
T2  - … Linguistics: ACL 2023
TI  - DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction
UR  - https://aclanthology.org/2023.findings-acl.339/
https://aclanthology.org/2023.findings-acl.339.pdf
ID  - 404
ER  - 

TY  - JOUR
AB  - … Understanding these concepts requires biomedical expertise. The sheer volume of new information … We use frozen weights from BERT and only train the token classification layer. …
AU  - Lymperopoulos, P.
AU  - Qiu, H.
AU  - Min, B.
DA  - //
N1  - Cited By (since 2020): 7
PY  - 2020
ST  - Concept wikification for covid-19
T2  - … for COVID-19 (Part 2) at EMNLP …
TI  - Concept wikification for covid-19
UR  - https://aclanthology.org/2020.nlpcovid19-2.29/
https://aclanthology.org/2020.nlpcovid19-2.29.pdf
ID  - 343
ER  - 

TY  - JOUR
AB  - … We derive the transactivity scores for subsequent analyses using BERT, and defer details of … For each paragraph-segment pair, we classify whether it is transactive using the BERT …
AU  - Manzoor, E.
AU  - Jo, Y.
AU  - Montgomery, A.
DA  - //
PY  - 2022
ST  - Status Biases in Deliberation Online: Evidence from a Randomized Experiment on ChangeMyView
T2  - … Linguistics: EMNLP 2022
TI  - Status Biases in Deliberation Online: Evidence from a Randomized Experiment on ChangeMyView
UR  - https://aclanthology.org/2022.findings-emnlp.474/
https://aclanthology.org/2022.findings-emnlp.474.pdf
ID  - 423
ER  - 

TY  - JOUR
AB  - … neural network, and we show that it outperforms several strong BERT-based baselines. … BERT-Joint. We use the layers for both tasks in the BERT baseline, Lg1 and Lg2 , and we train …
AU  - Martino, G. Da San
AU  - Yu, S.
AU  - Barrón-Cedeno, A.
DA  - //
N1  - Cited By (since 2019): 228
PY  - 2019
ST  - Fine-grained analysis of propaganda in news article
T2  - … processing (EMNLP …
TI  - Fine-grained analysis of propaganda in news article
UR  - https://aclanthology.org/D19-1565/
https://aclanthology.org/D19-1565.pdf
ID  - 356
ER  - 

TY  - JOUR
AB  - … This could happen when: (1) The task requires specialized expertise, like biomedical … In Proceedings of the 21st Workshop on Biomedical Language Processing, pages 323–329, Dublin…
AU  - Mehta, M.
AU  - Srikumar, V.
DA  - //
PY  - 2023
ST  - Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS
T2  - … for Computational Linguistics: ACL 2023
TI  - Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS
UR  - https://aclanthology.org/2023.findings-acl.696/
https://aclanthology.org/2023.findings-acl.696.pdf
ID  - 461
ER  - 

TY  - JOUR
AB  - … The comparison between META-CNN, METABERT, and their respective ablated versions META-CNN-NoMeta, META-BERT-NoMeta demonstrate that our proposed approach provides …
AU  - Mekala, D.
AU  - Zhang, X.
AU  - Shang, J.
DA  - //
N1  - Cited By (since 2020): 32
PY  - 2020
ST  - Meta: Metadata-empowered weak supervision for text classification
T2  - … in Natural Language Processing (EMNLP …
TI  - Meta: Metadata-empowered weak supervision for text classification
UR  - https://par.nsf.gov/biblio/10250427
https://par.nsf.gov/servlets/purl/10250427
ID  - 378
ER  - 

TY  - JOUR
AB  - Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training …
AU  - Menon, R. R.
AU  - Wang, B.
AU  - Araki, J.
AU  - Zhou, Z.
DA  - //
PY  - 2023
ST  - CoAug: Combining Augmentation of Labels and Labelling Rules
T2  - … Linguistics: ACL 2023
TI  - CoAug: Combining Augmentation of Labels and Labelling Rules
UR  - https://aclanthology.org/2023.findings-acl.577/
https://aclanthology.org/2023.findings-acl.577.pdf
ID  - 449
ER  - 

TY  - JOUR
AB  - We introduce MedicalSum, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the Unified …
AU  - Michalopoulos, G.
AU  - Williams, K.
AU  - Singh, G.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - MedicalSum: A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations
T2  - … Linguistics: EMNLP …
TI  - MedicalSum: A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations
UR  - https://aclanthology.org/2022.findings-emnlp.349/
https://aclanthology.org/2022.findings-emnlp.349.pdf
ID  - 431
ER  - 

TY  - JOUR
AB  - Welcome to the Findings of ACL: EMNLP 2021! To continue the success of Findings of ACL series, we followed EMNLP 2020’s initiative to produce this accompanying volume, for …
AU  - Moens, M. F.
AU  - Huang, X. J.
AU  - Specia, L.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Findings of the association for computational linguistics: Emnlp 2021
T2  - … Linguistics: EMNLP 2021
TI  - Findings of the association for computational linguistics: Emnlp 2021
UR  - https://aclanthology.org/2021.findings-emnlp.0.pdf
ID  - 435
ER  - 

TY  - JOUR
AB  - … We present COVID-QA, a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. To …
AU  - Möller, T.
AU  - Reina, A.
AU  - Jayakumar, R.
DA  - //
N1  - Cited By (since 2020): 56
PY  - 2020
ST  - COVID-QA: A question answering dataset for COVID-19
T2  - ACL 2020 Workshop on …
TI  - COVID-QA: A question answering dataset for COVID-19
UR  - https://openreview.net/forum?id=JENSKEEzsoU
https://openreview.net/pdf?id=JENSKEEzsoU
ID  - 337
ER  - 

TY  - JOUR
AB  - … datasets are used to develop NER models in the biomedical domain. In practice, there is not a … During training, [CLS]context[SEP]TD/TM hypothesis[SEP] is given as input to the BERT-…
AU  - Mondal, I.
AU  - Hou, Y.
AU  - Jochim, C.
DA  - //
N1  - Cited By (since 2021): 14
PY  - 2021
ST  - End-to-end construction of NLP knowledge graph
T2  - … for Computational Linguistics: ACL …
TI  - End-to-end construction of NLP knowledge graph
UR  - https://aclanthology.org/2021.findings-acl.165.pdf
ID  - 373
ER  - 

TY  - JOUR
AB  - … Medical subject headings used to search the biomedical literature. Journal of the American … Bert: Pre-training of deep bidirectional transformers for language understanding. In …
AU  - Nesterov, A.
AU  - Zubkova, G.
AU  - Miftahutdinov, Z.
DA  - //
N1  - Cited By (since 2022): 2
PY  - 2022
ST  - RuCCoN: clinical concept normalization in Russian
T2  - … Linguistics: ACL …
TI  - RuCCoN: clinical concept normalization in Russian
UR  - https://aclanthology.org/2022.findings-acl.21/
https://aclanthology.org/2022.findings-acl.21.pdf
ID  - 338
ER  - 

TY  - JOUR
AB  - … and BERT represent the fully connected layer and the BERT … , we used the pretrained Japanese BERT model 6. We have … rate of 2.0 × 10−5 for the BERT layer and 2.0 × 10−3 for the …
AU  - Nishino, T.
AU  - Ozaki, R.
AU  - Momoki, Y.
AU  - Taniguchi, T.
DA  - //
N1  - Cited By (since 2020): 15
PY  - 2020
ST  - Reinforcement learning with imbalanced dataset for data-to-text medical report generation
T2  - … Linguistics: EMNLP …
TI  - Reinforcement learning with imbalanced dataset for data-to-text medical report generation
UR  - https://aclanthology.org/2020.findings-emnlp.202/
https://aclanthology.org/2020.findings-emnlp.202.pdf
ID  - 381
ER  - 

TY  - JOUR
AB  - … Next, we extend the model (§ 4.3) by using our BERT-based encoders that dynamically predict entity embeddings from descriptions and relevant sentences (§ 4.2), thus enabling the …
AU  - Oba, D.
AU  - Yamada, I.
AU  - Yoshinaga, N.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Entity Embedding Completion for Wide-Coverage Entity Disambiguation
T2  - … Linguistics: EMNLP 2022
TI  - Entity Embedding Completion for Wide-Coverage Entity Disambiguation
UR  - https://aclanthology.org/2022.findings-emnlp.472/
https://aclanthology.org/2022.findings-emnlp.472.pdf
ID  - 403
ER  - 

TY  - JOUR
AB  - … 2021) introduces a BERT-based method that utilizes pretraining on the ROCO dataset with a masked language modeling objective. The model predicts answers by performing an …
AU  - Ossowski, T.
AU  - Hu, J.
DA  - //
PY  - 2023
ST  - Retrieving Multimodal Prompts for Generative Visual Question Answering
T2  - … Association for Computational Linguistics: ACL …
TI  - Retrieving Multimodal Prompts for Generative Visual Question Answering
UR  - https://aclanthology.org/2023.findings-acl.158/
https://aclanthology.org/2023.findings-acl.158.pdf
ID  - 451
ER  - 

TY  - JOUR
AB  - … on BERT, but trained on a large corpus of scientific text, including text from biomedical domain (… This dataset has been annotated by 15 biomedical experts taking 147 articles from the …
AU  - Otegi, A.
AU  - Campos, J. A.
AU  - Azkune, G.
AU  - Soroa, A.
DA  - //
N1  - Cited By (since 2020): 9
PY  - 2020
ST  - Automatic evaluation vs. user preference in neural textual QuestionAnswering over COVID-19 scientific literature
T2  - … -19 (Part 2) at EMNLP …
TI  - Automatic evaluation vs. user preference in neural textual QuestionAnswering over COVID-19 scientific literature
UR  - https://openreview.net/forum?id=R2Vm-G0ios
https://openreview.net/pdf?id=R2Vm-G0ios
ID  - 360
ER  - 

TY  - JOUR
AB  - … The benchmark dataset we present is based on biomedical literature for major eye diseases such as glaucoma and macular degeneration. We chose to focus the dataset around a …
AU  - Otmakhova, J.
AU  - Verspoor, K.
AU  - Baldwin, T.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - M3: Multi-level dataset for Multi-document summarisation of Medical studies
T2  - … Linguistics: EMNLP …
TI  - M3: Multi-level dataset for Multi-document summarisation of Medical studies
UR  - https://aclanthology.org/2022.findings-emnlp.286/
https://aclanthology.org/2022.findings-emnlp.286.pdf
ID  - 345
ER  - 

TY  - JOUR
AB  - … token embeddings z used to calculate (8) and (11) are obtained from a trainable single layer bidirectional LSTM that uses contextual word embeddings from pre-trained BERT models …
AU  - Parker, J.
AU  - Yu, S.
DA  - //
N1  - Cited By (since 2021): 4
PY  - 2021
ST  - Named entity recognition through deep representation learning and weak supervision
T2  - … Association for Computational Linguistics: ACL …
TI  - Named entity recognition through deep representation learning and weak supervision
UR  - https://aclanthology.org/2021.findings-acl.335.pdf
ID  - 445
ER  - 

TY  - JOUR
AB  - … 2014) layer to better adapt BERT embeddings to the task to obtain token representations (e1 ···el… We also observe that fine-tuning the BERT model requires learning rates comparable in …
AU  - Patra, B.
AU  - Moniz, J. R. A.
DA  - //
N1  - Cited By (since 2019): 2
PY  - 2019
ST  - Weakly supervised attention networks for entity recognition
T2  - … on Natural Language Processing (EMNLP …
TI  - Weakly supervised attention networks for entity recognition
UR  - https://aclanthology.org/D19-1652/
https://aclanthology.org/D19-1652.pdf
ID  - 424
ER  - 

TY  - JOUR
AB  - … BERT have been the driving force behind recent improvements across many NLP tasks. However, BERT … For our case of combining BERT with the injection sequence, we can therefore …
AU  - Peinelt, N.
AU  - Rei, M.
AU  - Liakata, M.
DA  - //
N1  - Cited By (since 2021): 1
PY  - 2021
ST  - GiBERT: Enhancing BERT with linguistic information using a lightweight gated injection method
T2  - … Computational Linguistics: EMNLP …
TI  - GiBERT: Enhancing BERT with linguistic information using a lightweight gated injection method
UR  - https://aclanthology.org/2021.findings-emnlp.200/
https://aclanthology.org/2021.findings-emnlp.200.pdf
ID  - 334
ER  - 

TY  - JOUR
AB  - … In addition, enhanced by the pretrained BERT model, our … annotated data it requires for the BERT model to achieve similar … Biomedical named entity recognition using conditional …
AU  - Peng, M.
AU  - Ma, R.
AU  - Zhang, Q.
AU  - Zhao, L.
AU  - Wei, M.
DA  - //
N1  - Cited By (since 2020): 4
PY  - 2020
ST  - Toward recognizing more entity types in NER: an efficient implementation using only entity lexicons
T2  - … Linguistics: EMNLP …
TI  - Toward recognizing more entity types in NER: an efficient implementation using only entity lexicons
UR  - https://aclanthology.org/2020.findings-emnlp.60/
https://aclanthology.org/2020.findings-emnlp.60.pdf
ID  - 342
ER  - 

TY  - JOUR
AB  - … The PLM of the learning model was implemented with bert-base-uncased. Results of BERT… In similar, "biomedical engineering" occurs 37 times in positive class training data and 7 …
AU  - Peng, M.
AU  - Sun, M.
DA  - //
PY  - 2023
ST  - NormNet: Normalize Noun Phrases for More Robust NLP
T2  - … Association for Computational Linguistics: ACL …
TI  - NormNet: Normalize Noun Phrases for More Robust NLP
UR  - https://aclanthology.org/2023.findings-acl.136/
https://aclanthology.org/2023.findings-acl.136.pdf
ID  - 390
ER  - 

TY  - JOUR
AB  - … models (using multilingual BERT or XLM-R) and monolingual models (English BERT and Roberta). Figure 2 provides cross-domain results of training only on English subdomains. …
AU  - Plank, B.
DA  - //
N1  - Cited By (since 2021): 1
PY  - 2021
ST  - Cross-lingual cross-domain nested named entity evaluation on English web texts
T2  - Findings of ACL 2021
TI  - Cross-lingual cross-domain nested named entity evaluation on English web texts
UR  - https://pure.itu.dk/ws/files/86213673/crossNNER_1_.pdf
ID  - 405
ER  - 

TY  - JOUR
AB  - … We use BERT and RoBERTa to uncover a single word and T5 to uncover variablelength … We cannot use BERT based simple approach for free paraphrases. We use T5 model to …
AU  - Ponkiya, G.
AU  - Murthy, R.
AU  - Bhattacharyya, P.
DA  - //
N1  - Cited By (since 2020): 12
PY  - 2020
ST  - Looking inside noun compounds: Unsupervised prepositional and free paraphrasing
T2  - … linguistics: EMNLP …
TI  - Looking inside noun compounds: Unsupervised prepositional and free paraphrasing
UR  - https://aclanthology.org/2020.findings-emnlp.386/
https://aclanthology.org/2020.findings-emnlp.386.pdf
ID  - 363
ER  - 

TY  - JOUR
AB  - Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we …
AU  - Pu, R.
AU  - Li, Y.
AU  - Wang, S.
AU  - Li, D.
AU  - Zheng, J.
DA  - //
PY  - 2023
ST  - Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph
T2  - … Linguistics: ACL 2023
TI  - Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph
UR  - https://aclanthology.org/2023.findings-acl.655/
https://aclanthology.org/2023.findings-acl.655.pdf
ID  - 439
ER  - 

TY  - JOUR
AB  - Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various …
AU  - Qi, L.
AU  - Lv, S.
AU  - Li, H.
AU  - Liu, J.
AU  - Zhang, Y.
AU  - She, Q.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - Dureadervis: A: A chinese dataset for open-domain document visual question answering
T2  - … Linguistics: ACL …
TI  - Dureadervis: A: A chinese dataset for open-domain document visual question answering
UR  - https://aclanthology.org/2022.findings-acl.105/
https://aclanthology.org/2022.findings-acl.105.pdf
ID  - 394
ER  - 

TY  - JOUR
AB  - … In addition, to study the effect of different sizes of backbone language models, we use bert-base and bert-large models with 110M parameters and 340M parameters, respectively. …
AU  - Qian, F.
AU  - Han, J.
AU  - He, Y.
AU  - Zheng, T.
DA  - //
PY  - 2023
ST  - Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis
T2  - … Linguistics: ACL 2023
TI  - Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis
UR  - https://aclanthology.org/2023.findings-acl.821/
https://aclanthology.org/2023.findings-acl.821.pdf
ID  - 413
ER  - 

TY  - JOUR
AB  - … Importantly, we also provide first results on biomedical event extraction without gold entity … We used BioBERT-Base 1.1 as our BERT model for experiments, since it provides state-of-…
AU  - Ramponi, A.
AU  - Goot, R. van der
AU  - Lombardo, R.
DA  - //
N1  - Cited By (since 2020): 37
PY  - 2020
ST  - Biomedical event extraction as sequence labeling
T2  - … processing (emnlp)
TI  - Biomedical event extraction as sequence labeling
UR  - https://aclanthology.org/2020.emnlp-main.431/
https://aclanthology.org/2020.emnlp-main.431.pdf
ID  - 313
ER  - 

TY  - JOUR
AB  - … For our BERT experiments we use a PyTorch implementation5 with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam optimizer, a batch …
AU  - Rosenthal, S.
AU  - Barker, K.
AU  - Liang, Z.
DA  - //
N1  - Cited By (since 2019): 17
PY  - 2019
ST  - Leveraging medical literature for section prediction in electronic health records
T2  - … Language Processing (EMNLP …
TI  - Leveraging medical literature for section prediction in electronic health records
UR  - https://aclanthology.org/D19-1492/
https://aclanthology.org/D19-1492.pdf
ID  - 364
ER  - 

TY  - JOUR
AB  - … who have expertise in biomedical NLP). The annotators … different baseline models including BERT, SciBERT, BioBERT, … by using in-domain BERT-based language models such as …
AU  - Sarrouti, M.
AU  - Abacha, A. B.
AU  - M'Rabet, Y.
DA  - //
N1  - Cited By (since 2021): 26
PY  - 2021
ST  - Evidence-based fact-checking of health-related claims
T2  - … Linguistics: EMNLP …
TI  - Evidence-based fact-checking of health-related claims
UR  - https://aclanthology.org/2021.findings-emnlp.297/
https://aclanthology.org/2021.findings-emnlp.297.pdf
ID  - 350
ER  - 

TY  - JOUR
AB  - The paper presents a discourse-based approach to the analysis of argumentative texts departing from the assumption that the coherence of a text should capture argumentation …
AU  - Saveleva, E.
AU  - Petukhova, V.
AU  - Mosbach, M.
DA  - //
N1  - Cited By (since 2021): 1
PY  - 2021
ST  - Discourse-based Argument Segmentation and Annotation
T2  - … of the 17th Joint ACL …
TI  - Discourse-based Argument Segmentation and Annotation
UR  - https://aclanthology.org/2021.isa-1.5/
https://aclanthology.org/2021.isa-1.5.pdf
ID  - 484
ER  - 

TY  - JOUR
AB  - … Based on empirical comparisons and the success of transfer learning in NLP, we finetune pre-trained BERT embeddings on the Emonet dataset (Abdul-Mageed and Ungar, 2017). The …
AU  - Sawhney, R.
AU  - Joshi, H.
AU  - Gandhi, S.
DA  - //
N1  - Cited By (since 2020): 57
PY  - 2020
ST  - A time-aware transformer based model for suicide ideation detection on social media
T2  - … processing (EMNLP)
TI  - A time-aware transformer based model for suicide ideation detection on social media
UR  - https://aclanthology.org/2020.emnlp-main.619/
https://aclanthology.org/2020.emnlp-main.619.pdf
ID  - 383
ER  - 

TY  - JOUR
AB  - Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural …
AU  - Sawhney, R.
AU  - Neerkaje, A. T.
DA  - //
N1  - Cited By (since 2022): 7
PY  - 2022
ST  - A risk-averse mechanism for suicidality assessment on social media
T2  - … Linguistics 2022 (ACL …
TI  - A risk-averse mechanism for suicidality assessment on social media
UR  - https://scholarcommons.sc.edu/aii_fac_pub/538/
https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=1555&context=aii_fac_pub
ID  - 464
ER  - 

TY  - BOOK
AB  - … Reliable negation parsing affects results in biomedical text min… representation models, Multilingual BERT and XLM-RoBERTa. … Moreover, BERT became widely used for transfer learning …
AU  - Shaitarova, A.
AU  - Rinaldi, F.
M1  - Query date: 2023-07-14 10:46:20
N1  - Cited By (since 2021): 3
PB  - zora.uzh.ch
PY  - 2021
ST  - Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in Russian, French, and Spanish
TI  - Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in Russian, French, and Spanish
UR  - https://www.zora.uzh.ch/id/eprint/205787/
https://www.zora.uzh.ch/id/eprint/205787/1/2021.naacl-srw.3.pdf
ID  - 330
ER  - 

TY  - JOUR
AB  - … We also analyse the cosine similarity of BERT representations of the names of the labels. We find that the average cosine similarity for the 5th most similar tag is 71.73% and maximum …
AU  - Sharma, S.
AU  - Khatuya, S.
AU  - Hegde, M.
AU  - Shaikh, A.
DA  - //
PY  - 2023
ST  - Financial Numeric Extreme Labelling: A dataset and benchmarking
T2  - … Linguistics: ACL …
TI  - Financial Numeric Extreme Labelling: A dataset and benchmarking
UR  - https://aclanthology.org/2023.findings-acl.219/
https://aclanthology.org/2023.findings-acl.219.pdf
ID  - 387
ER  - 

TY  - JOUR
AB  - … Biosimplify: an open source sentence simplification engine to improve recall in automatic biomedical information extraction. In AMIA Annual Symposium Proceedings, volume 2010, …
AU  - Sheang, K. C.
AU  - Saggion, H.
DA  - //
N1  - Cited By (since 2021): 15
PY  - 2021
ST  - Controllable sentence simplification with a unified text-to-text transfer transformer
T2  - … of the 14th International Conference on …
TI  - Controllable sentence simplification with a unified text-to-text transfer transformer
UR  - https://repositori.upf.edu/handle/10230/54079
https://repositori.upf.edu/bitstream/handle/10230/54079/Saggion_acl_cont.pdf?sequence=1&isAllowed=y
ID  - 463
ER  - 

TY  - JOUR
AB  - Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement …
AU  - Shen, L.
AU  - Zou, B.
AU  - Hong, Y.
AU  - Zhou, G.
AU  - Zhu, Q.
DA  - //
N1  - Cited By (since 2019): 6
PY  - 2019
ST  - Negative focus detection via contextual attention mechanism
T2  - … Processing (EMNLP …
TI  - Negative focus detection via contextual attention mechanism
UR  - https://aclanthology.org/D19-1230/
https://aclanthology.org/D19-1230.pdf
ID  - 467
ER  - 

TY  - JOUR
AB  - … We evaluated three versions of the BERT-based model. All of our BERT models use the pre-… We believe that gains for further pre-training on GENIA for the biomedical domain are higher …
AU  - Shi, W.
AU  - Demberg, V.
DA  - //
N1  - Cited By (since 2019): 77
PY  - 2019
ST  - Next sentence prediction helps implicit discourse relation classification within and across domains
T2  - … on natural language processing (EMNLP …
TI  - Next sentence prediction helps implicit discourse relation classification within and across domains
UR  - https://aclanthology.org/D19-1586/
https://aclanthology.org/D19-1586.pdf
ID  - 323
ER  - 

TY  - JOUR
AB  - … For each time expression, we query BERT for substitutes for the masked token in each template in the top part of Table 1. We translated the templates to other languages using Google …
AU  - Shwartz, V.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - Good night at 4 pm?! time expressions in different cultures
T2  - … of the Association for Computational Linguistics: ACL …
TI  - Good night at 4 pm?! time expressions in different cultures
UR  - https://aclanthology.org/2022.findings-acl.224/
https://aclanthology.org/2022.findings-acl.224.pdf
ID  - 398
ER  - 

TY  - JOUR
AB  - … • BERT. We fine-tune BERT on training data to train a sentence classifier, because the … the length constraint of BERT and it is expensive to train BERT-like models at discourse level. …
AU  - Song, W.
AU  - Song, Z.
AU  - Fu, R.
AU  - Liu, L.
AU  - Cheng, M.
DA  - //
N1  - Cited By (since 2020): 8
PY  - 2020
ST  - Discourse self-attention for discourse element identification in argumentative student essays
T2  - … Processing (EMNLP)
TI  - Discourse self-attention for discourse element identification in argumentative student essays
UR  - https://aclanthology.org/2020.emnlp-main.225/
https://aclanthology.org/2020.emnlp-main.225.pdf
ID  - 411
ER  - 

TY  - JOUR
AB  - Since the late 1990s, automatic text simplification (ATS) was promoted as a natural language processing (NLP) task with great potential to make texts more accessible to people with …
AU  - Štajner, S.
DA  - //
N1  - Cited By (since 2021): 26
PY  - 2021
ST  - Automatic text simplification for social good: Progress and challenges
T2  - … of the Association for Computational Linguistics: ACL …
TI  - Automatic text simplification for social good: Progress and challenges
UR  - https://aclanthology.org/2021.findings-acl.233.pdf
ID  - 460
ER  - 

TY  - JOUR
AB  - … BERT (base version) and initialize parameters of the BERT encoding layer with pretrained clinical BERT … Biomedical relation extraction with pre-trained language representations and …
AU  - Sui, D.
AU  - Chen, Y.
AU  - Zhao, J.
AU  - Jia, Y.
AU  - Xie, Y.
DA  - //
N1  - Cited By (since 2020): 65
PY  - 2020
ST  - Feded: Federated learning via ensemble distillation for medical relation extraction
T2  - … processing (EMNLP)
TI  - Feded: Federated learning via ensemble distillation for medical relation extraction
UR  - https://aclanthology.org/2020.emnlp-main.165/
https://aclanthology.org/2020.emnlp-main.165.pdf
ID  - 358
ER  - 

TY  - JOUR
AB  - … Pre-trained BERT contextualized representations have achieved state-of-the-art results on … We show that the pre-trained BERT model can be improved by augmenting data from the …
AU  - Sung, C.
AU  - Dhamecha, T.
AU  - Saha, S.
AU  - Ma, T.
DA  - //
N1  - Cited By (since 2019): 86
PY  - 2019
ST  - Pre-training BERT on domain resources for short answer grading
T2  - … Processing (EMNLP …
TI  - Pre-training BERT on domain resources for short answer grading
UR  - https://aclanthology.org/D19-1628/
https://aclanthology.org/D19-1628.pdf
ID  - 320
ER  - 

TY  - JOUR
AB  - … In this paper, we illustrate an example of transitioning from two biomedical IR benchmarks, ie, NFCorpus and TREC-COVID, to a practical case of seeking information about a specific …
AU  - Tahri, C.
AU  - Bochnakian, A.
AU  - Haouat, P.
DA  - //
PY  - 2023
ST  - Transitioning from benchmarks to a real-world case of information-seeking in Scientific Publications
T2  - … Linguistics: ACL 2023
TI  - Transitioning from benchmarks to a real-world case of information-seeking in Scientific Publications
UR  - https://aclanthology.org/2023.findings-acl.68/
https://aclanthology.org/2023.findings-acl.68.pdf
ID  - 410
ER  - 

TY  - JOUR
AB  - … First, we derive an extension vocabulary from the target domain (biomedical for this paper) … 2016), while keeping the original general vocabulary used by BERT unchanged. Any token in …
AU  - Tai, W.
AU  - Kung, H. T.
AU  - Dong, X. L.
AU  - Comiter, M.
DA  - //
N1  - Cited By (since 2020): 74
PY  - 2020
ST  - exBERT: Extending pre-trained models with domain-specific vocabulary under constrained training resources
T2  - … Linguistics: EMNLP …
TI  - exBERT: Extending pre-trained models with domain-specific vocabulary under constrained training resources
UR  - https://aclanthology.org/2020.findings-emnlp.129/
https://aclanthology.org/2020.findings-emnlp.129.pdf
ID  - 307
ER  - 

TY  - JOUR
AB  - … and then identifies the inter causal relations by BERT-base. For Event Causality Identification, we concatenate two event trigger representations from the context encoded by BERT, …
AU  - Tao, Z.
AU  - Jin, Z.
AU  - Bai, X.
AU  - Zhao, H.
AU  - Dou, C.
AU  - Zhao, Y.
DA  - //
PY  - 2023
ST  - SEAG: Structure-Aware Event Causality Generation
T2  - … Linguistics: ACL …
TI  - SEAG: Structure-Aware Event Causality Generation
UR  - https://aclanthology.org/2023.findings-acl.283/
https://aclanthology.org/2023.findings-acl.283.pdf
ID  - 430
ER  - 

TY  - JOUR
AB  - … Mention pretraining is performed as described in Section 3.1 MedMentions We also repurpose MedMentions (Mohan and Li, 2019) an existing entity linking dataset in the biomedical do…
AU  - Thirukovalluru, R.
AU  - Monath, N.
DA  - //
N1  - Cited By (since 2021): 15
PY  - 2021
ST  - Scaling within document coreference to long texts
T2  - … Linguistics: ACL …
TI  - Scaling within document coreference to long texts
UR  - https://www.research-collection.ethz.ch/handle/20.500.11850/527310
https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/527310/2021.findings-acl.343.pdf?sequence=2
ID  - 455
ER  - 

TY  - JOUR
AB  - … BERT and ZEN 2.0, we use the default settings (ie, 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT … other parameters (including BERT). For evaluation, …
AU  - Tian, Y.
AU  - Chen, G.
AU  - Qin, H.
AU  - Song, Y.
DA  - //
N1  - Cited By (since 2021): 6
PY  - 2021
ST  - Federated Chinese word segmentation with global character associations
T2  - … Computational Linguistics: ACL …
TI  - Federated Chinese word segmentation with global character associations
UR  - https://aclanthology.org/2021.findings-acl.376.pdf
ID  - 388
ER  - 

TY  - JOUR
AB  - Transliteration is an important task in natural language processing (NLP) which aims to convert a name in the source language to the target language without changing its pronunciation…
AU  - Tian, Y.
AU  - Lou, R.
AU  - Pang, X.
AU  - Wang, L.
AU  - Jiang, S.
DA  - //
PY  - 2022
ST  - Improving English-Arabic Transliteration with Phonemic Memories
T2  - … Linguistics: EMNLP …
TI  - Improving English-Arabic Transliteration with Phonemic Memories
UR  - https://aclanthology.org/2022.findings-emnlp.238/
https://aclanthology.org/2022.findings-emnlp.238.pdf
ID  - 472
ER  - 

TY  - JOUR
AB  - … We had shown that the proposed BERT-based jointly sentence and token label model was valid without using external data and hand-crafted feature for BioNER in three datasets: …
AU  - Tong, Y.
AU  - Chen, Y.
AU  - Shi, X.
DA  - //
N1  - Cited By (since 2021): 15
PY  - 2021
ST  - A multi-task approach for improving biomedical named entity recognition by incorporating multi-granularity information
T2  - … for Computational Linguistics: ACL …
TI  - A multi-task approach for improving biomedical named entity recognition by incorporating multi-granularity information
UR  - https://aclanthology.org/2021.findings-acl.424.pdf
ID  - 311
ER  - 

TY  - JOUR
AB  - … Second, comparing word2vec and BERT, we find that BERT … the training data for BERT and the cybersecurity domain. Third… 2009) for the biomedical domain). The closest works to our in …
AU  - Trong, H. M. D.
AU  - Le, D. T.
AU  - Veyseh, A. P. B.
DA  - //
N1  - Cited By (since 2020): 28
PY  - 2020
ST  - Introducing a new dataset for event detection in cybersecurity texts
T2  - … Processing (EMNLP)
TI  - Introducing a new dataset for event detection in cybersecurity texts
UR  - https://aclanthology.org/2020.emnlp-main.433/
https://aclanthology.org/2020.emnlp-main.433.pdf
ID  - 368
ER  - 

TY  - JOUR
AB  - … All models presented in this paper follow the BERT base architecture (… In order to do that we train a BERT model on S_beto2eu (S_BERT), and evaluate if the model trained exclusively …
AU  - Urbizu, G.
AU  - Vicente, I. San
AU  - Saralegi, X.
DA  - //
PY  - 2023
ST  - Not Enough Data to Pre-train Your Language Model? MT to the Rescue!
T2  - … Linguistics: ACL 2023
TI  - Not Enough Data to Pre-train Your Language Model? MT to the Rescue!
UR  - https://aclanthology.org/2023.findings-acl.235/
https://aclanthology.org/2023.findings-acl.235.pdf
ID  - 382
ER  - 

TY  - JOUR
AB  - We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event duration—how long an event lasts…
AU  - Vashishtha, S.
AU  - Poliak, A.
AU  - Lal, Y. K.
DA  - //
N1  - Cited By (since 2020): 28
PY  - 2020
ST  - Temporal reasoning in natural language inference
T2  - … Linguistics: EMNLP …
TI  - Temporal reasoning in natural language inference
UR  - https://aclanthology.org/2020.findings-emnlp.363/
https://aclanthology.org/2020.findings-emnlp.363.pdf
ID  - 456
ER  - 

TY  - JOUR
AB  - … For each word wi ∈ T, the corresponding hidden vector ei in the final layer of the BERT model is … On the effectiveness of the pooling methods for biomedical relation extraction with deep …
AU  - Veyseh, A. P. B.
AU  - Dernoncourt, F.
AU  - Min, B.
DA  - //
PY  - 2023
ST  - Generating Labeled Data for Relation Extraction: A Meta Learning Approach with Joint GPT-2 Training
T2  - … Linguistics: ACL 2023
TI  - Generating Labeled Data for Relation Extraction: A Meta Learning Approach with Joint GPT-2 Training
UR  - https://aclanthology.org/2023.findings-acl.727/
https://aclanthology.org/2023.findings-acl.727.pdf
ID  - 446
ER  - 

TY  - JOUR
AB  - … Also, in our experiments, we find that fixing the BERT parameters is more helpful. As such, … Comparable study of event extraction in newswire and biomedical domains. In Proceedings of …
AU  - Veyseh, A. P. B.
AU  - Nguyen, M. Van
DA  - //
N1  - Cited By (since 2022): 6
PY  - 2022
ST  - Document-level event argument extraction via optimal transport
T2  - … Linguistics: ACL …
TI  - Document-level event argument extraction via optimal transport
UR  - https://aclanthology.org/2022.findings-acl.130/
https://aclanthology.org/2022.findings-acl.130.pdf
ID  - 397
ER  - 

TY  - JOUR
AB  - … BERT, which can model interactions among words between two sentences by directly feeding the sentence pair to BERT, … which pre-train BERT on biomedical domain corpora, such as …
AU  - Wang, H.
AU  - Ma, F.
AU  - Wang, Y.
AU  - Gao, J.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Knowledge-guided paraphrase identification
T2  - … Linguistics: EMNLP 2021
TI  - Knowledge-guided paraphrase identification
UR  - https://aclanthology.org/2021.findings-emnlp.72/
https://aclanthology.org/2021.findings-emnlp.72.pdf
ID  - 306
ER  - 

TY  - JOUR
AB  - … bedding into the pretrained bert-based-uncased model on 9 public classification benchmarks of various sizes. We find that our method improves the results from BERT on small datasets…
AU  - Wang, R.
AU  - Si, S.
AU  - Wang, G.
AU  - Zhang, L.
AU  - Carin, L.
DA  - //
N1  - Cited By (since 2020): 5
PY  - 2020
ST  - Integrating task specific information into pretrained language models for low resource fine tuning
T2  - … Linguistics: EMNLP …
TI  - Integrating task specific information into pretrained language models for low resource fine tuning
UR  - https://aclanthology.org/2020.findings-emnlp.285/
https://aclanthology.org/2020.findings-emnlp.285.pdf
ID  - 353
ER  - 

TY  - JOUR
AB  - … in biomedical literature. In 2nd Workshop on Building and evaluating resources for biomedical text … Bert: Pre-training of deep bidirectional transformers for language understanding. In …
AU  - Wang, R.
AU  - Yu, T.
AU  - Wu, J.
AU  - Zhao, H.
AU  - Kim, S.
DA  - //
PY  - 2023
ST  - Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets
T2  - … Linguistics: ACL …
TI  - Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets
UR  - https://aclanthology.org/2023.findings-acl.470/
https://aclanthology.org/2023.findings-acl.470.pdf
ID  - 458
ER  - 

TY  - JOUR
AB  - Medical code prediction from clinical notes aims at automatically associating medical codes with the clinical notes. Rare code problem, the medical codes with low occurrences, is …
AU  - Wang, T.
AU  - Zhang, L.
AU  - Ye, C.
AU  - Liu, J.
DA  - //
N1  - Cited By (since 2022): 7
PY  - 2022
ST  - A novel framework based on medical concept driven attention for explainable medical code prediction via external knowledge
T2  - … Linguistics: ACL 2022
TI  - A novel framework based on medical concept driven attention for explainable medical code prediction via external knowledge
UR  - https://aclanthology.org/2022.findings-acl.110/
https://aclanthology.org/2022.findings-acl.110.pdf
ID  - 452
ER  - 

TY  - JOUR
AB  - … BERT We adopt the official BERT-base model pre-trained on the Chinese Wikipedia corpus … Additionally, our method is pre-trained on the Bert-like transformers to enhance the …
AU  - Wang, Y.
AU  - Wang, J.
AU  - Zhao, D.
AU  - Zheng, Z.
DA  - //
PY  - 2023
ST  - Rethinking Dictionaries and Glyphs for Chinese Language Pre-training
T2  - … Linguistics: ACL 2023
TI  - Rethinking Dictionaries and Glyphs for Chinese Language Pre-training
UR  - https://aclanthology.org/2023.findings-acl.70/
https://aclanthology.org/2023.findings-acl.70.pdf
ID  - 408
ER  - 

TY  - JOUR
AB  - … (2) Knowledge of linguistic ambiguity learned from the general domain benefits biomedical domain as well, which suggests ambiguity signals can be transferred across domains. But …
AU  - Wang, Y.
AU  - Wang, M.
AU  - Chen, Y.
AU  - Tao, S.
AU  - Guo, J.
DA  - //
N1  - Cited By (since 2022): 3
PY  - 2022
ST  - Capture human disagreement distributions by calibrated networks for natural language inference
T2  - … Linguistics: ACL …
TI  - Capture human disagreement distributions by calibrated networks for natural language inference
UR  - https://aclanthology.org/2022.findings-acl.120/
https://aclanthology.org/2022.findings-acl.120.pdf
ID  - 400
ER  - 

TY  - JOUR
AB  - … We evaluate MUNCHABLES on eight chemical/biomedical/scientific domain NER tasks, where seven training datasets are used as auxiliary training data. The experiment results show …
AU  - Watanabe, T.
AU  - Ichikawa, T.
AU  - Tamura, A.
DA  - //
PY  - 2022
ST  - Auxiliary Learning for Named Entity Recognition with Multiple Auxiliary Training Data
T2  - BioNLP 2022@ ACL …
TI  - Auxiliary Learning for Named Entity Recognition with Multiple Auxiliary Training Data
UR  - https://aclanthology.org/2022.bionlp-1.pdf#page=150
ID  - 331
ER  - 

TY  - JOUR
AB  - Like my colleague, Dan Jurafsky, General Chair of ACL-2020, the first ACL conference to be hit by the virus that has up-ended our world, I am sitting at my laptop, in the same chair I …
AU  - Webber, B.
AU  - Cohn, T.
AU  - He, Y.
AU  - Liu, Y.
DA  - //
N1  - Cited By (since 2020): 2
PY  - 2020
ST  - Proceedings of the 2020 conference on empirical methods in natural language processing (emnlp)
T2  - … Language Processing (EMNLP)
TI  - Proceedings of the 2020 conference on empirical methods in natural language processing (emnlp)
UR  - https://aclanthology.org/2020.emnlp-main.0.pdf
ID  - 485
ER  - 

TY  - JOUR
AB  - … In alleviating such context bias, we add a deconfounding layer on top of a BERT encoder, … Active learning with deep pre-trained models for sequence tagging of clinical and biomedical …
AU  - Wu, J.
AU  - Wang, R.
AU  - Yu, T.
AU  - Zhang, R.
AU  - Zhao, H.
DA  - //
PY  - 2022
ST  - Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling
T2  - … Linguistics: EMNLP …
TI  - Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling
UR  - https://aclanthology.org/2022.findings-emnlp.251/
https://aclanthology.org/2022.findings-emnlp.251.pdf
ID  - 427
ER  - 

TY  - JOUR
AB  - … 2, we can conclude that: (1) S2S tends to repeat words, which makes it get high BLEU but low BERT SCORE. (2) Oversampling strategy doesn’t benefit the models, hence, it cannot …
AU  - Wu, Y.
AU  - Kuang, K.
AU  - Zhang, Y.
AU  - Liu, X.
AU  - Sun, C.
DA  - //
N1  - Cited By (since 2020): 43
PY  - 2020
ST  - De-biased court's view generation with causality
T2  - … Processing (EMNLP)
TI  - De-biased court's view generation with causality
UR  - https://aclanthology.org/2020.emnlp-main.56/
https://aclanthology.org/2020.emnlp-main.56.pdf
ID  - 422
ER  - 

TY  - JOUR
AB  - … We use BERT to encode the sentence and then use three feed-forward (FF) layers in parallel, to predict scope label and the BSM labels. The losses for the three label classifiers Lscope…
AU  - Wu, Y.
AU  - Sun, A.
DA  - //
PY  - 2023
ST  - Negation Scope Refinement via Boundary Shift Loss
T2  - … the Association for Computational Linguistics: ACL …
TI  - Negation Scope Refinement via Boundary Shift Loss
UR  - https://aclanthology.org/2023.findings-acl.379/
https://aclanthology.org/2023.findings-acl.379.pdf
ID  - 402
ER  - 

TY  - JOUR
AB  - An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-…
AU  - Wunnava, S.
AU  - Qin, X.
AU  - Kakar, T.
AU  - Kong, X.
DA  - //
N1  - Cited By (since 2020): 9
PY  - 2020
ST  - A dual-attention network for joint named entity recognition and sentence classification of adverse drug events
T2  - … Linguistics: EMNLP …
TI  - A dual-attention network for joint named entity recognition and sentence classification of adverse drug events
UR  - https://aclanthology.org/2020.findings-emnlp.306/
https://aclanthology.org/2020.findings-emnlp.306.pdf
ID  - 447
ER  - 

TY  - JOUR
AB  - A private learning scheme TextHide was recently proposed to protect the private text data during the training phase via so-called instance encoding. We propose a novel reconstruction …
AU  - Xie, S.
AU  - Hong, Y.
DA  - //
N1  - Cited By (since 2021): 9
PY  - 2021
ST  - Reconstruction attack on instance encoding for language understanding
T2  - … Methods in Natural Language Processing (EMNLP'21)
TI  - Reconstruction attack on instance encoding for language understanding
UR  - https://par.nsf.gov/servlets/purl/10322066
ID  - 428
ER  - 

TY  - BOOK
AB  - … BERT rows in Table 5 demonstrate that the triplet training is a critical part of the success: if we use PubMed-BERT … , as on the BC5CDR datasets, PubMedBERT can get within 3 points of …
AU  - Xu, D.
AU  - Bethard, S.
M1  - Query date: 2023-07-14 10:46:20
N1  - Cited By (since 2021): 6
PB  - repository.arizona.edu
PY  - 2021
ST  - Triplet-trained vector space and sieve-based search improve biomedical concept normalization
TI  - Triplet-trained vector space and sieve-based search improve biomedical concept normalization
UR  - https://repository.arizona.edu/handle/10150/663578
https://repository.arizona.edu/bitstream/handle/10150/663578/2021bionlp_1_2.pdf?sequence=1
ID  - 314
ER  - 

TY  - JOUR
AB  - … Experimental Setup We use the bert-basecased and roberta-base as the base encoders for CoNLL03, WNUT16, and WikiGold datasets. BC5CDR is in the biomedical domain, and we …
AU  - Xu, L.
AU  - Bing, L.
AU  - Lu, W.
DA  - //
PY  - 2023
ST  - Sampling Better Negatives for Distantly Supervised Named Entity Recognition
T2  - … for Computational Linguistics: ACL 2023
TI  - Sampling Better Negatives for Distantly Supervised Named Entity Recognition
UR  - https://aclanthology.org/2023.findings-acl.300/
https://aclanthology.org/2023.findings-acl.300.pdf
ID  - 354
ER  - 

TY  - JOUR
AB  - … First, we feed the input text into BERT to obtain the representation h ∈ Rn×d, where d is the dimension of the BERT hidden states. For each token xi, BERT tokenizer may divide it into …
AU  - Xu, Y.
AU  - Yang, Z.
AU  - Zhang, L.
AU  - Zhou, D.
AU  - Wu, T.
DA  - //
PY  - 2023
ST  - Focusing, Bridging and Prompting for Few-shot Nested Named Entity Recognition
T2  - … Linguistics: ACL 2023
TI  - Focusing, Bridging and Prompting for Few-shot Nested Named Entity Recognition
UR  - https://aclanthology.org/2023.findings-acl.164/
https://aclanthology.org/2023.findings-acl.164.pdf
ID  - 434
ER  - 

TY  - JOUR
AB  - Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages …
AU  - Xu, Z.
AU  - Lee, J. Y.
AU  - Huang, L.
DA  - //
PY  - 2023
ST  - Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from Abstract Meaning Representation
T2  - … for Computational Linguistics: ACL 2023
TI  - Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from Abstract Meaning Representation
UR  - https://aclanthology.org/2023.findings-acl.662/
https://aclanthology.org/2023.findings-acl.662.pdf
ID  - 466
ER  - 

TY  - JOUR
AB  - … We see that for targets such as lesions, shadows and skin thickening BERT is able … BERT degrades the encoder’s results. We argue that the high level convolutional features that BERT …
AU  - Yalunin, A.
AU  - Sokolova, E.
AU  - Burenko, I.
DA  - //
PY  - 2021
ST  - Generating Mammography Reports from Multi-view Mammograms with BERT
T2  - … Linguistics: EMNLP …
TI  - Generating Mammography Reports from Multi-view Mammograms with BERT
UR  - https://aclanthology.org/2021.findings-emnlp.15/
https://aclanthology.org/2021.findings-emnlp.15.pdf
ID  - 340
ER  - 

TY  - JOUR
AB  - … Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and …
AU  - Yamada, K.
AU  - Hirao, T.
AU  - Sasano, R.
AU  - Takeda, K.
DA  - //
N1  - Cited By (since 2020): 7
PY  - 2020
ST  - Sequential span classification with neural semi-markov crfs for biomedical abstracts
T2  - … Linguistics: EMNLP …
TI  - Sequential span classification with neural semi-markov crfs for biomedical abstracts
UR  - https://aclanthology.org/2020.findings-emnlp.77/
https://aclanthology.org/2020.findings-emnlp.77.pdf
ID  - 317
ER  - 

TY  - JOUR
AB  - … BERT… biomedical entity normalization. Since the data used in this study are in Chinese, we replaced the original English pre-trained BERT model with a pre-trained Chinese model BERT…
AU  - Yan, J.
AU  - Wang, Y.
AU  - Xiang, L.
AU  - Zhou, Y.
DA  - //
N1  - Cited By (since 2020): 13
PY  - 2020
ST  - A knowledge-driven generative model for multi-implication chinese medical procedure entity normalization
T2  - … Processing (EMNLP)
TI  - A knowledge-driven generative model for multi-implication chinese medical procedure entity normalization
UR  - https://aclanthology.org/2020.emnlp-main.116/
https://aclanthology.org/2020.emnlp-main.116.pdf
ID  - 328
ER  - 

TY  - JOUR
AB  - … ferent parser models, and does BERT give similar improvements for all … Besides, we show that BERT benefits parsers on cross… From the table, we can see that the biomedical and review …
AU  - Yang, S.
AU  - Cui, L.
AU  - Ning, R.
AU  - Wu, D.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - Challenges to open-domain constituency parsing
T2  - … Linguistics: ACL 2022
TI  - Challenges to open-domain constituency parsing
UR  - https://aclanthology.org/2022.findings-acl.11/
https://aclanthology.org/2022.findings-acl.11.pdf
ID  - 336
ER  - 

TY  - JOUR
AB  - … sentences from biomedical Chinese text that captures relations between medical entities (… 5 which is based on the structure of BERT and trained on a mount of Chinese clinic texts. The …
AU  - Yang, Z.
AU  - Huang, Y.
AU  - Feng, J.
DA  - //
PY  - 2023
ST  - Learning to Leverage High-Order Medical Knowledge Graph for Joint Entity and Relation Extraction
T2  - … for Computational Linguistics: ACL …
TI  - Learning to Leverage High-Order Medical Knowledge Graph for Joint Entity and Relation Extraction
UR  - https://aclanthology.org/2023.findings-acl.575/
https://aclanthology.org/2023.findings-acl.575.pdf
ID  - 420
ER  - 

TY  - JOUR
AB  - … For sentence-level embeddings, we use the BERT and ALBERT embeddings to further improve the NNER. For ACE-2004, ACE-2005, and NNE datasets, the dimensions of character-…
AU  - Yang, Z.
AU  - Ma, J.
AU  - Chen, H.
AU  - Zhang, Y.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - HiTRANS: a hierarchical transformer network for nested named entity recognition
T2  - … Linguistics: EMNLP 2021
TI  - HiTRANS: a hierarchical transformer network for nested named entity recognition
UR  - https://aclanthology.org/2021.findings-emnlp.12/
https://aclanthology.org/2021.findings-emnlp.12.pdf
ID  - 391
ER  - 

TY  - JOUR
AB  - Recent works suggest that transformer models are capable of multi-tasking on diverse NLP tasks and adapt to new tasks efficiently. However, the potential of these multi-task models …
AU  - Ye, Q.
AU  - Zha, J.
AU  - Ren, X.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts
T2  - … for Computational Linguistics: EMNLP 2022
TI  - Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts
UR  - https://aclanthology.org/2022.findings-emnlp.189/
https://aclanthology.org/2022.findings-emnlp.189.pdf
ID  - 465
ER  - 

TY  - JOUR
AB  - … BERT is the latest method for pre-training language … Pytorch version of BERT with its pre-trained model (Cased BERT-Base, … In our current work we chose the biomedical domain as the …
AU  - Yu, B.
AU  - Li, Y.
AU  - Wang, J.
DA  - //
N1  - Cited By (since 2019): 31
PY  - 2019
ST  - Detecting causal language use in science findings
T2  - … on Natural Language Processing (EMNLP …
TI  - Detecting causal language use in science findings
UR  - https://aclanthology.org/D19-1473/
https://aclanthology.org/D19-1473.pdf
ID  - 329
ER  - 

TY  - JOUR
AB  - … from Sentence BERT, capturing the language cues of the tweet to be assessed, with an aggregate representation of the emotional spectrum, obtained from a pre-trained BERT model …
AU  - Zanwar, S.
AU  - Li, X.
AU  - Wiechmann, D.
AU  - Qiao, Y.
DA  - //
PY  - 2023
ST  - What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection
T2  - … Linguistics: ACL 2023
TI  - What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection
UR  - https://aclanthology.org/2023.findings-acl.568/
https://aclanthology.org/2023.findings-acl.568.pdf
ID  - 426
ER  - 

TY  - JOUR
AB  - … We evaluate our framework based on four kinds of BERT models: regular BERT, SpanBERT (Joshi et al., 2020), BioBERT, and SciBERT. Experiments on seven SciNER benchmarks …
AU  - Zeng, Q.
AU  - Yu, W.
AU  - Yu, M.
AU  - Jiang, T.
DA  - //
N1  - Cited By (since 2020): 6
PY  - 2020
ST  - Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER
T2  - … Linguistics: EMNLP …
TI  - Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER
UR  - https://aclanthology.org/2020.findings-emnlp.429/
https://aclanthology.org/2020.findings-emnlp.429.pdf
ID  - 361
ER  - 

TY  - JOUR
AB  - … Similarly, our BERTTagger consists of a pretrained BERT for encoding the input example … in most cases, especially for models based on BERT. This phenomenon suggests that the non-…
AU  - Zeng, X.
AU  - Li, Y.
AU  - Zhai, Y.
AU  - Zhang, Y.
DA  - //
N1  - Cited By (since 2020): 54
PY  - 2020
ST  - Counterfactual generator: A weakly-supervised method for named entity recognition
T2  - … Language Processing (EMNLP)
TI  - Counterfactual generator: A weakly-supervised method for named entity recognition
UR  - https://aclanthology.org/2020.emnlp-main.590/
https://aclanthology.org/2020.emnlp-main.590.pdf
ID  - 379
ER  - 

TY  - JOUR
AB  - … 2021): A BERTbased model which uses BERT-CRF and crossdomain corpora to detect event-related negation and speculation scope for factuality identification. 4) ULGN (Cao et al.…
AU  - Zhang, H.
AU  - Li, P.
AU  - Qian, Z.
AU  - Zhu, X.
DA  - //
PY  - 2023
ST  - Incorporating Factuality Inference to Identify Document-level Event Factuality
T2  - … for Computational Linguistics: ACL …
TI  - Incorporating Factuality Inference to Identify Document-level Event Factuality
UR  - https://aclanthology.org/2023.findings-acl.879/
https://aclanthology.org/2023.findings-acl.879.pdf
ID  - 440
ER  - 

TY  - JOUR
AB  - Continual Language Learning (CLL) in multilingual translation is inevitable when new languages are required to be translated. Due to the lack of unified and generalized benchmarks, …
AU  - Zhang, H.
AU  - Zhang, S.
AU  - Xiang, Y.
AU  - Liang, B.
AU  - Su, J.
DA  - //
PY  - 2022
ST  - CLLE: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation
T2  - … Linguistics: EMNLP …
TI  - CLLE: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation
UR  - https://aclanthology.org/2022.findings-emnlp.30/
https://aclanthology.org/2022.findings-emnlp.30.pdf
ID  - 476
ER  - 

TY  - JOUR
AB  - … For BERT/XLNet4, we utilize the abstractive summarization schema as the encoder part is replaced with the BERT/XLNet encoder (question&answer) and the decoder is trained from …
AU  - Zhang, N.
AU  - Deng, S.
AU  - Li, J.
AU  - Chen, X.
AU  - Zhang, W.
DA  - //
N1  - Cited By (since 2020): 13
PY  - 2020
ST  - Summarizing chinese medical answer with graph convolution networks and question-focused dual attention
T2  - … Linguistics: EMNLP …
TI  - Summarizing chinese medical answer with graph convolution networks and question-focused dual attention
UR  - https://aclanthology.org/2020.findings-emnlp.2/
https://aclanthology.org/2020.findings-emnlp.2.pdf
ID  - 359
ER  - 

TY  - JOUR
AB  - … We compare the base version of BERT and MPNet, which have the same architecture and model size but different pretraining schemes. MPNet has been shown to outperform …
AU  - Zhang, T.
AU  - Xu, Z.
AU  - Medini, T.
DA  - //
PY  - 2022
ST  - Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification
T2  - … Linguistics: EMNLP 2022
TI  - Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification
UR  - https://aclanthology.org/2022.findings-emnlp.362/
https://aclanthology.org/2022.findings-emnlp.362.pdf
ID  - 416
ER  - 

TY  - JOUR
AB  - … In Proceedings of the 20th Workshop on Biomedical Language Processing, pages 96–102, … Gaml-bert: Improving bert early exiting by gradient aligned mutual learning. In EMNLP. …
AU  - Zhang, Y.
AU  - Wang, P.
AU  - Tan, M.
AU  - Zhu, W.
DA  - //
PY  - 2023
ST  - Learned Adapters Are Better Than Manually Designed Adapters
T2  - … Computational Linguistics: ACL …
TI  - Learned Adapters Are Better Than Manually Designed Adapters
UR  - https://aclanthology.org/2023.findings-acl.468/
https://aclanthology.org/2023.findings-acl.468.pdf
ID  - 469
ER  - 

TY  - JOUR
AB  - … , 2020) version of the language model BERT to extract text features of the entities. The dimension numbers Nd and Nk of hidden features in co-attention module are set to 128, and the …
AU  - Zhang, Y.
AU  - Zhou, B.
AU  - Song, K.
AU  - Sui, X.
AU  - Zhao, G.
DA  - //
PY  - 2022
ST  - PM2F2N: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction
T2  - … Linguistics: EMNLP …
TI  - PM2F2N: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction
UR  - https://aclanthology.org/2022.findings-emnlp.144/
https://aclanthology.org/2022.findings-emnlp.144.pdf
ID  - 453
ER  - 

TY  - JOUR
AB  - … In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these … variants BERT-Flat and BERT-AMR, where BERTFlat only uses the BERT representations …
AU  - Zhang, Z.
AU  - Parulian, N. N.
AU  - Ji, H.
AU  - Elsayed, A. S.
DA  - //
N1  - Cited By (since 2021): 21
PY  - 2021
ST  - Fine-grained information extraction from biomedical literature based on knowledge-enriched abstract meaning representation
T2  - … Processing (ACL …
TI  - Fine-grained information extraction from biomedical literature based on knowledge-enriched abstract meaning representation
UR  - https://par.nsf.gov/servlets/purl/10337686
ID  - 310
ER  - 

TY  - JOUR
AB  - … To show the influence of incorporating the pretrained language model BERT (Devlin et al.… 2019), in CompGCNBERT, we use the bert-base-uncased model checkpoint5 to initialize entity …
AU  - Zhang, Z.
AU  - Wang, H.
AU  - Zhao, H.
AU  - Tong, H.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - Eventke: Event-enhanced knowledge graph embedding
T2  - … Linguistics: EMNLP 2021
TI  - Eventke: Event-enhanced knowledge graph embedding
UR  - https://aclanthology.org/2021.findings-emnlp.120/
https://aclanthology.org/2021.findings-emnlp.120.pdf
ID  - 415
ER  - 

TY  - JOUR
AB  - … detecting resources in biomedical literature. Some approaches to this end are reported in (Duck et al.… For future work, we will try using pre-trained BERT for scientific domain such as the …
AU  - Zhao, H.
AU  - Luo, Z.
AU  - Feng, C.
AU  - Zheng, A.
DA  - //
N1  - Cited By (since 2019): 20
PY  - 2019
ST  - A context-based framework for modeling the role and function of on-line resource citations in scientific literature
T2  - … Processing (EMNLP …
TI  - A context-based framework for modeling the role and function of on-line resource citations in scientific literature
UR  - https://aclanthology.org/D19-1524/
https://aclanthology.org/D19-1524.pdf
ID  - 366
ER  - 

TY  - JOUR
AB  - … (2021) constructs the BioLAMA, a biomedical factual knowledge dataset for probing biomedical LMs, and further explores the capability of LM to act as a specific-domain KB. Moreover, …
AU  - Zhao, R.
AU  - Zhao, F.
AU  - Xu, G.
AU  - Zhang, S.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Can Language Models Serve as Temporal Knowledge Bases?
T2  - … Linguistics: EMNLP 2022
TI  - Can Language Models Serve as Temporal Knowledge Bases?
UR  - https://aclanthology.org/2022.findings-emnlp.147/
https://aclanthology.org/2022.findings-emnlp.147.pdf
ID  - 437
ER  - 

TY  - JOUR
AB  - … IEEE transactions on biomedical engineering, 67(6):1739–… Coupling artificial neurons in bert and biological neurons in … Bertviz: A tool for visualizing multihead self-attention in the bert …
AU  - Zhou, M.
AU  - Liu, X.
AU  - Liu, D.
AU  - Wu, Z.
AU  - Liu, Z.
AU  - Zhao, L.
DA  - //
N1  - Cited By (since 2023): 1
PY  - 2023
ST  - Fine-grained Artificial Neurons in Audio-transformers for Disentangling Neural Auditory Encoding
T2  - … Linguistics: ACL …
TI  - Fine-grained Artificial Neurons in Audio-transformers for Disentangling Neural Auditory Encoding
UR  - https://aclanthology.org/2023.findings-acl.503/
https://aclanthology.org/2023.findings-acl.503.pdf
ID  - 441
ER  - 

TY  - JOUR
AB  - … ), namely news track WMT2020-News and biomedical track WMT2020-Bio. We consider news and bio as new domains, compared to our training data FLORES-101 whose contents are …
AU  - Zhuo, T. Y.
AU  - Xu, Q.
AU  - He, X.
AU  - Cohn, T.
DA  - //
PY  - 2023
ST  - Rethinking Round-Trip Translation for Machine Translation Evaluation
T2  - … for Computational Linguistics: ACL …
TI  - Rethinking Round-Trip Translation for Machine Translation Evaluation
UR  - https://aclanthology.org/2023.findings-acl.22/
https://aclanthology.org/2023.findings-acl.22.pdf
ID  - 468
ER  - 

TY  - JOUR
AB  - … embedding methods (eg Sen-BERT (Reimers and Gurevych… The results on two biomedical benchmark corpora show the … insights into the field of biomedical data acquisition by conduct…
AU  - Zlabinger, M.
AU  - Sabou, M.
AU  - Hofstätter, S.
DA  - //
N1  - Cited By (since 2020): 5
PY  - 2020
ST  - Effective crowd-annotation of participants, interventions, and outcomes in the text of clinical trial reports
T2  - … Linguistics: EMNLP …
TI  - Effective crowd-annotation of participants, interventions, and outcomes in the text of clinical trial reports
UR  - https://aclanthology.org/2020.findings-emnlp.274/
https://aclanthology.org/2020.findings-emnlp.274.pdf
ID  - 333
ER  - 

TY  - JOUR
AB  - Welcome to the Findings of ACL: ACL-IJCNLP 2021! To continue the success of Findings of ACL: EMNLP 2020, we decided to follow this initiative to produce this accompanying volume…
AU  - Zong, C.
AU  - Xia, F.
AU  - Li, W.
AU  - Navigli, R.
DA  - //
N1  - Cited By (since 2021): 6
PY  - 2021
ST  - Findings of the association for computational linguistics: ACL-IJCNLP 2021
T2  - … for Computational Linguistics: ACL …
TI  - Findings of the association for computational linguistics: ACL-IJCNLP 2021
UR  - https://aclanthology.org/2021.findings-acl.0.pdf
ID  - 474
ER  - 

TY  - JOUR
AB  - … We present a query-based biomedical information retrieval task across … biomedical literature while BERT and RoBERTa are trained on general texts, we also use the Bio+Clinical BERT …
AU  - Zuo, C.
AU  - Acharya, N.
AU  - Banerjee, R.
DA  - //
N1  - Cited By (since 2020): 9
PY  - 2020
ST  - Querying across genres for medical claims in news
T2  - … Natural Language Processing (EMNLP)
TI  - Querying across genres for medical claims in news
UR  - https://par.nsf.gov/biblio/10233316
https://par.nsf.gov/servlets/purl/10233316
ID  - 321
ER  - 

TY  - JOUR
AB  - … through perplexity based on a large Transformer language model: GPT-2 (Radford et al.… A comprehensive evaluation of chatgpt’s zero-shot text-to-sql capability. arXiv preprint arXiv:…
AU  - Hu, X.
AU  - Jiang, Y.
AU  - Liu, A.
AU  - Huang, Z.
AU  - Xie, P.
DA  - //
PY  - 2023
ST  - Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks
T2  - … Linguistics: ACL …
TI  - Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks
UR  - https://aclanthology.org/2023.findings-acl.578/
https://aclanthology.org/2023.findings-acl.578.pdf
ID  - 501
ER  - 

TY  - JOUR
AB  - … aim here at generalisability but at performing a specific task (translating biomedical texts). … has the same model architecture as OpenNMT (Transformer). Because of these issues, we …
AU  - Ballier, N.
AU  - Cho, D.
AU  - Faye, B.
AU  - Ke, Z. Y.
DA  - //
N1  - Cited By (since 2021): 1
PY  - 2021
ST  - The SPECTRANS System Description for the WMT21 Terminology Task
T2  - EMNLP 2021 SIXTH …
TI  - The SPECTRANS System Description for the WMT21 Terminology Task
UR  - https://hal.science/hal-03574680/
https://hal.science/hal-03574680/document
ID  - 607
ER  - 

TY  - BOOK
AB  - … We first train a baseline big Transformer on filtered ParaCrawl and an oversampled version of the remaining parallel data and then continue training with NewsCrawl backtranslations. …
AU  - Barrault, L.
AU  - Bojar, O.
AU  - Costa-Jussa, M. R.
AU  - Federmann, C.
M1  - Query date: 2023-07-14 10:44:31
N1  - Cited By (since 2019): 619
PB  - zora.uzh.ch
PY  - 2019
ST  - Findings of the 2019 conference on machine translation (WMT19)
TI  - Findings of the 2019 conference on machine translation (WMT19)
UR  - https://www.zora.uzh.ch/id/eprint/176407/
https://www.zora.uzh.ch/id/eprint/176407/1/WMT01.pdf
ID  - 544
ER  - 

TY  - JOUR
AB  - … biomedical corpus of patient letters. In the later paper on the annotation of full-text biomedical … each discourse unit with CLS-pooling of a pretrained transformer into a vector vi ∈ RdLM : …
AU  - Chistova, E.
DA  - //
PY  - 2023
ST  - End-to-End Argument Mining over Varying Rhetorical Structures
T2  - … of the Association for Computational Linguistics: ACL …
TI  - End-to-End Argument Mining over Varying Rhetorical Structures
UR  - https://aclanthology.org/2023.findings-acl.209/
https://aclanthology.org/2023.findings-acl.209.pdf
ID  - 605
ER  - 

TY  - JOUR
AB  - … Anatomy is one of the longest running tracks in the OAEI, which consists of human and mouse anatomy ontologies from the biomedical domain and have been manually matched by …
AU  - Guo, S.
AU  - Wang, C.
AU  - Chen, Y.
AU  - Liu, K.
AU  - Li, R.
DA  - //
PY  - 2023
ST  - EventOA: An Event Ontology Alignment Benchmark Based on FrameNet and Wikidata
T2  - … Linguistics: ACL 2023
TI  - EventOA: An Event Ontology Alignment Benchmark Based on FrameNet and Wikidata
UR  - https://aclanthology.org/2023.findings-acl.637/
https://aclanthology.org/2023.findings-acl.637.pdf
ID  - 675
ER  - 

TY  - BOOK
AB  - … As mentioned earlier, we used the MarianNMT toolkit to train our Transformer models. The … The ADAPT’s submissions to the WMT20 biomedical translation task. In Proceedings of the …
AU  - Jooste, W.
AU  - Haque, R.
AU  - Way, A.
M1  - Query date: 2023-07-14 10:44:31
N1  - Cited By (since 2020): 1
PB  - doras.dcu.ie
PY  - 2020
ST  - The ADAPT Centre's neural MT systems for the WAT 2020 document-level translation task
TI  - The ADAPT Centre's neural MT systems for the WAT 2020 document-level translation task
UR  - https://doras.dcu.ie/25205/
https://doras.dcu.ie/25205/1/WAT_2020.pdf
ID  - 632
ER  - 

TY  - JOUR
AB  - … On the Transformer baseline, we combine the input sentence … on English-French dataset forn WMT’18 Biomedical Task … and multi-source encoding for transformer baselines in all tasks, …
AU  - Khan, A. R.
AU  - Xu, J.
AU  - Sun, W.
DA  - //
N1  - Cited By (since 2020): 1
PY  - 2020
ST  - Coding textual inputs boosts the accuracy of neural networks
T2  - … in Natural Language Processing (EMNLP …
TI  - Coding textual inputs boosts the accuracy of neural networks
UR  - https://aclanthology.org/2020.emnlp-main.104/
https://aclanthology.org/2020.emnlp-main.104.pdf
ID  - 583
ER  - 

TY  - JOUR
AB  - We present set to ordered text, a natural language generation task applied to automatically generating discharge instructions from admission ICD (International Classification of …
AU  - Kurisinkel, L. J.
AU  - Chen, N.
DA  - //
N1  - Cited By (since 2019): 4
PY  - 2019
ST  - Set to ordered text: Generating discharge instructions from medical billing codes
T2  - … on Natural Language Processing (EMNLP …
TI  - Set to ordered text: Generating discharge instructions from medical billing codes
UR  - https://aclanthology.org/D19-1638/
https://aclanthology.org/D19-1638.pdf
ID  - 700
ER  - 

TY  - JOUR
AB  - … Namely, the selfattention mechanism from the transformer is combined with a temporal … Learning disentangled representations of texts with application to biomedical abstracts. In …
AU  - Li, D.
AU  - Fei, H.
AU  - Ren, S.
AU  - Li, P.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - A deep decomposable model for disentangling syntax and semantics in sentence representation
T2  - … for Computational Linguistics: EMNLP …
TI  - A deep decomposable model for disentangling syntax and semantics in sentence representation
UR  - https://aclanthology.org/2021.findings-emnlp.364/
https://aclanthology.org/2021.findings-emnlp.364.pdf
ID  - 653
ER  - 

TY  - JOUR
AB  - … We also explore whether the transformer architecture is more effective than representative … Our primary contributions can be summarized as follows: (1) We apply the transformer model …
AU  - Lovelace, J.
AU  - Mortazavi, B.
DA  - //
N1  - Cited By (since 2020): 38
PY  - 2020
ST  - Learning to generate clinically coherent chest X-ray reports
T2  - … for Computational Linguistics: EMNLP …
TI  - Learning to generate clinically coherent chest X-ray reports
UR  - https://aclanthology.org/2020.findings-emnlp.110/
https://aclanthology.org/2020.findings-emnlp.110.pdf
ID  - 546
ER  - 

TY  - BOOK
AB  - … In this task, in order to improve our baseline Transformer models, we augmented our training data with both the target- and source-original synthetic data. As in Caswell et al. (2019), in …
AU  - Nayak, P.
AU  - Haque, R.
AU  - Way, A.
M1  - Query date: 2023-07-14 10:44:31
N1  - Cited By (since 2020): 3
PB  - doras.dcu.ie
PY  - 2020
ST  - The ADAPT's submissions to the WMT20 biomedical translation task
TI  - The ADAPT's submissions to the WMT20 biomedical translation task
UR  - https://doras.dcu.ie/25107/
https://doras.dcu.ie/25107/1/WMT_en_eu_Biomed.pdf
ID  - 524
ER  - 

TY  - BOOK
AB  - … We used the state-of-the-art Transformer model in order to prepare our MT systems. For … The adapt’s submissions to the WMT20 biomedical translation task. In Proceedings of the Fifth …
AU  - Nayak, P.
AU  - Haque, R.
AU  - Way, A.
M1  - Query date: 2023-07-14 10:44:31
N1  - Cited By (since 2020): 1
PB  - doras.dcu.ie
PY  - 2020
ST  - The ADAPT centre's participation in WAT 2020 English-to-Odia translation task
TI  - The ADAPT centre's participation in WAT 2020 English-to-Odia translation task
UR  - https://doras.dcu.ie/25462/
https://doras.dcu.ie/25462/1/wat20.pdf
ID  - 558
ER  - 

TY  - JOUR
AB  - … Transformer. In detail, the encoding process is formulated as … The second, namely BASE+RL is the Transformer model with the … The third, namely BASE+CMM, is the Transformer model …
AU  - Qin, H.
AU  - Song, Y.
DA  - //
N1  - Cited By (since 2022): 12
PY  - 2022
ST  - Reinforced cross-modal alignment for radiology report generation
T2  - … Association for Computational Linguistics: ACL …
TI  - Reinforced cross-modal alignment for radiology report generation
UR  - https://aclanthology.org/2022.findings-acl.38/
https://aclanthology.org/2022.findings-acl.38.pdf
ID  - 564
ER  - 

TY  - BOOK
AB  - Automatic recognition of customer complaints on products or services that they purchase can be crucial for the organizations, multinationals and online retailers since they can exploit …
AU  - Singh, R. P.
AU  - Haque, R.
AU  - Hasanuzzaman, M.
AU  - Way, A.
M1  - Query date: 2023-07-14 10:44:31
N1  - Cited By (since 2020): 2
PB  - doras.dcu.ie
PY  - 2020
ST  - Identifying complaints from product reviews in low-resource scenarios via neural machine translation
TI  - Identifying complaints from product reviews in low-resource scenarios via neural machine translation
UR  - https://doras.dcu.ie/25291/
https://doras.dcu.ie/25291/1/33_Paper(1).pdf
ID  - 693
ER  - 

TY  - JOUR
AB  - Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system …
AU  - Wang, L.
AU  - Ni, J.
AU  - Gao, H.
AU  - Li, J.
AU  - Chang, K. C.
DA  - //
PY  - 2023
ST  - Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition
T2  - … Linguistics: ACL …
TI  - Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition
UR  - https://aclanthology.org/2023.findings-acl.424/
https://aclanthology.org/2023.findings-acl.424.pdf
ID  - 687
ER  - 

TY  - JOUR
AB  - … biomedicine statistical machine translation srl … pretrained language models contextual embeddings multilingual bert roberta … pretrained language models multilingual bert mlm …
AU  - Balepur, N.
AU  - Agarwal, S.
AU  - Ramanan, K. V.
DA  - //
PY  - 2023
ST  - DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance
T2  - … Linguistics: ACL …
TI  - DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance
UR  - https://aclanthology.org/2023.findings-acl.14/
https://aclanthology.org/2023.findings-acl.14.pdf
ID  - 1157
ER  - 

TY  - JOUR
AB  - … With XLNET or BERT as the baseline PLM, the improvement of LBGCN on F-scores and Roov … the proposed framework LBGCN based on the BERT or RoBERTa outperforms all existing …
AU  - Huang, K.
AU  - Yu, H.
AU  - Liu, J.
AU  - Liu, W.
AU  - Cao, J.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Lexicon-based graph convolutional network for Chinese word segmentation
T2  - … Linguistics: EMNLP …
TI  - Lexicon-based graph convolutional network for Chinese word segmentation
UR  - https://aclanthology.org/2021.findings-emnlp.248/
https://aclanthology.org/2021.findings-emnlp.248.pdf
ID  - 1121
ER  - 

TY  - JOUR
AB  - … However, 4) what is unexpected is that pre-trained sentence encoding methods based on modification of BERT do not outperform the BERT-base encoder and even damage …
AU  - Liu, P.
AU  - Lin, H.
AU  - Liao, M.
AU  - Xiang, H.
AU  - Han, X.
DA  - //
PY  - 2023
ST  - WebDP: Understanding Discourse Structures in Semi-Structured Web Documents
T2  - … Linguistics: ACL 2023
TI  - WebDP: Understanding Discourse Structures in Semi-Structured Web Documents
UR  - https://aclanthology.org/2023.findings-acl.650/
https://aclanthology.org/2023.findings-acl.650.pdf
ID  - 1151
ER  - 

TY  - JOUR
AB  - Named Entity Recognition (NER) is deeply explored and widely used in various tasks. Usually, some entity mentions are nested in other entities, which leads to the nested NER problem…
AU  - Long, X.
AU  - Niu, S.
AU  - Li, Y.
DA  - //
N1  - Cited By (since 2020): 1
PY  - 2020
ST  - Hierarchical region learning for nested named entity recognition
T2  - … for Computational Linguistics: EMNLP 2020
TI  - Hierarchical region learning for nested named entity recognition
UR  - https://aclanthology.org/2020.findings-emnlp.430/
https://aclanthology.org/2020.findings-emnlp.430.pdf
ID  - 1158
ER  - 

TY  - JOUR
AB  - … approach similar to the above models, but for contextualized embedding models like BERT (… on natural language processing in biomedicine and its applications, pages 70–75. Citeseer. …
AU  - Patel, R.
AU  - Domeniconi, C.
DA  - //
PY  - 2023
ST  - Enhancing Out-of-Vocabulary Estimation with Subword Attention
T2  - … for Computational Linguistics: ACL 2023
TI  - Enhancing Out-of-Vocabulary Estimation with Subword Attention
UR  - https://aclanthology.org/2023.findings-acl.221/
https://aclanthology.org/2023.findings-acl.221.pdf
ID  - 1146
ER  - 

TY  - JOUR
AB  - … Terms contained in Gene Ontology (GO) have been widely used in biology and bio-medicine. Most previous research focuses on inferring new GO terms, while the term names that re…
AU  - Zhang, Y.
AU  - Chen, Q.
AU  - Zhang, Y.
AU  - Wei, Z.
AU  - Gao, Y.
DA  - //
N1  - Cited By (since 2020): 8
PY  - 2020
ST  - Automatic term name generation for gene ontology: task and dataset
T2  - … Linguistics: EMNLP …
TI  - Automatic term name generation for gene ontology: task and dataset
UR  - https://aclanthology.org/2020.findings-emnlp.422/
https://aclanthology.org/2020.findings-emnlp.422.pdf
ID  - 1175
ER  - 

TY  - JOUR
AB  - … Data was collected through validated clinical tasks and paired … It is our hope that this dataset will offer high value to clinical … using fine-tuned versions of BERT, MentalBERT, and Mental-…
AU  - Aich, A.
AU  - Quynh, A.
AU  - Badal, V.
AU  - Pinkham, A.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Towards Intelligent Clinically-Informed Language Analyses of People with Bipolar Disorder and Schizophrenia
T2  - … Linguistics: EMNLP …
TI  - Towards Intelligent Clinically-Informed Language Analyses of People with Bipolar Disorder and Schizophrenia
UR  - https://aclanthology.org/2022.findings-emnlp.208/
https://aclanthology.org/2022.findings-emnlp.208.pdf
ID  - 1248
ER  - 

TY  - JOUR
AB  - Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence …
AU  - August, T.
AU  - Kim, L.
AU  - Reinecke, K.
DA  - //
N1  - Cited By (since 2020): 19
PY  - 2020
ST  - Writing strategies for science communication: Data and computational analysis
T2  - … Processing (EMNLP)
TI  - Writing strategies for science communication: Data and computational analysis
UR  - https://aclanthology.org/2020.emnlp-main.429/
https://aclanthology.org/2020.emnlp-main.429.pdf
ID  - 1377
ER  - 

TY  - JOUR
AB  - … In addition to a baseline Naive Bayes classifier, we fine-tune bert-base-multilingual-cased (mBERT) and xlm-roberta-base (XLM-RoBERTa) classifiers using Huggingface.Because of …
AU  - Belani, R.
AU  - Flanigan, J.
DA  - //
PY  - 2023
ST  - Automatic Identification of Code-Switching Functions in Speech Transcripts
T2  - … for Computational Linguistics: ACL 2023
TI  - Automatic Identification of Code-Switching Functions in Speech Transcripts
UR  - https://aclanthology.org/2023.findings-acl.469/
https://aclanthology.org/2023.findings-acl.469.pdf
ID  - 1345
ER  - 

TY  - JOUR
AB  - … Bert+ MLP:For this model, we experimented with context-aware version of Bert-based classifier as explained above. We freeze the first 8 layers of Bert and add a non-linear activation …
AU  - Bhat, M. M.
AU  - Hosseini, S.
AU  - Hassan, A.
DA  - //
N1  - Cited By (since 2021): 10
PY  - 2021
ST  - Say 'YES'to positivity: Detecting toxic language in workplace communications
T2  - … Linguistics: EMNLP …
TI  - Say 'YES'to positivity: Detecting toxic language in workplace communications
UR  - https://aclanthology.org/2021.findings-emnlp.173/
https://aclanthology.org/2021.findings-emnlp.173.pdf
ID  - 1308
ER  - 

TY  - JOUR
AB  - … that BERT has very low sensitivity to the concept of negation shows. This is not to say that we should stop using BERT; … Hurtful words: quantifying biases in clinical contextual word …
AU  - Bianchi, F.
AU  - Hovy, D.
DA  - //
N1  - Cited By (since 2021): 21
PY  - 2021
ST  - On the gap between adoption and understanding in NLP
T2  - … for Computational Linguistics: ACL-IJCNLP …
TI  - On the gap between adoption and understanding in NLP
UR  - https://aclanthology.org/2021.findings-acl.340.pdf
ID  - 1301
ER  - 

TY  - JOUR
AB  - In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to …
AU  - Brunato, D.
AU  - Dell'Orletta, F.
AU  - Dini, I.
DA  - //
PY  - 2023
ST  - Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages
T2  - … Linguistics: ACL 2023
TI  - Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages
UR  - https://aclanthology.org/2023.findings-acl.680/
https://aclanthology.org/2023.findings-acl.680.pdf
ID  - 1370
ER  - 

TY  - JOUR
AB  - … On the SST2 and QNLI datasets, we set the batch size to 64 and the learning rate to 2 × 10−5 using bert-baseuncased4 as the pre-trained model. On the MedSTS dataset, we set the …
AU  - Chen, S.
AU  - Mo, F.
AU  - Wang, Y.
AU  - Chen, C.
AU  - Nie, J. Y.
DA  - //
PY  - 2023
ST  - A Customized Text Sanitization Mechanism with Differential Privacy
T2  - … Linguistics: ACL …
TI  - A Customized Text Sanitization Mechanism with Differential Privacy
UR  - https://aclanthology.org/2023.findings-acl.355/
https://aclanthology.org/2023.findings-acl.355.pdf
ID  - 1323
ER  - 

TY  - JOUR
AB  - … Ben was slated for a clinical trial with an experimental drug.... ben was slated for a clinical … BERT: Pre-training of deep bidirectional transformers for language understanding. In …
AU  - Chen, Y.
AU  - Liu, P.
AU  - Qiu, X.
DA  - //
N1  - Cited By (since 2021): 8
PY  - 2021
ST  - Are Factuality Checkers Reliable? Adversarial Meta-evaluation of Factuality in Summarization
T2  - … for Computational Linguistics: EMNLP 2021
TI  - Are Factuality Checkers Reliable? Adversarial Meta-evaluation of Factuality in Summarization
UR  - https://aclanthology.org/2021.findings-emnlp.179/
https://aclanthology.org/2021.findings-emnlp.179.pdf
ID  - 1348
ER  - 

TY  - JOUR
AB  - Classifying temporal relations between a pair of events is crucial to natural language understanding and a well-known natural language processing task. Given a document and two …
AU  - Cohen, O.
AU  - Bar, K.
DA  - //
PY  - 2023
ST  - Temporal Relation Classification using Boolean Question Answering
T2  - … Association for Computational Linguistics: ACL …
TI  - Temporal Relation Classification using Boolean Question Answering
UR  - https://aclanthology.org/2023.findings-acl.116/
https://aclanthology.org/2023.findings-acl.116.pdf
ID  - 1359
ER  - 

TY  - JOUR
AB  - … All these variations of real clinical interviews are challenging compared to other general conversation datasets. These variations make our task more challenging. All the recordings …
AU  - Dawalatabad, N.
AU  - Gong, Y.
AU  - Khurana, S.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Detecting Dementia from Long Neuropsychological Interviews
T2  - … Linguistics: EMNLP …
TI  - Detecting Dementia from Long Neuropsychological Interviews
UR  - https://aclanthology.org/2022.findings-emnlp.386/
https://aclanthology.org/2022.findings-emnlp.386.pdf
ID  - 1340
ER  - 

TY  - JOUR
AB  - Suicide prevention hotline counselors aid individuals during difficult times through millions of calls and chats. A chatbot cannot safely replace a counselor, but we explore whether a …
AU  - Demasi, O.
AU  - Li, Y.
AU  - Yu, Z.
DA  - //
N1  - Cited By (since 2020): 8
PY  - 2020
ST  - A multi-persona chatbot for hotline counselor training
T2  - … for Computational Linguistics: EMNLP …
TI  - A multi-persona chatbot for hotline counselor training
UR  - https://aclanthology.org/2020.findings-emnlp.324/
https://aclanthology.org/2020.findings-emnlp.324.pdf
ID  - 1371
ER  - 

TY  - JOUR
AB  - … 2017): the fraction of instances in which a model is capable of fooling a fine-tuned BERT classifier with the above 95% accuracy on the development set of the classification task. Higher …
AU  - Diao, S.
AU  - Shen, X.
AU  - Shum, K.
AU  - Song, Y.
DA  - //
N1  - Cited By (since 2021): 13
PY  - 2021
ST  - TILGAN: transformer-based implicit latent GAN for diverse and coherent text generation
T2  - … Linguistics: ACL-IJCNLP …
TI  - TILGAN: transformer-based implicit latent GAN for diverse and coherent text generation
UR  - https://aclanthology.org/2021.findings-acl.428.pdf
ID  - 1331
ER  - 

TY  - JOUR
AB  - Purpose Adding external focus of attention (EF, focus on the movement effect) may optimize current anterior cruciate ligament (ACL) injury prevention programs. The purpose of the …
AU  - Diercks, R. L.
AU  - Lemmink, Kapm
DA  - //
PY  - 2015
ST  - Anne Benjaminse, Bert Otten, Alli Gokeler
T2  - Motor Learning in ACL Injury Prevention
TI  - Anne Benjaminse, Bert Otten, Alli Gokeler
UR  - https://core.ac.uk/download/pdf/232484959.pdf#page=76
ID  - 1267
ER  - 

TY  - JOUR
AB  - … 2 The clinical examples in this section are based on cumulative evidence of 2 key components that make up effective ACL injury prevention programs, 61 namely balance and …
AU  - Dowling, A. F.
DA  - //
PY  - 2015
ST  - Anne Benjaminse, Alli Gokeler, Ariel V. Dowling, Avery Faigenbaum, Kevin R. Ford, Timothy E. Hewett, James A. Onate, Bert Otten, Gregory D. Myer
T2  - Motor Learning in ACL Injury Prevention
TI  - Anne Benjaminse, Alli Gokeler, Ariel V. Dowling, Avery Faigenbaum, Kevin R. Ford, Timothy E. Hewett, James A. Onate, Bert Otten, Gregory D. Myer
UR  - https://research.rug.nl/files/24390583/Complete_thesis.pdf#page=48
ID  - 1259
ER  - 

TY  - JOUR
AB  - … As the result, the performance of these models is limited when they are applied to number-intensive applications in clinical and financial domains. In this work, we propose a simple …
AU  - Duan, H.
AU  - Yang, Y.
AU  - Tam, K. Y.
DA  - //
N1  - Cited By (since 2021): 4
PY  - 2021
ST  - Learning numeracy: a simple yet effective number embedding approach using knowledge graph
T2  - … for Computational Linguistics: EMNLP …
TI  - Learning numeracy: a simple yet effective number embedding approach using knowledge graph
UR  - https://aclanthology.org/2021.findings-emnlp.221/
https://aclanthology.org/2021.findings-emnlp.221.pdf
ID  - 1298
ER  - 

TY  - JOUR
AB  - … As a result, the standard question-answering BERT models that predict only the starting and ending token cannot provide multiple spans as answers. So, for extractive (eg, BanglaBERT…
AU  - Ekram, S. M. S.
AU  - Rahman, A. A.
AU  - Altaf, M. S.
DA  - //
PY  - 2022
ST  - BanglaRQA: A Benchmark Dataset for Under-resourced Bangla Language Reading Comprehension-based Question Answering with Diverse Question-Answer Types
T2  - … Linguistics: EMNLP …
TI  - BanglaRQA: A Benchmark Dataset for Under-resourced Bangla Language Reading Comprehension-based Question Answering with Diverse Question-Answer Types
UR  - https://aclanthology.org/2022.findings-emnlp.186/
https://aclanthology.org/2022.findings-emnlp.186.pdf
ID  - 1362
ER  - 

TY  - JOUR
AB  - … A qualitative analyses is also presented where we compare BERT and GloVe results of the five NCs in Table 1 (which shows the naturalistic sentences for each NC, together with their …
AU  - Garcia, M.
AU  - Vieira, T. K.
AU  - Scarton, C.
DA  - //
N1  - Cited By (since 2021): 37
PY  - 2021
ST  - Probing for idiomaticity in vector space models
T2  - Proceedings of the …
TI  - Probing for idiomaticity in vector space models
UR  - https://eprints.whiterose.ac.uk/170754/
https://eprints.whiterose.ac.uk/170754/7/2021.eacl-main.310.pdf
ID  - 1283
ER  - 

TY  - JOUR
AB  - … (LMs) pretrained on massive text corpus, such as BERT (… -3 and thus is not applicable to other rich LMs such as BERT (… on the source language models like BERT. The risks of language …
AU  - Hao, S.
AU  - Tan, B.
AU  - Tang, K.
AU  - Ni, B.
AU  - Shao, X.
DA  - //
PY  - 2023
ST  - BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models
T2  - … Linguistics: ACL …
TI  - BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models
UR  - https://aclanthology.org/2023.findings-acl.309/
https://aclanthology.org/2023.findings-acl.309.pdf
ID  - 1342
ER  - 

TY  - JOUR
AB  - … 2019), which involves computation over BERT token … -Twitter-BERT instead of spelling out the full model name. … Average embedding, both with GloVe and BERT embeddings, perform …
AU  - Hossain, T.
AU  - Iv, R. L. Logan
AU  - Ugarte, A.
DA  - //
N1  - Cited By (since 2020): 169
PY  - 2020
ST  - COVIDLies: Detecting COVID-19 misinformation on social media
T2  - … -19 (Part 2) at EMNLP …
TI  - COVIDLies: Detecting COVID-19 misinformation on social media
UR  - https://openreview.net/forum?id=FCna-s-ZaIE
https://openreview.net/pdf?id=FCna-s-ZaIE
ID  - 1296
ER  - 

TY  - JOUR
AB  - … and teacher annealing achieve clear improvements over the best BERT model and multitask training. Starting with the single task BERT baseline with an F1 score of 63.79, distilling …
AU  - Hosseini, M.
AU  - Caragea, C.
DA  - //
N1  - Cited By (since 2021): 11
PY  - 2021
ST  - Distilling knowledge for empathy detection
T2  - … for Computational Linguistics: EMNLP …
TI  - Distilling knowledge for empathy detection
UR  - https://aclanthology.org/2021.findings-emnlp.314/
https://aclanthology.org/2021.findings-emnlp.314.pdf
ID  - 1299
ER  - 

TY  - JOUR
AB  - … on validation data by a margin of more than 0.2 with the best baseline performance(BERT) (… , including Random Forest, SVM,CNN and BERT. Furthermore, the analysis of the results …
AU  - Jagadeesh, M. S.
AU  - Alphonse, P. J. A.
DA  - //
N1  - Cited By (since 2020): 12
PY  - 2020
ST  - NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets
T2  - W-NUT@ EMNLP
TI  - NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets
UR  - https://arxiv.org/abs/2011.05551
https://arxiv.org/pdf/2011.05551
ID  - 1274
ER  - 

TY  - JOUR
AB  - … The performance delta between BERT and GloVe does not justify the additional computation costs involved with using a BERT encoding for our problem space. Both neural NLP …
AU  - Khanna, S.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - Conical Classification For Efficient One-Class Topic Determination
T2  - … the Association for Computational Linguistics: EMNLP …
TI  - Conical Classification For Efficient One-Class Topic Determination
UR  - https://aclanthology.org/2021.findings-emnlp.143/
https://aclanthology.org/2021.findings-emnlp.143.pdf
ID  - 1334
ER  - 

TY  - JOUR
AB  - … in this section, and the experimental results based on BERT are reported in Appendix. … Empirical Result for BERT. We report empirical results for BERT in Table 9 and Table 10…
AU  - Kim, J.
AU  - Jung, K.
AU  - Na, D.
AU  - Jang, S.
AU  - Park, E.
DA  - //
PY  - 2023
ST  - Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers
T2  - … Linguistics: ACL 2023
TI  - Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers
UR  - https://aclanthology.org/2023.findings-acl.95/
https://aclanthology.org/2023.findings-acl.95.pdf
ID  - 1341
ER  - 

TY  - JOUR
AB  - … Issue: FDA Commissioner Acknowledges Misrepresenting Convalescent Plasma Data Claim A: The FDA made the decision based on data the Mayo Clinic collected from hospitals …
AU  - Ko, M.
AU  - Seong, I.
AU  - Lee, H.
AU  - Park, J.
AU  - Chang, M.
DA  - //
PY  - 2023
ST  - ClaimDiff: Comparing and Contrasting Claims on Contentious Issues
T2  - … Linguistics: ACL 2023
TI  - ClaimDiff: Comparing and Contrasting Claims on Contentious Issues
UR  - https://aclanthology.org/2023.findings-acl.289/
https://aclanthology.org/2023.findings-acl.289.pdf
ID  - 1365
ER  - 

TY  - JOUR
AB  - … To evaluate the comprehensiveness, we start with training a BERT-based sentiment analysis … Inferring which medical treatments work from reports of clinical trials. arXiv preprint arXiv:…
AU  - Kung, P. N.
AU  - Yang, T. H.
AU  - Chen, Y. C.
AU  - Yin, S. S.
DA  - //
N1  - Cited By (since 2020): 8
PY  - 2020
ST  - Zero-shot rationalization by multi-task transfer learning from question answering
T2  - … Linguistics: EMNLP …
TI  - Zero-shot rationalization by multi-task transfer learning from question answering
UR  - https://aclanthology.org/2020.findings-emnlp.198/
https://aclanthology.org/2020.findings-emnlp.198.pdf
ID  - 1333
ER  - 

TY  - JOUR
AB  - … We use the bert-base-multilingualcased2 model pre-trained on 104 languages as the encoder, which has 12 layers, 768-d hidden size, 12 heads and 110M total parameters. The …
AU  - Li, Z.
AU  - Kumar, M.
AU  - Headden, W.
AU  - Yin, B.
AU  - Wei, Y.
DA  - //
N1  - Cited By (since 2020): 28
PY  - 2020
ST  - Learn to cross-lingual transfer with meta graph learning across heterogeneous languages
T2  - … processing (EMNLP)
TI  - Learn to cross-lingual transfer with meta graph learning across heterogeneous languages
UR  - https://aclanthology.org/2020.emnlp-main.179/
https://aclanthology.org/2020.emnlp-main.179.pdf
ID  - 1347
ER  - 

TY  - JOUR
AB  - … Besides cBERT, we also investigate the effects of the copy block on BERT to further demonstrate its effectiveness. Table 9 illustrates the results. We observe that with the incorporation …
AU  - Liu, S.
AU  - Song, S.
AU  - Yue, T.
AU  - Yang, T.
AU  - Cai, H.
DA  - //
N1  - Cited By (since 2022): 6
PY  - 2022
ST  - CRASpell: A contextual typo robust approach to improve Chinese spelling correction
T2  - … Linguistics: ACL 2022
TI  - CRASpell: A contextual typo robust approach to improve Chinese spelling correction
UR  - https://aclanthology.org/2022.findings-acl.237/
https://aclanthology.org/2022.findings-acl.237.pdf
ID  - 1265
ER  - 

TY  - JOUR
AB  - … Compared with previous tagging-based and span-based methods like BERT-CRF and Two-Step, … We use average pooling representations encoded by BERT for arguments in (a) and …
AU  - Liu, W.
AU  - Cheng, S.
AU  - Zeng, D.
AU  - Hong, Q.
DA  - //
PY  - 2023
ST  - Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance
T2  - … Linguistics: ACL 2023
TI  - Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance
UR  - https://aclanthology.org/2023.findings-acl.817/
https://aclanthology.org/2023.findings-acl.817.pdf
ID  - 1332
ER  - 

TY  - JOUR
AB  - … More specifically, we consider the following modification to input of BERT for text classification: we append the corresponding class name after each training sentence (for which we …
AU  - Luo, Q.
AU  - Liu, L.
AU  - Lin, Y.
AU  - Zhang, W.
DA  - //
N1  - Cited By (since 2021): 31
PY  - 2021
ST  - Don't miss the labels: Label-semantic augmented meta-learner for few-shot text classification
T2  - … for Computational Linguistics: ACL …
TI  - Don't miss the labels: Label-semantic augmented meta-learner for few-shot text classification
UR  - https://aclanthology.org/2021.findings-acl.245.pdf
ID  - 1275
ER  - 

TY  - JOUR
AB  - Arthroscopic setup is crucial for a safe ACL reconstruction. The aim of this chapter is to describe anesthesiology procedures, prophylactic antibiotics, thromboprophylaxis, patient …
AU  - Miller, M.
AU  - Compagnoni, R.
AU  - Randelli, P.
DA  - //
DO  - 10.1007/978-3-662-52742-9_2
PY  - 2017
ST  - Arthroscopic Setup for ACL Reconstruction
T2  - … in the Technical Aspects of ACL …
TI  - Arthroscopic Setup for ACL Reconstruction
UR  - https://link.springer.com/chapter/10.1007/978-3-662-52742-9_2
ID  - 1343
ER  - 

TY  - JOUR
AB  - … We are aware of only three randomized clinical trials of thromboprophylaxis in knee arthroscopy patients that show the effectiveness and security of low-molecular-weight heparin (…
AU  - Montesinos-Berry, E.
AU  - Sanchis-Alfonso, V.
DA  - //
DO  - 10.1007/978-1-4471-4270-6_33
N1  - Cited By (since 2012): 1
PY  - 2012
ST  - Deep Venous Thrombosis and Pulmonary Embolism After ACL Reconstruction: What Can We Do to Prevent It?
T2  - … ACL-Deficient Knee: A …
TI  - Deep Venous Thrombosis and Pulmonary Embolism After ACL Reconstruction: What Can We Do to Prevent It?
UR  - https://link.springer.com/chapter/10.1007/978-1-4471-4270-6_33
ID  - 1369
ER  - 

TY  - JOUR
AB  - ACL 2022 will be a hybrid conference. After two fully virtual editions, ACL 2020 and ACL 2021, due to the covid-19 pandemic, this year we are gradually coming back to normality, …
AU  - Muresan, S.
AU  - Nakov, P.
AU  - Villavicencio, A.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Findings of the Association for Computational Linguistics: ACL 2022
T2  - … Linguistics: ACL 2022
TI  - Findings of the Association for Computational Linguistics: ACL 2022
UR  - https://aclanthology.org/2022.findings-acl.0.pdf
ID  - 1381
ER  - 

TY  - JOUR
AB  - … With this, we hypothesize that BERT can implicitly encode the different aspects of input for QG (Sun et al., 2018; Zhao et al., 2018) within the selfattentions across layers. As self-attention …
AU  - Neumann, Svsag
DA  - //
N1  - Cited By (since 2020): 1
PY  - 2020
ST  - Copybert: A unified approach to question generation with self-attention
T2  - ACL 2020
TI  - Copybert: A unified approach to question generation with self-attention
UR  - https://aclanthology.org/2020.nlp4convai-1.pdf#page=35
ID  - 1279
ER  - 

TY  - JOUR
AB  - Existing definitions of lexical substitutes are often vague or inconsistent with the gold annotations. We propose a new definition which is grounded in the relation of entailment; namely, …
AU  - Omarov, T.
AU  - Kondrak, G.
DA  - //
PY  - 2023
ST  - Grounding the Lexical Substitution Task in Entailment
T2  - … for Computational Linguistics: ACL 2023
TI  - Grounding the Lexical Substitution Task in Entailment
UR  - https://aclanthology.org/2023.findings-acl.179/
https://aclanthology.org/2023.findings-acl.179.pdf
ID  - 1351
ER  - 

TY  - JOUR
AB  - … a pre-trained BERT model5 with a constant learning rate of 5e−5. We use an AdamW optimizer (… A clinical text classification paradigm using weak supervision and deep representation. …
AU  - Park, S.
AU  - Kim, K.
AU  - Lee, J.
DA  - //
PY  - 2023
ST  - Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification
T2  - … for Computational Linguistics: ACL 2023
TI  - Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification
UR  - https://aclanthology.org/2023.findings-acl.328/
https://aclanthology.org/2023.findings-acl.328.pdf
ID  - 1346
ER  - 

TY  - JOUR
AB  - … clinical … BERT. This is a pre-trained model which is publicly available in GitHub. We use this model to fine-tune on our data. We set all details the same as described in Multilingual BERT …
AU  - Park, S.
AU  - Park, K.
AU  - Ahn, J.
AU  - Oh, A.
DA  - //
N1  - Cited By (since 2020): 1
PY  - 2020
ST  - Suicidal risk detection for military personnel
T2  - … Language Processing (EMNLP)
TI  - Suicidal risk detection for military personnel
UR  - https://aclanthology.org/2020.emnlp-main.198/
https://aclanthology.org/2020.emnlp-main.198.pdf
ID  - 1315
ER  - 

TY  - JOUR
AB  - The paper presents ongoing efforts in design of a typology of metacognitive events observed in a multimodal dialogue. The typology will serve as a tool to identify relations between …
AU  - Petukhova, V.
AU  - Manzoor, H. E.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - Towards the iso 24617-2-compliant typology of metacognitive events
T2  - … the 17th Joint ACL-ISO Workshop on …
TI  - Towards the iso 24617-2-compliant typology of metacognitive events
UR  - https://aclanthology.org/2021.isa-1.2/
https://aclanthology.org/2021.isa-1.2.pdf
ID  - 1383
ER  - 

TY  - JOUR
AB  - … For each NE in the text, a pair of start and end offsets are available: one pair extracted by the HuBERT based NE extraction from the original audio and a second pair from the audio …
AU  - Preiss, J.
DA  - //
PY  - 2023
ST  - Automatic Named Entity Obfuscation in Speech
T2  - … of the Association for Computational Linguistics: ACL …
TI  - Automatic Named Entity Obfuscation in Speech
UR  - https://aclanthology.org/2023.findings-acl.39/
https://aclanthology.org/2023.findings-acl.39.pdf
ID  - 1313
ER  - 

TY  - JOUR
AB  - Being able to reliably estimate self-disclosure–a key component of friendship and intimacy–from language is important for many psychology studies. We build single-task models on five …
AU  - Reuel, A. K.
AU  - Peralta, S.
AU  - Sedoc, J.
DA  - //
N1  - Cited By (since 2022): 3
PY  - 2022
ST  - Measuring the language of self-disclosure across corpora
T2  - … Linguistics: ACL 2022
TI  - Measuring the language of self-disclosure across corpora
UR  - https://aclanthology.org/2022.findings-acl.83/
https://aclanthology.org/2022.findings-acl.83.pdf
ID  - 1373
ER  - 

TY  - JOUR
AB  - … Demographics, personality, clinical conditions, political preferences influence what we speak about and how, suggesting that many individual attributes could be inferred from adequate …
AU  - Rocca, R.
AU  - Yarkoni, T.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Language as a fingerprint: Self-supervised learning of user encodings using transformers
T2  - … for Computational Linguistics: EMNLP 2022
TI  - Language as a fingerprint: Self-supervised learning of user encodings using transformers
UR  - https://aclanthology.org/2022.findings-emnlp.123/
https://aclanthology.org/2022.findings-emnlp.123.pdf
ID  - 1303
ER  - 

TY  - JOUR
AB  - … BERT vocabulary; thus, such numerals in the datasets are split into multiple numeral tokens by the BERT … into multiple numeral tokens by the BERT tokenizer in the preprocessing stage. …
AU  - Sakamoto, T.
AU  - Aizawa, A.
DA  - //
PY  - 2023
ST  - Predicting Numerals in Text Using Nearest Neighbor Language Models
T2  - … for Computational Linguistics: ACL 2023
TI  - Predicting Numerals in Text Using Nearest Neighbor Language Models
UR  - https://aclanthology.org/2023.findings-acl.295/
https://aclanthology.org/2023.findings-acl.295.pdf
ID  - 1326
ER  - 

TY  - JOUR
AB  - … For CE-BERT, we use Keras BERT2 uncased large model to get contextualized word representations, apply max pooling to all the word representations and then add a linear layer for …
AU  - Shi, W.
AU  - Wu, J.
AU  - Yang, X.
AU  - Chen, N.
AU  - Mien, I. H.
DA  - //
PY  - 2021
ST  - Analyzing Code Embeddings for Coding Clinical Narratives
T2  - … Linguistics: ACL …
TI  - Analyzing Code Embeddings for Coding Clinical Narratives
UR  - https://aclanthology.org/2021.findings-acl.410.pdf
ID  - 1244
ER  - 

TY  - JOUR
AB  - … We trained a Japanese pre-trained BERT model using Japanese Wikipedia, which consists of … The parameters of BERT are the same as English BERTBASE. The number of epochs for …
AU  - Shibata, N. T. T.
AU  - Kawahara, D.
DA  - //
PY  - 2019
ST  - Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument Structure Analysis
T2  - EMNLP 2019 …
TI  - Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument Structure Analysis
UR  - http://aclanthology.lst.uni-saarland.de/D19-58.pdf#page=110
ID  - 1335
ER  - 

TY  - JOUR
AB  - This chapter summarizes the current evidence and knowledge regarding ACL injuries in female soccer athletes. Epidemiology injury data are presented for both male and female …
AU  - Silvers, H. J.
AU  - Mandelbaum, B. R.
DA  - //
DO  - 10.1007/978-3-642-32592-2_16
N1  - Cited By (since 2012): 1
PY  - 2012
ST  - ACL Injury Prevention in Soccer: The Santa Monica Experience
T2  - ACL Injuries in the Female Athlete: Causes …
TI  - ACL Injury Prevention in Soccer: The Santa Monica Experience
UR  - https://link.springer.com/chapter/10.1007/978-3-642-32592-2_16
ID  - 1353
ER  - 

TY  - JOUR
AB  - This chapter summarizes current data related to the epidemiology of soccer-related injuries and injury prevention programs designed to reduce the risk of these injuries. Studies …
AU  - Silvers-Granelli, H. J.
AU  - Brophy, R. H.
DA  - //
DO  - 10.1007/978-3-662-56558-2_19
PY  - 2018
ST  - ACL Injury Prevention in Soccer: The Santa Monica Experience
T2  - ACL Injuries in the Female …
TI  - ACL Injury Prevention in Soccer: The Santa Monica Experience
UR  - https://link.springer.com/chapter/10.1007/978-3-662-56558-2_19
ID  - 1363
ER  - 

TY  - JOUR
AB  - … model (Clinical Filtered CNet) over the BERT model with no … Next, using our best Clinical Filtered CNet BERT model, we … mislabeled classes by BERT and Clinical BERT, and observe a …
AU  - Sosea, T.
AU  - Caragea, C.
DA  - //
N1  - Cited By (since 2020): 20
PY  - 2020
ST  - Canceremo: A dataset for fine-grained emotion detection
T2  - … in Natural Language Processing (EMNLP)
TI  - Canceremo: A dataset for fine-grained emotion detection
UR  - https://aclanthology.org/2020.emnlp-main.715/
https://aclanthology.org/2020.emnlp-main.715.pdf
ID  - 1242
ER  - 

TY  - JOUR
AB  - … BERT embeddings (Devlin et al., 2019) (ie, the base model) are employed to encode the sentences. Both ELMo and BERT … We also see that replacing ELMo with the BERT embeddings …
AU  - Tran, M. P.
AU  - Nguyen, M. V.
AU  - Nguyen, T. H.
DA  - //
N1  - Cited By (since 2021): 3
PY  - 2021
ST  - Fine-grained temporal relation extraction with ordered-neuron LSTM and graph convolutional networks
T2  - … Text (W-NUT 2021) at EMNLP 2021
TI  - Fine-grained temporal relation extraction with ordered-neuron LSTM and graph convolutional networks
UR  - https://par.nsf.gov/servlets/purl/10309672
ID  - 1358
ER  - 

TY  - JOUR
AB  - … Regarding the event detection task, our baselines include adaption results of BERT and BERT+Adapter models fine-tuned using only source dataset, and finally BERT+DANN which …
AU  - Trung, N. N.
AU  - Phung, D.
AU  - Nguyen, T. H.
DA  - //
N1  - Cited By (since 2021): 10
PY  - 2021
ST  - Unsupervised domain adaptation for event detection using domain-specific adapters
T2  - … Computational Linguistics: ACL …
TI  - Unsupervised domain adaptation for event detection using domain-specific adapters
UR  - https://aclanthology.org/2021.findings-acl.351.pdf
ID  - 1291
ER  - 

TY  - JOUR
AB  - … bert(hist) which uses the same architecture and hyperparameters for tuning as the original BiLSTMbert but … In Proceedings of the sixth workshop on computational linguistics and clinical …
AU  - Tseriotou, T.
AU  - Tsakalidis, A.
AU  - Foster, P.
DA  - //
PY  - 2023
ST  - Sequential Path Signature Networks for Personalised Longitudinal Language Modeling
T2  - … Linguistics: ACL …
TI  - Sequential Path Signature Networks for Personalised Longitudinal Language Modeling
UR  - https://aclanthology.org/2023.findings-acl.310/
https://aclanthology.org/2023.findings-acl.310.pdf
ID  - 1319
ER  - 

TY  - JOUR
AB  - … Information retrieval and extraction on COVID-19 clinical articles using graph community detection and Bio-BERT embeddings. In Proceedings of the 1st Workshop on NLP for COVID19 …
AU  - Verspoor, K.
AU  - Cohen, K. B.
AU  - Dredze, M.
DA  - //
N1  - Cited By (since 2020): 6
PY  - 2020
ST  - Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020
T2  - … for COVID-19 at ACL …
TI  - Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020
UR  - https://aclanthology.org/2020.nlpcovid19-acl.0.pdf
ID  - 1289
ER  - 

TY  - JOUR
AB  - … Note SciBERT and BioBERT are the variants of BERT. T5 is the prior state-of-the-art effective… BERT: Pre-training of deep bidirectional transformers for language understanding. In …
AU  - Wang, L.
AU  - Zhang, P.
AU  - Lu, X.
AU  - Zhang, L.
AU  - Yan, C.
DA  - //
PY  - 2022
ST  - QaDialMoE: Question-answering Dialogue based Fact Verification with Mixture of Experts
T2  - … Linguistics: EMNLP …
TI  - QaDialMoE: Question-answering Dialogue based Fact Verification with Mixture of Experts
UR  - https://aclanthology.org/2022.findings-emnlp.229/
https://aclanthology.org/2022.findings-emnlp.229.pdf
ID  - 1338
ER  - 

TY  - JOUR
AB  - … and cost, we also report the performance of the BERT-base on the WOS dataset with these … results of this study show that as patients and hcs have similar clinical phenotypes with high …
AU  - Wang, Y.
AU  - Qiao, D.
AU  - Li, J.
AU  - Chang, J.
AU  - Zhang, Q.
DA  - //
PY  - 2023
ST  - Towards Better Hierarchical Text Classification with Data Generation
T2  - … Linguistics: ACL …
TI  - Towards Better Hierarchical Text Classification with Data Generation
UR  - https://aclanthology.org/2023.findings-acl.489/
https://aclanthology.org/2023.findings-acl.489.pdf
ID  - 1314
ER  - 

TY  - JOUR
AB  - … This dissertation started with an investigation of the clinical relevance of the sex differences during sidestep cutting. An overview of sex differences in sidestep cutting technique was …
AU  - Welling, W.
AU  - Benjaminse, A.
AU  - Otten, B.
DA  - //
PY  - 2015
ST  - Enhanced retention of drop vertical jump landing strategies: a randomized controlled trial
T2  - Motor Learning in ACL …
TI  - Enhanced retention of drop vertical jump landing strategies: a randomized controlled trial
UR  - https://core.ac.uk/download/pdf/232484959.pdf#page=100
ID  - 1350
ER  - 

TY  - JOUR
AB  - Multimodal fusion addresses the problem of analyzing spoken words in the multimodal context, including visual expressions and prosodic cues. Even when multimodal models lead to …
AU  - Wörtwein, T.
AU  - Sheeber, L.
AU  - Allen, N.
AU  - Cohn, J.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions
T2  - … Linguistics: EMNLP …
TI  - Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions
UR  - https://aclanthology.org/2022.findings-emnlp.344/
https://aclanthology.org/2022.findings-emnlp.344.pdf
ID  - 1368
ER  - 

TY  - JOUR
AB  - … For comparison, we plot the results of two baselines for each group of users (BERT-part) and for all the users (BERT-all) respectively in Figure 3. One notable phenomenon is that PQ-…
AU  - Yang, F.
AU  - Yang, T.
AU  - Quan, X.
AU  - Su, Q.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Learning to answer psychological questionnaire for personality detection
T2  - … Linguistics: EMNLP 2021
TI  - Learning to answer psychological questionnaire for personality detection
UR  - https://aclanthology.org/2021.findings-emnlp.98/
https://aclanthology.org/2021.findings-emnlp.98.pdf
ID  - 1309
ER  - 

TY  - JOUR
AB  - … Moreover, we can observe that BERT does not outperform all baselines on the other three … Constructing fine-grained entity recognition corpora based on clinical records of traditional …
AU  - Yang, J.
AU  - Luo, J.
AU  - Guo, W.
AU  - Niu, D.
AU  - Xu, Y.
DA  - //
PY  - 2023
ST  - Exploiting Hierarchically Structured Categories in Fine-grained Chinese Named Entity Recognition
T2  - … Linguistics: ACL 2023
TI  - Exploiting Hierarchically Structured Categories in Fine-grained Chinese Named Entity Recognition
UR  - https://aclanthology.org/2023.findings-acl.211/
https://aclanthology.org/2023.findings-acl.211.pdf
ID  - 1325
ER  - 

TY  - JOUR
AB  - … models from the BERT family and GPT-2 variants. Zhou and Chen … 2022) finetune BERT and RoBERTa models for the … shot prompt with GPT-3 for the extraction of clinical relations. …
AU  - Yang, Z.
AU  - Ishay, A.
AU  - Lee, J.
DA  - //
PY  - 2023
ST  - Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
T2  - … for Computational Linguistics: ACL 2023
TI  - Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
UR  - https://aclanthology.org/2023.findings-acl.321/
https://aclanthology.org/2023.findings-acl.321.pdf
ID  - 1366
ER  - 

TY  - JOUR
AB  - … During training, we start from a pre-trained BERT model3, and then fine-tune it using our … 2019b) discovered synonyms on privacy-aware clinical data by utilizing the surface form …
AU  - Yu, J.
AU  - Shen, Y.
AU  - Ma, X.
AU  - Jia, C.
AU  - Chen, C.
DA  - //
N1  - Cited By (since 2020): 3
PY  - 2020
ST  - Synet: Synonym expansion using transitivity
T2  - … Linguistics: EMNLP 2020
TI  - Synet: Synonym expansion using transitivity
UR  - https://aclanthology.org/2020.findings-emnlp.177/
https://aclanthology.org/2020.findings-emnlp.177.pdf
ID  - 1311
ER  - 

TY  - JOUR
AB  - … history of patients and providing clinical advice. They can also … Given a sequence of tokens, BERT randomly marks out some … In BERT-GPT, the BERT encoder generates representation …
AU  - Zeng, G.
AU  - Yang, W.
AU  - Ju, Z.
AU  - Yang, Y.
AU  - Wang, S.
DA  - //
N1  - Cited By (since 2020): 82
PY  - 2020
ST  - MedDialog: Large-scale medical dialogue datasets
T2  - … Processing (EMNLP)
TI  - MedDialog: Large-scale medical dialogue datasets
UR  - https://aclanthology.org/2020.emnlp-main.743/
https://aclanthology.org/2020.emnlp-main.743.pdf
ID  - 1277
ER  - 

TY  - JOUR
AB  - … Given a discourse with k-1 DTUs, we use the pretrained Bert3 model to encode the entire discourse where each DTU is surrounded by the [CLS] and [SEP] tokens. And we take the Bert …
AU  - Zhang, L.
AU  - Tan, X.
AU  - Kong, F.
AU  - Zhou, G.
DA  - //
PY  - 2021
ST  - EDTC: A Corpus for Discourse-Level Topic Chain Parsing
T2  - … Linguistics: EMNLP 2021
TI  - EDTC: A Corpus for Discourse-Level Topic Chain Parsing
UR  - https://aclanthology.org/2021.findings-emnlp.113/
https://aclanthology.org/2021.findings-emnlp.113.pdf
ID  - 1318
ER  - 

TY  - JOUR
AB  - A promising approach to estimate the causal effects of peer review policies is to analyze data from publication venues that shift policies from single-blind to double-blind from one year …
AU  - Zhang, R.
AU  - Kennard, N. N.
AU  - Smith, D.
DA  - //
PY  - 2023
ST  - Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies
T2  - … Linguistics: ACL …
TI  - Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies
UR  - https://aclanthology.org/2023.findings-acl.83/
https://aclanthology.org/2023.findings-acl.83.pdf
ID  - 1367
ER  - 

TY  - JOUR
AB  - … We adopt LSTM and BERT models as our encoder, and other models can also be applied to our framework. We obtain the contextaware word representations x “ rx1,x2, ..., x|T|s, where …
AU  - Zhou, J.
AU  - Wu, Y.
AU  - Chen, Q.
AU  - Huang, X. J.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Attending via both fine-tuning and compressing
T2  - … Linguistics: ACL-IJCNLP …
TI  - Attending via both fine-tuning and compressing
UR  - https://aclanthology.org/2021.findings-acl.189.pdf
ID  - 1307
ER  - 

TY  - JOUR
AB  - … SVM and BERT-based classifiers were tested to recognize condolence in comments, using the same setup as those for recognizing distress. Performance at recognizing condolence, …
AU  - Zhou, N.
AU  - Jurgens, D.
DA  - //
N1  - Cited By (since 2020): 31
PY  - 2020
ST  - Condolence and empathy in online communities
T2  - … in Natural Language Processing (EMNLP)
TI  - Condolence and empathy in online communities
UR  - https://aclanthology.org/2020.emnlp-main.45/
https://aclanthology.org/2020.emnlp-main.45.pdf
ID  - 1322
ER  - 

TY  - JOUR
AB  - … BERT model using the two QA approaches are also shown. Using the span extraction approach, BERT … Clicr: a dataset of clinical case reports for machine reading comprehension. In …
AU  - Zhu, M.
AU  - Ahuja, A.
AU  - Juan, D. C.
AU  - Wei, W.
DA  - //
N1  - Cited By (since 2020): 42
PY  - 2020
ST  - Question answering with long multiple-span answers
T2  - … Linguistics: EMNLP …
TI  - Question answering with long multiple-span answers
UR  - https://aclanthology.org/2020.findings-emnlp.342/
https://aclanthology.org/2020.findings-emnlp.342.pdf
ID  - 1281
ER  - 

TY  - JOUR
AB  - … layer of BERT 1. To … of BERT (BERT embedding for short). However, by investigating the 5 nearest neighbours (NN) of each word in fMRI180 using BERT embedding, we find that BERT …
AU  - Zou, S.
AU  - Wang, S.
AU  - Zhang, J.
AU  - Zong, C.
DA  - //
N1  - Cited By (since 2022): 2
PY  - 2022
ST  - Cross-modal cloze task: A new task to brain-to-word decoding
T2  - … Linguistics: ACL 2022
TI  - Cross-modal cloze task: A new task to brain-to-word decoding
UR  - https://aclanthology.org/2022.findings-acl.54/
https://aclanthology.org/2022.findings-acl.54.pdf
ID  - 1310
ER  - 

TY  - JOUR
AB  - … and recent advances in dialog agents, such as OpenAI’s GPT- … of an NLG system that summarises clinical data. In Proceedings … We used the following format for the prompt for GPT-3: …
AU  - Lee, S.
AU  - DeLucia, A.
AU  - Nangia, N.
AU  - Ganedi, P.
DA  - //
PY  - 2023
ST  - Common Law Annotations: Investigating the Stability of Dialog System Output Annotations
T2  - … Linguistics: ACL …
TI  - Common Law Annotations: Investigating the Stability of Dialog System Output Annotations
UR  - https://aclanthology.org/2023.findings-acl.780/
https://aclanthology.org/2023.findings-acl.780.pdf
ID  - 1401
ER  - 

TY  - JOUR
AB  - … 3) we evaluate, together with a clinical psychologist with diagnosis expertise, the faux pas … In later tests with direct access to the API (gpt-3.5-turbo-0301), it turns out that the advantage …
AU  - Shapira, N.
AU  - Zwirn, G.
AU  - Goldberg, Y.
DA  - //
PY  - 2023
ST  - How Well Do Large Language Models Perform on Faux Pas Tests?
T2  - … Computational Linguistics: ACL …
TI  - How Well Do Large Language Models Perform on Faux Pas Tests?
UR  - https://aclanthology.org/2023.findings-acl.663/
https://aclanthology.org/2023.findings-acl.663.pdf
ID  - 1388
ER  - 

TY  - JOUR
AB  - … Because recent pre-trained models, such as the GPT series, are language models trained on a large amount of training data, experimental results in this section give an insight for pre-…
AU  - Takase, S.
AU  - Kiyono, S.
AU  - Kobayashi, S.
DA  - //
PY  - 2023
ST  - B2T Connection: Serving Stability and Performance in Deep Transformers
T2  - … Linguistics: ACL 2023
TI  - B2T Connection: Serving Stability and Performance in Deep Transformers
UR  - https://aclanthology.org/2023.findings-acl.192/
https://aclanthology.org/2023.findings-acl.192.pdf
ID  - 1409
ER  - 

TY  - JOUR
AB  - … Knowledge-enriched transformer for emotion detection in textual conversations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the …
AU  - Gollapalli, S. D.
AU  - Rozenshtein, P.
DA  - //
N1  - Cited By (since 2020): 9
PY  - 2020
ST  - ESTeR: Combining word co-occurrences and word associations for unsupervised emotion detection
T2  - … Linguistics: EMNLP 2020
TI  - ESTeR: Combining word co-occurrences and word associations for unsupervised emotion detection
UR  - https://aclanthology.org/2020.findings-emnlp.93/
https://aclanthology.org/2020.findings-emnlp.93.pdf
ID  - 1535
ER  - 

TY  - JOUR
AB  - … - @siggyflicker I understand that, but unless the clinical studies are throughly completed (… Most of the self-supervised techniques for latent text representation rely on Transformer architec…
AU  - Lwowski, B.
AU  - Najafirad, P.
DA  - //
N1  - Cited By (since 2020): 11
PY  - 2020
ST  - COVID-19 surveillance through Twitter using self-supervised and few shot learning
T2  - … on NLP for COVID-19 (Part 2) at EMNLP …
TI  - COVID-19 surveillance through Twitter using self-supervised and few shot learning
UR  - https://aclanthology.org/2020.nlpcovid19-2.9/
https://aclanthology.org/2020.nlpcovid19-2.9.pdf
ID  - 1475
ER  - 

TY  - JOUR
AB  - … zero-shot set, also improving over GlossBERT by 4.3 points. This demonstrates that, when … plex expressions, such as free phrases (eg wrong medicine or hot forehead), that are rarely …
AU  - Bevilacqua, M.
AU  - Maru, M.
AU  - Navigli, R.
DA  - //
N1  - Cited By (since 2020): 43
PY  - 2020
ST  - Generationary or “how we went beyond word sense inventories and learned to gloss”
T2  - … Language Processing (EMNLP …
TI  - Generationary or “how we went beyond word sense inventories and learned to gloss”
UR  - https://iris.uniroma1.it/handle/11573/1465840
https://iris.uniroma1.it/bitstream/11573/1465840/1/Bevilacqua_Generationary_2020.pdf
ID  - 1713
ER  - 

TY  - JOUR
AB  - … models to a dataset tailored to the medical domain, which is quite different … a similar in-KB accuracy on the aida-A dataset, the bert-large-… in overall performance over bert-large-uncased. …
AU  - Blair, P.
AU  - Bar, K.
DA  - //
PY  - 2022
ST  - Improving Few-Shot Domain Transfer for Named Entity Disambiguation with Pattern Exploitation
T2  - … Association for Computational Linguistics: EMNLP …
TI  - Improving Few-Shot Domain Transfer for Named Entity Disambiguation with Pattern Exploitation
UR  - https://aclanthology.org/2022.findings-emnlp.506/
https://aclanthology.org/2022.findings-emnlp.506.pdf
ID  - 1734
ER  - 

TY  - JOUR
AB  - … fail to train with adapter modules for transferring BERT style parameters across NLP tasks. … It contains sentences from twelve domains including News, Spoken, Laws and Medical etc. …
AU  - Deng, Y.
AU  - Yu, H.
AU  - Yu, H.
AU  - Duan, X.
AU  - Luo, W.
DA  - //
N1  - Cited By (since 2020): 3
PY  - 2020
ST  - Factorized transformer for multi-domain neural machine translation
T2  - … Linguistics: EMNLP 2020
TI  - Factorized transformer for multi-domain neural machine translation
UR  - https://aclanthology.org/2020.findings-emnlp.377/
https://aclanthology.org/2020.findings-emnlp.377.pdf
ID  - 1736
ER  - 

TY  - JOUR
AB  - … version, and +3 points improvement over the previous BERT-based stateof-the-art model. … of "gender dysphoria , "which would require medical treatment , from being admitted to the …
AU  - Devatine, N.
AU  - Muller, P.
AU  - Braud, C.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Predicting Political Orientation in News with Latent Discourse Structure to Improve Bias Understanding
T2  - 3rd Workshop on Computational …
TI  - Predicting Political Orientation in News with Latent Discourse Structure to Improve Bias Understanding
UR  - https://hal.science/hal-03877859/
https://hal.science/hal-03877859/file/2022.codi-1.10.pdf
ID  - 1676
ER  - 

TY  - JOUR
AB  - … a simple BERTbased model for this task. Specifically, We employ a pretrained BERT model (… , besides the parameters of BERT itself, ERIN(Bert-base) and ERIN(Bert-large) include …
AU  - Ding, N.
AU  - Hu, C.
AU  - Sun, K.
AU  - Mensah, S.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Explicit role interaction network for event argument extraction
T2  - … Linguistics: EMNLP 2022
TI  - Explicit role interaction network for event argument extraction
UR  - https://aclanthology.org/2022.findings-emnlp.254/
https://aclanthology.org/2022.findings-emnlp.254.pdf
ID  - 1706
ER  - 

TY  - JOUR
AB  - … can no longer afford the expensive medical expenses (?3). He … model “BERTBase, Chinese”. We apply the linear warmup … be greatly improved by using BERT as the clause encoder. In …
AU  - Ding, Z.
AU  - Xia, R.
AU  - Yu, J.
DA  - //
N1  - Cited By (since 2020): 55
PY  - 2020
ST  - End-to-end emotion-cause pair extraction based on sliding window multi-label learning
T2  - … in natural language processing (EMNLP)
TI  - End-to-end emotion-cause pair extraction based on sliding window multi-label learning
UR  - https://aclanthology.org/2020.emnlp-main.290/
https://aclanthology.org/2020.emnlp-main.290.pdf
ID  - 1657
ER  - 

TY  - JOUR
AB  - … As we evaluate all the different variables, we observe that TweetBERT, linear regression and least confidence strategy yield comparable results as non-Active Learning methods yet …
AU  - Farinneya, P.
AU  - Pour, M. M. A.
AU  - Hamidian, S.
DA  - //
N1  - Cited By (since 2021): 6
PY  - 2021
ST  - Active learning for rumor identification on social media
T2  - … Linguistics: EMNLP …
TI  - Active learning for rumor identification on social media
UR  - https://aclanthology.org/2021.findings-emnlp.387/
https://aclanthology.org/2021.findings-emnlp.387.pdf
ID  - 1663
ER  - 

TY  - JOUR
AB  - … by the BERT specific [SEP] limiter, similar to Section 3.2… • AId → Regression Tasks: We use the BERT encoder output as … • Regression Tasks → AId: ED and AQ use the BERT …
AU  - Fromm, M.
AU  - Berrendorf, M.
AU  - Faerman, E.
DA  - //
PY  - 2023
ST  - Cross-Domain Argument Quality Estimation
T2  - … Linguistics: ACL 2023
TI  - Cross-Domain Argument Quality Estimation
UR  - https://aclanthology.org/2023.findings-acl.848/
https://aclanthology.org/2023.findings-acl.848.pdf
ID  - 1729
ER  - 

TY  - JOUR
AB  - … Particularly, we specify the encoder and the decoder to be fine-tuned BERT and GPT-2 … the need situations (eg, the need for water or medical aid) and the issue situations (eg, crime vio…
AU  - Guo, K.
AU  - Yu, H.
AU  - Liao, C.
AU  - Li, J.
AU  - Zhang, H.
DA  - //
PY  - 2023
ST  - ZeroAE: Pre-trained Language Model based Autoencoder for Transductive Zero-shot Text Classification
T2  - … Linguistics: ACL 2023
TI  - ZeroAE: Pre-trained Language Model based Autoencoder for Transductive Zero-shot Text Classification
UR  - https://aclanthology.org/2023.findings-acl.200/
https://aclanthology.org/2023.findings-acl.200.pdf
ID  - 1668
ER  - 

TY  - JOUR
AB  - … It consists of financial and medical texts from real scenarios and the corresponding manual … To ensure fairness in the number of parameters, we replaced BERT-Base with ROBERTA-…
AU  - Guo, X.
AU  - Deng, W.
AU  - Chen, Y.
AU  - Li, Y.
AU  - Zhou, M.
DA  - //
PY  - 2023
ST  - CoMave: Contrastive Pre-training with Multi-scale Masking for Attribute Value Extraction
T2  - … Linguistics: ACL …
TI  - CoMave: Contrastive Pre-training with Multi-scale Masking for Attribute Value Extraction
UR  - https://aclanthology.org/2023.findings-acl.373/
https://aclanthology.org/2023.findings-acl.373.pdf
ID  - 1697
ER  - 

TY  - JOUR
AB  - … Q: Discovered in 1886 by Clemens Winkler, this element is used in glass in infrared optical devices, its oxide has been used in medicine, and its dioxide is used to produce glass with a …
AU  - He, W.
AU  - Mao, A.
AU  - Boyd-Graber, J.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain QA
T2  - … Computational Linguistics: EMNLP …
TI  - Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain QA
UR  - https://aclanthology.org/2022.findings-emnlp.266/
https://aclanthology.org/2022.findings-emnlp.266.pdf
ID  - 1725
ER  - 

TY  - JOUR
AB  - … Second, RoBERTa and XLNet obtain roughly the same results with the new pairs including negation, but BERT falls slightly … BMC medical informatics and decision making, 5(1):13. …
AU  - Hossain, M. M.
AU  - Kovatchev, V.
AU  - Dutta, P.
AU  - Kao, T.
DA  - //
N1  - Cited By (since 2020): 60
PY  - 2020
ST  - An analysis of natural language inference benchmarks through the lens of negation
T2  - … Processing (EMNLP)
TI  - An analysis of natural language inference benchmarks through the lens of negation
UR  - https://aclanthology.org/2020.emnlp-main.732/
https://aclanthology.org/2020.emnlp-main.732.pdf
ID  - 1751
ER  - 

TY  - JOUR
AB  - Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) …
AU  - Huang, K.
AU  - Huang, D.
AU  - Liu, Z.
AU  - Mo, F.
DA  - //
N1  - Cited By (since 2020): 15
PY  - 2020
ST  - A joint multiple criteria model in transfer learning for cross-domain chinese word segmentation
T2  - … language processing (EMNLP)
TI  - A joint multiple criteria model in transfer learning for cross-domain chinese word segmentation
UR  - https://aclanthology.org/2020.emnlp-main.318/
https://aclanthology.org/2020.emnlp-main.318.pdf
ID  - 1681
ER  - 

TY  - JOUR
AB  - … The extended BERT model is then used for both LM pre-… Chinese NER methods, including BERT and ERNIE (Sun et al.… ison, we also complement both BERT and ERNIE with our entity …
AU  - Jia, C.
AU  - Shi, Y.
AU  - Yang, Q.
AU  - Zhang, Y.
DA  - //
N1  - Cited By (since 2020): 35
PY  - 2020
ST  - Entity enhanced BERT pre-training for Chinese NER
T2  - … Language Processing (EMNLP)
TI  - Entity enhanced BERT pre-training for Chinese NER
UR  - https://aclanthology.org/2020.emnlp-main.518/
https://aclanthology.org/2020.emnlp-main.518.pdf
ID  - 1587
ER  - 

TY  - JOUR
AB  - … ) is collected from 270 medical journals over a five-year period (19871991) with 23 … 2020) for BERT and specifically we use bert-base-uncased checkpoint for BERT and bert-…
AU  - Jiang, Z.
AU  - Yang, M.
AU  - Tsirlin, M.
AU  - Tang, R.
DA  - //
PY  - 2023
ST  - “Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors
T2  - … Linguistics: ACL 2023
TI  - “Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors
UR  - https://aclanthology.org/2023.findings-acl.426/
https://aclanthology.org/2023.findings-acl.426.pdf
ID  - 1682
ER  - 

TY  - JOUR
AB  - … news classification task, in detecting the reports of medical drug side-effects (the ADR task), … We include the pretrained BERT model (base version) followed by one layer fully connected …
AU  - Karisani, P.
DA  - //
PY  - 2023
ST  - Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets
T2  - … of the Association for Computational Linguistics: ACL …
TI  - Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets
UR  - https://aclanthology.org/2023.findings-acl.769/
https://aclanthology.org/2023.findings-acl.769.pdf
ID  - 1718
ER  - 

TY  - JOUR
AB  - … BERT model. The results show that our results are comparable and, in some settings, even better than BERT’s.… in Table 1, the target of both topic and claim is medical marijuana. Our …
AU  - Kobbe, J.
AU  - Hulpuș, I.
DA  - //
N1  - Cited By (since 2020): 18
PY  - 2020
ST  - Unsupervised stance detection for arguments from consequences
T2  - … processing (EMNLP)
TI  - Unsupervised stance detection for arguments from consequences
UR  - https://aclanthology.org/2020.emnlp-main.4/?rs=true&
https://aclanthology.org/2020.emnlp-main.4.pdf
ID  - 1602
ER  - 

TY  - JOUR
AB  - … In the field of medicine, we only use publications from Semantic Scholar that originate from Medline, a medical publication database. Due to a high number of venues with few …
AU  - Kobs, K.
AU  - Koopmann, T.
AU  - Zehe, A.
AU  - Fernes, D.
DA  - //
N1  - Cited By (since 2020): 3
PY  - 2020
ST  - Where to Submit? Helping Researchers to Choose the Right Venue
T2  - … Linguistics: EMNLP …
TI  - Where to Submit? Helping Researchers to Choose the Right Venue
UR  - https://aclanthology.org/2020.findings-emnlp.78/
https://aclanthology.org/2020.findings-emnlp.78.pdf
ID  - 1588
ER  - 

TY  - JOUR
AB  - … Overall performance Using BERT, we started with an initial sequence length of 128, batch size of 32, and 5 epochs of fine-tuning but otherwise standard parameter choices to see how …
AU  - Körner, E.
AU  - Hakimi, A. D.
AU  - Heyer, G.
DA  - //
PY  - 2021
ST  - Casting the Same Sentiment Classification Problem
T2  - … Linguistics: EMNLP 2021
TI  - Casting the Same Sentiment Classification Problem
UR  - https://aclanthology.org/2021.findings-emnlp.53/
https://aclanthology.org/2021.findings-emnlp.53.pdf
ID  - 1628
ER  - 

TY  - JOUR
AB  - … life.die.death-caused-by-violent-events: ... the medical examiner believed the manner of … We examine two state-of-theart BERT-based sentence encoders φ for ED, ie BERTMLP (Yang …
AU  - Lai, V.
AU  - Dernoncourt, F.
AU  - Nguyen, T. H.
DA  - //
N1  - Cited By (since 2021): 22
PY  - 2021
ST  - Learning prototype representations across few-shot tasks for event detection
T2  - … Language Processing (EMNLP 2021 …
TI  - Learning prototype representations across few-shot tasks for event detection
UR  - https://par.nsf.gov/servlets/purl/10309674
ID  - 1743
ER  - 

TY  - JOUR
AB  - … Our models outperform previous baselines, one of which is BERT-based, by a substantial … domain at hand, eg statistical terms in the medical domain. Since the time of the challenge the …
AU  - Lang, C.
AU  - Wachowiak, L.
AU  - Heinisch, B.
DA  - //
N1  - Cited By (since 2021): 12
PY  - 2021
ST  - Transforming term extraction: transformer-based approaches to multilingual term extraction across domains
T2  - … Linguistics: ACL …
TI  - Transforming term extraction: transformer-based approaches to multilingual term extraction across domains
UR  - https://aclanthology.org/2021.findings-acl.316.pdf
ID  - 1630
ER  - 

TY  - JOUR
AB  - … get a high uncertainty score from an expert trained on a medical QA dataset. In practice, we leverage … Multi-passage BERT: A globally normalized BERT model for open-domain question …
AU  - Li, M.
AU  - Li, M.
AU  - Xiong, K.
AU  - Lin, J.
DA  - //
N1  - Cited By (since 2021): 6
PY  - 2021
ST  - Multi-task dense retrieval via model uncertainty fusion for open-domain question answering
T2  - … for Computational Linguistics: EMNLP …
TI  - Multi-task dense retrieval via model uncertainty fusion for open-domain question answering
UR  - https://aclanthology.org/2021.findings-emnlp.26/
https://aclanthology.org/2021.findings-emnlp.26.pdf
ID  - 1754
ER  - 

TY  - JOUR
AB  - … Paying off all medical bills? I think #bernie has truly gone off the … including pre-trained language models, eg, BERT (Devlin et al.… We fine-tune the BERT and BERTweet models on our …
AU  - Li, Y.
AU  - Sosea, T.
AU  - Sawant, A.
AU  - Nair, A. J.
DA  - //
N1  - Cited By (since 2021): 25
PY  - 2021
ST  - P-stance: A large dataset for stance detection in political domain
T2  - … Linguistics: ACL …
TI  - P-stance: A large dataset for stance detection in political domain
UR  - https://aclanthology.org/2021.findings-acl.208.pdf
ID  - 1656
ER  - 

TY  - JOUR
AB  - … Experimental Settings In order to compare MIL methods fairly, we set the base model f() to be BERT-base for all experiments. The learning rate of all methods is set to 1e-5 and the …
AU  - Liu, J.
AU  - Kong, D.
AU  - Huang, L.
AU  - Mao, D.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Multiple Instance Learning for Offensive Language Detection
T2  - … Linguistics: EMNLP 2022
TI  - Multiple Instance Learning for Offensive Language Detection
UR  - https://aclanthology.org/2022.findings-emnlp.546/
https://aclanthology.org/2022.findings-emnlp.546.pdf
ID  - 1715
ER  - 

TY  - JOUR
AB  - … randomly selected from the SST2 dataset with the BERT backbone. As shown in Figure 4, … in other applications like medical diagnosis and extend our idea to other data domains …
AU  - Liu, J.
AU  - Lin, Y.
AU  - Jiang, L.
AU  - Liu, J.
AU  - Wen, Z.
DA  - //
PY  - 2022
ST  - Improve Interpretability of Neural Networks via Sparse Contrastive Coding
T2  - … Linguistics: EMNLP 2022
TI  - Improve Interpretability of Neural Networks via Sparse Contrastive Coding
UR  - https://aclanthology.org/2022.findings-emnlp.32/
https://aclanthology.org/2022.findings-emnlp.32.pdf
ID  - 1649
ER  - 

TY  - JOUR
AB  - … Therefore, in the present architecture we assume that the internal parameters of BERT are not frozen. Furthermore, in BERT-PsyAM, we have replaced the original BERT Layer for fine-…
AU  - Liu, X.
AU  - Jaidka, K.
DA  - //
PY  - 2023
ST  - I am PsyAM: Modeling Happiness with Cognitive Appraisal Dimensions
T2  - … Association for Computational Linguistics: ACL …
TI  - I am PsyAM: Modeling Happiness with Cognitive Appraisal Dimensions
UR  - https://aclanthology.org/2023.findings-acl.77/
https://aclanthology.org/2023.findings-acl.77.pdf
ID  - 1622
ER  - 

TY  - JOUR
AB  - … Among these baselines, only HiAGM and HTCInfoMax do not adopt the BERT encoder. For fair comparison, we implement them with BERT encoder, and denote them as HiAGM+BERT …
AU  - Liu, Y.
AU  - Zhang, K.
AU  - Huang, Z.
AU  - Wang, K.
DA  - //
PY  - 2023
ST  - Enhancing Hierarchical Text Classification through Knowledge Graph Integration
T2  - … Linguistics: ACL …
TI  - Enhancing Hierarchical Text Classification through Knowledge Graph Integration
UR  - https://aclanthology.org/2023.findings-acl.358/
https://aclanthology.org/2023.findings-acl.358.pdf
ID  - 1691
ER  - 

TY  - JOUR
AB  - … BERT embeddings and by concatenating f(w;c) twice, we can obtain a 2048dimensional BERT-… Such resources might not be available in specialised domains such as medical or legal …
AU  - Luo, H.
AU  - Zhou, Y.
AU  - Bollegala, D.
DA  - //
PY  - 2023
ST  - Together We Make Sense–Learning Meta-Sense Embeddings
T2  - … for Computational Linguistics: ACL …
TI  - Together We Make Sense–Learning Meta-Sense Embeddings
UR  - https://aclanthology.org/2023.findings-acl.165/
https://aclanthology.org/2023.findings-acl.165.pdf
ID  - 1664
ER  - 

TY  - JOUR
AB  - … We can observe that both the NNShot and BERT show less separable representations after coarse training due to disregard for the structure of sub-classes. By training with Cluster-…
AU  - Ma, R.
AU  - Lin, Z.
AU  - Chen, X.
AU  - Zhou, X.
AU  - Wang, J.
DA  - //
PY  - 2023
ST  - Coarse-to-fine Few-shot Learning for Named Entity Recognition
T2  - … Linguistics: ACL …
TI  - Coarse-to-fine Few-shot Learning for Named Entity Recognition
UR  - https://aclanthology.org/2023.findings-acl.253/
https://aclanthology.org/2023.findings-acl.253.pdf
ID  - 1714
ER  - 

TY  - JOUR
AB  - … samples may be impractical for domains such as medical or legal, considering the cost and … We evaluate the methods by fine-tuning the pretrained BERT in active iterations on seven …
AU  - Maekawa, S.
AU  - Zhang, D.
AU  - Kim, H.
AU  - Rahman, S.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Low-resource interactive active labeling for fine-tuning language models
T2  - … Linguistics: EMNLP …
TI  - Low-resource interactive active labeling for fine-tuning language models
UR  - https://aclanthology.org/2022.findings-emnlp.235/
https://aclanthology.org/2022.findings-emnlp.235.pdf
ID  - 1654
ER  - 

TY  - JOUR
AB  - … The advanced medical care brings with it more benefits than disadvantages. [The main advantage of high tech medical care is that people are better taken care so that they have a good …
AU  - Marro, S.
AU  - Cabrio, E.
AU  - Villata, S.
DA  - //
N1  - Cited By (since 2022): 3
PY  - 2022
ST  - Graph embeddings for argumentation quality assessment
T2  - EMNLP 2022-Conference on Empirical …
TI  - Graph embeddings for argumentation quality assessment
UR  - https://hal.science/hal-03934474/
https://hal.science/hal-03934474/file/Quality_EMNLP%20%282%29.pdf
ID  - 1643
ER  - 

TY  - JOUR
AB  - … Following this idea, here we fine-tune a BERT-base model … those from the pre-trained BERT-base model, and continue … —an absolute improvement from the BERT-base model of 24.5% …
AU  - Martinez, V.
AU  - Somandepalli, K.
DA  - //
N1  - Cited By (since 2020): 7
PY  - 2020
ST  - Joint estimation and analysis of risk behavior ratings in movie scripts
T2  - … Processing (EMNLP)
TI  - Joint estimation and analysis of risk behavior ratings in movie scripts
UR  - https://aclanthology.org/2020.emnlp-main.387/
https://aclanthology.org/2020.emnlp-main.387.pdf
ID  - 1686
ER  - 

TY  - JOUR
AB  - … • We compare several BERT variants and the size of unlabeled to examine the effectiveness … , compared to BERT. Table 3 also shows that Longformer4 performs better than BERT on Γ…
AU  - Muangkammuen, P.
AU  - Fukumoto, F.
AU  - Li, J.
DA  - //
PY  - 2022
ST  - Exploiting Labeled and Unlabeled Data via Transformer Fine-tuning for Peer-Review Score Prediction
T2  - … Linguistics: EMNLP …
TI  - Exploiting Labeled and Unlabeled Data via Transformer Fine-tuning for Peer-Review Score Prediction
UR  - https://aclanthology.org/2022.findings-emnlp.164/
https://aclanthology.org/2022.findings-emnlp.164.pdf
ID  - 1674
ER  - 

TY  - JOUR
AB  - … a Victim event argument, documents recording this type of event in medical records may … We focus on applying the latter in transformer-based model (BERT) for IE tasks. In particular, …
AU  - Ngo, N.
AU  - Min, B.
AU  - Nguyen, T.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Unsupervised Domain Adaptation for Joint Information Extraction
T2  - … for Computational Linguistics: EMNLP …
TI  - Unsupervised Domain Adaptation for Joint Information Extraction
UR  - https://aclanthology.org/2022.findings-emnlp.434/
https://aclanthology.org/2022.findings-emnlp.434.pdf
ID  - 1680
ER  - 

TY  - JOUR
AB  - … ) looked into DP fine-tuning of a (publicly) pre-trained BERT model (… 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models. …
AU  - Ponomareva, N.
AU  - Bastings, J.
DA  - //
N1  - Cited By (since 2022): 5
PY  - 2022
ST  - Training text-to-text transformers with privacy guarantees
T2  - … Linguistics: ACL 2022
TI  - Training text-to-text transformers with privacy guarantees
UR  - https://aclanthology.org/2022.findings-acl.171/
https://aclanthology.org/2022.findings-acl.171.pdf
ID  - 1611
ER  - 

TY  - JOUR
AB  - Complex Word Identification (CWI) is an essential task in helping Lexical Simplification (LS) identify the difficult words that should be simplified. In this paper, we present an approach to …
AU  - Sheang, K. C.
DA  - //
N1  - Cited By (since 2019): 13
PY  - 2019
ST  - Multilingual complex word identification: Convolutional neural networks with morphological and linguistic features
T2  - … Sep 2-4; Varna, Bulgaria.[Varna]: ACL; 2019. p. 83 …
TI  - Multilingual complex word identification: Convolutional neural networks with morphological and linguistic features
UR  - https://repositori.upf.edu/handle/10230/54419
https://repositori.upf.edu/bitstream/handle/10230/54419/Sheang_ranlp_mult.pdf?sequence=1&isAllowed=y
ID  - 1732
ER  - 

TY  - JOUR
AB  - … We use the standard base-uncased BERT pre-trained model available from the Hugging Face Transformers package1, to encode the meme text into a 768-dimensional vector. …
AU  - Singh, P.
AU  - Bauwelinck, N.
AU  - Lefever, E.
DA  - //
N1  - Cited By (since 2020): 6
PY  - 2020
ST  - Lt3 at semeval-2020 task 8: multi-modal multi-task learning for memotion analysis
T2  - the Fourteenth Workshop on …
TI  - Lt3 at semeval-2020 task 8: multi-modal multi-task learning for memotion analysis
UR  - https://biblio.ugent.be/publication/8684799
https://biblio.ugent.be/publication/8684799/file/8684800
ID  - 1619
ER  - 

TY  - JOUR
AB  - … In the experiments we fine-tune the parameters on pre-trained BERT classifier using the … calibration where we simply report the results of vanilla BERT classifier on the chosen tasks. …
AU  - Singh, R.
AU  - Goshtasbpour, S.
DA  - //
PY  - 2022
ST  - Platt-Bin: Efficient Posterior Calibrated Training for NLP Classifiers
T2  - … Linguistics: ACL 2022
TI  - Platt-Bin: Efficient Posterior Calibrated Training for NLP Classifiers
UR  - https://www.research-collection.ethz.ch/handle/20.500.11850/576925
https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/576925/2022.findings-acl.290.pdf?sequence=2
ID  - 1744
ER  - 

TY  - JOUR
AB  - … BERT, therefore, the settings refer to the pretrained model bert-base-uncased (Devlin et al.… We’ll have more... @name3 @name4 @name1 following strict medical guidelines is probably …
AU  - Song, Y. Z.
AU  - Chen, Y. S.
AU  - Chang, Y. T.
AU  - Weng, S. Y.
DA  - //
N1  - Cited By (since 2021): 14
PY  - 2021
ST  - Adversary-aware rumor detection
T2  - … Linguistics: ACL …
TI  - Adversary-aware rumor detection
UR  - https://aclanthology.org/2021.findings-acl.118.pdf
ID  - 1717
ER  - 

TY  - JOUR
AB  - … with 20 examples per class and over the fully supervised BERT by 12.6%. We see consistent … Notably, our AUM-ST improves upon the baseline BERT model by 10% in F1 score using …
AU  - Sosea, T.
AU  - Caragea, C.
DA  - //
N1  - Cited By (since 2022): 1
PY  - 2022
ST  - Leveraging Training Dynamics and Self-Training for Text Classification
T2  - … for Computational Linguistics: EMNLP 2022
TI  - Leveraging Training Dynamics and Self-Training for Text Classification
UR  - https://aclanthology.org/2022.findings-emnlp.350/
https://aclanthology.org/2022.findings-emnlp.350.pdf
ID  - 1667
ER  - 

TY  - JOUR
AB  - … ple, medical chatbots can convert user queries into SQL statements and then use them to retrieve relevant information from medical … 2020a) also encodes input instances with BERT but …
AU  - Sun, S.
AU  - Gao, Y.
AU  - Zhang, Y.
AU  - Su, J.
AU  - Chen, B.
DA  - //
PY  - 2023
ST  - An Exploratory Study on Model Compression for Text-to-SQL
T2  - … Linguistics: ACL 2023
TI  - An Exploratory Study on Model Compression for Text-to-SQL
UR  - https://aclanthology.org/2023.findings-acl.740/
https://aclanthology.org/2023.findings-acl.740.pdf
ID  - 1615
ER  - 

TY  - JOUR
AB  - … skills required by questions using BERT We also investigate BERT’s performance on the … We observe BERT obtains the lowest score on the options requiring deductive reasoning. …
AU  - Tan, H.
AU  - Wang, X.
AU  - Ji, Y.
AU  - Li, R.
AU  - Li, X.
AU  - Hu, Z.
DA  - //
N1  - Cited By (since 2021): 4
PY  - 2021
ST  - GCRC: A new challenging MRC dataset from Gaokao Chinese for explainable evaluation
T2  - … Linguistics: ACL …
TI  - GCRC: A new challenging MRC dataset from Gaokao Chinese for explainable evaluation
UR  - https://aclanthology.org/2021.findings-acl.113.pdf
ID  - 1692
ER  - 

TY  - JOUR
AB  - … ] and pass them into the BERT model. We use the last layer of BERT as the encoded hidden … dataset from the medical domain collected from extensive real-world medical queries from …
AU  - Tan, M.
AU  - Yang, M.
AU  - Xu, R.
DA  - //
PY  - 2023
ST  - Focal Training and Tagger Decouple for Grammatical Error Correction
T2  - … for Computational Linguistics: ACL 2023
TI  - Focal Training and Tagger Decouple for Grammatical Error Correction
UR  - https://aclanthology.org/2023.findings-acl.370/
https://aclanthology.org/2023.findings-acl.370.pdf
ID  - 1627
ER  - 

TY  - JOUR
AB  - … Figure 3 presents the top 5 ranking CCG supertags when using our model with BERT-base encoder on the test set of REST, LPTP, and TWTR. It is observed that, “N/N”, “S\NP”, “(S\NP)/…
AU  - Tian, Y.
AU  - Chen, W.
AU  - Hu, B.
AU  - Song, Y.
DA  - //
PY  - 2023
ST  - End-to-end Aspect-based Sentiment Analysis with Combinatory Categorial Grammar
T2  - … Linguistics: ACL 2023
TI  - End-to-end Aspect-based Sentiment Analysis with Combinatory Categorial Grammar
UR  - https://aclanthology.org/2023.findings-acl.859/
https://aclanthology.org/2023.findings-acl.859.pdf
ID  - 1659
ER  - 

TY  - JOUR
AB  - … Table 4: The comparison of F1 scores between previous studies and our best model with BERT-large on the test sets of ACE05 and SemEval. Previous studies that leverage syntactic …
AU  - Tian, Y.
AU  - Song, Y.
AU  - Xia, F.
DA  - //
N1  - Cited By (since 2022): 7
PY  - 2022
ST  - Improving relation extraction through syntax-induced pre-training with dependency masking
T2  - … for Computational Linguistics: ACL 2022
TI  - Improving relation extraction through syntax-induced pre-training with dependency masking
UR  - https://aclanthology.org/2022.findings-acl.147/
https://aclanthology.org/2022.findings-acl.147.pdf
ID  - 1629
ER  - 

TY  - JOUR
AB  - … exactly that a man is hurt, he needs medical assistance, and some others nearby might help him. … Note that the BERT model is BERT-large, which is the same as that in our method for …
AU  - Tian, Z.
AU  - Zhang, Y.
AU  - Liu, K.
AU  - Zhao, J.
AU  - Jia, Y.
DA  - //
N1  - Cited By (since 2020): 13
PY  - 2020
ST  - Scene restoring for narrative machine reading comprehension
T2  - … Processing (EMNLP)
TI  - Scene restoring for narrative machine reading comprehension
UR  - https://aclanthology.org/2020.emnlp-main.247/
https://aclanthology.org/2020.emnlp-main.247.pdf
ID  - 1647
ER  - 

TY  - JOUR
AB  - … To infer speakers’ relationships from dialogues we propose PRIDE, a neural multi-label classifier, based on BERT and Transformer for creating a conversation representation. PRIDE …
AU  - Tigunova, A.
AU  - Mirza, P.
AU  - Yates, A.
AU  - Weikum, G.
DA  - //
N1  - Cited By (since 2021): 7
PY  - 2021
ST  - PRIDE: Predicting Relationships in Conversations
T2  - The Conference on …
TI  - PRIDE: Predicting Relationships in Conversations
UR  - https://pure.mpg.de/rest/items/item_3502216/component/file_3502217/content
ID  - 1606
ER  - 

TY  - JOUR
AB  - … of BERT, we split both into 256-token chunks and run BERT on … with BERT, whereas CHARM processes N inputs with BERT … We observe that medical professions such as dentist, nurse, …
AU  - Tigunova, A.
AU  - Yates, A.
AU  - Mirza, P.
DA  - //
N1  - Cited By (since 2020): 16
PY  - 2020
ST  - CHARM: Inferring personal attributes from conversations
T2  - … Processing (EMNLP)
TI  - CHARM: Inferring personal attributes from conversations
UR  - https://aclanthology.org/2020.emnlp-main.434/
https://aclanthology.org/2020.emnlp-main.434.pdf
ID  - 1651
ER  - 

TY  - JOUR
AB  - … • Question embeddings: We used the pre-trained BERT … embeddings, asking how well BERT can capture the semantics … predictive model using 90 BERT question embeddings as input …
AU  - Vu, H.
AU  - Abdurahman, S.
AU  - Bhatia, S.
DA  - //
N1  - Cited By (since 2020): 9
PY  - 2020
ST  - Predicting responses to psychological questionnaires from participants' social media posts and question text embeddings
T2  - … Linguistics: EMNLP 2020
TI  - Predicting responses to psychological questionnaires from participants' social media posts and question text embeddings
UR  - https://aclanthology.org/2020.findings-emnlp.137/
https://aclanthology.org/2020.findings-emnlp.137.pdf
ID  - 1642
ER  - 

TY  - JOUR
AB  - … models are the multilingual variants of BERT and RoBERTa, since each of them is identical to either BERT or RoBERTa except being pre-trained … BMC medical research methodology. …
AU  - Wang, C.
AU  - Gaspers, J.
AU  - Do, T. N. Q.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - Exploring cross-lingual transfer learning with unsupervised machine translation
T2  - … Linguistics: ACL-IJCNLP …
TI  - Exploring cross-lingual transfer learning with unsupervised machine translation
UR  - https://aclanthology.org/2021.findings-acl.177.pdf
ID  - 1721
ER  - 

TY  - JOUR
AB  - … BERT. For pseudo KL-divergence regulation, we use BART to estimate the likelihood of di. During training, we use AdamW as the optimizer and an initial learning rate of 2e-5 for BERT, …
AU  - Wang, H. J.
AU  - Hsieh, K. Y.
AU  - Yu, H. C.
AU  - Tsou, J. C.
DA  - //
PY  - 2023
ST  - Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation
T2  - … Linguistics: ACL …
TI  - Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation
UR  - https://aclanthology.org/2023.findings-acl.790/
https://aclanthology.org/2023.findings-acl.790.pdf
ID  - 1749
ER  - 

TY  - JOUR
AB  - … For the strategy BERT, we used the uncased BERTBase model6 pre-trained on Wikipedia, through bert-as-service7 to obtain the sentence embedding of 768 dimensions. All the …
AU  - Wang, K.
AU  - Chang, B.
AU  - Sui, Z.
DA  - //
N1  - Cited By (since 2020): 10
PY  - 2020
ST  - A spectral method for unsupervised multi-document summarization
T2  - … Natural Language Processing (EMNLP)
TI  - A spectral method for unsupervised multi-document summarization
UR  - https://aclanthology.org/2020.emnlp-main.32/
https://aclanthology.org/2020.emnlp-main.32.pdf
ID  - 1688
ER  - 

TY  - JOUR
AB  - … In our research, we utilize BERT as a feature extraction model to learn a sense embedding … In our final proposal, for each sense, we use BERT to learn its basic sense embedding from …
AU  - Wang, M.
AU  - Wang, Y.
DA  - //
N1  - Cited By (since 2020): 20
PY  - 2020
ST  - A synset relation-enhanced framework with a try-again mechanism for word sense disambiguation
T2  - … in Natural Language Processing (EMNLP)
TI  - A synset relation-enhanced framework with a try-again mechanism for word sense disambiguation
UR  - https://aclanthology.org/2020.emnlp-main.504/
https://aclanthology.org/2020.emnlp-main.504.pdf
ID  - 1570
ER  - 

TY  - JOUR
AB  - … , Tagging, Ranking and Joint method) upon BERT and its variants. Note that for fair comparison, we report all results obtained by BERT-based baseline models and compare them …
AU  - Wang, Y.
AU  - Fan, Z.
AU  - Rose, C.
DA  - //
N1  - Cited By (since 2020): 14
PY  - 2020
ST  - Incorporating multimodal information in open-domain web keyphrase extraction
T2  - … Natural Language Processing (EMNLP)
TI  - Incorporating multimodal information in open-domain web keyphrase extraction
UR  - https://aclanthology.org/2020.emnlp-main.140/
https://aclanthology.org/2020.emnlp-main.140.pdf
ID  - 1607
ER  - 

TY  - JOUR
AB  - … BERT models performed with the highest accuracy, according to our analysis of the models (see Table 7). Therefore, we decided to use three BERT … of reasoning about medical theories …
AU  - Weber, F.
AU  - Wambsganss, T.
AU  - Neshaei, S. P.
DA  - //
PY  - 2023
ST  - Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions
T2  - … Linguistics: ACL 2023
TI  - Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions
UR  - https://aclanthology.org/2023.findings-acl.145/
https://aclanthology.org/2023.findings-acl.145.pdf
ID  - 1731
ER  - 

TY  - JOUR
AB  - … Typically, this kind of data pertains to specialised domains such as medical or legal, and is … While our experiments use a state-of-theart BERT-based text classifier, RAL is not limited to …
AU  - Wertz, L.
AU  - Bogojeska, J.
AU  - Mirylenka, K.
DA  - //
PY  - 2023
ST  - Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification
T2  - … Linguistics: ACL 2023
TI  - Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification
UR  - https://aclanthology.org/2023.findings-acl.697/
https://aclanthology.org/2023.findings-acl.697.pdf
ID  - 1689
ER  - 

TY  - JOUR
AB  - … Experiments on two orthogonal inquiry conversation datasets (judicial, medical domain) demonstrate that our method generates results significantly better in automatic metrics and …
AU  - Wu, Y.
AU  - Lu, W.
AU  - Zhang, Y.
AU  - Jatowt, A.
AU  - Feng, J.
DA  - //
PY  - 2023
ST  - Focus-aware Response Generation in Inquiry Conversation
T2  - … Linguistics: ACL …
TI  - Focus-aware Response Generation in Inquiry Conversation
UR  - https://aclanthology.org/2023.findings-acl.797/
https://aclanthology.org/2023.findings-acl.797.pdf
ID  - 1638
ER  - 

TY  - JOUR
AB  - … with two pre-training language models, including BERT‡ … initialize the textual representation by BERT and set the dimen… 2021) we use BERT and unlabeled domain-specific corpus to …
AU  - Xu, J.
AU  - Zheng, C.
AU  - Cai, Y.
AU  - Chua, T. S.
DA  - //
PY  - 2023
ST  - Improving Named Entity Recognition via Bridge-based Domain Adaptation
T2  - … Computational Linguistics: ACL …
TI  - Improving Named Entity Recognition via Bridge-based Domain Adaptation
UR  - https://aclanthology.org/2023.findings-acl.238/
https://aclanthology.org/2023.findings-acl.238.pdf
ID  - 1666
ER  - 

TY  - JOUR
AB  - … Implementation Details We used the publicly released BERT model2 and fine-tuned it on our QA tasks. Considering the maximum input length BERT allows (512 tokens) and the query …
AU  - Xu, Y.
AU  - Lapata, M.
DA  - //
N1  - Cited By (since 2020): 59
PY  - 2020
ST  - Coarse-to-fine query focused multi-document summarization
T2  - … in natural language processing (EMNLP)
TI  - Coarse-to-fine query focused multi-document summarization
UR  - https://aclanthology.org/2020.emnlp-main.296/
https://aclanthology.org/2020.emnlp-main.296.pdf
ID  - 1644
ER  - 

TY  - JOUR
AB  - … Datasets We evaluate the effectiveness of our model on five benchmarked datasets: R52 and R8 Reuters dataset1 for news documents classification, Ohsumed dataset2 for medical …
AU  - Zhang, H.
AU  - Zhang, J.
DA  - //
N1  - Cited By (since 2020): 39
PY  - 2020
ST  - Text graph transformer for document classification
T2  - … methods in natural language processing (EMNLP)
TI  - Text graph transformer for document classification
UR  - https://par.nsf.gov/servlets/purl/10228180
ID  - 1712
ER  - 

TY  - JOUR
AB  - … beddings are also learned as in BERT. We apply dropout with … We train BERT as the PLM in our experiments, with the same … Medical data is from WMT 2014 medical translation task7. …
AU  - Zhang, M.
AU  - Li, L.
AU  - Liu, Q.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - Two parents, one child: Dual transfer for low-resource neural machine translation
T2  - … for Computational Linguistics: ACL-IJCNLP …
TI  - Two parents, one child: Dual transfer for low-resource neural machine translation
UR  - https://aclanthology.org/2021.findings-acl.241.pdf
ID  - 1618
ER  - 

TY  - JOUR
AB  - … BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. … ; she puts the medicine bottle in her hand on the table; she picked up the medicine bottle and …
AU  - Zhang, Q.
AU  - Yue, Z.
AU  - Hu, A.
AU  - Wang, Z.
DA  - //
PY  - 2022
ST  - MovieUN: A Dataset for Movie Understanding and Narrating
T2  - … Linguistics: EMNLP 2022
TI  - MovieUN: A Dataset for Movie Understanding and Narrating
UR  - https://aclanthology.org/2022.findings-emnlp.135/
https://aclanthology.org/2022.findings-emnlp.135.pdf
ID  - 1742
ER  - 

TY  - JOUR
AB  - … We utilize the pre-trained BERT (BERT-base, cased) where the number of transformer … Label-aware double transfer learning for cross-specialty medical named entity recognition. In …
AU  - Zhang, T.
AU  - Xia, C.
AU  - Yu, P. S.
AU  - Liu, Z.
AU  - Zhao, S.
DA  - //
N1  - Cited By (since 2021): 12
PY  - 2021
ST  - PDALN: Progressive domain adaptation over a pre-trained model for low-resource cross-domain named entity recognition
T2  - EMNLP
TI  - PDALN: Progressive domain adaptation over a pre-trained model for low-resource cross-domain named entity recognition
UR  - https://par.nsf.gov/servlets/purl/10324892
ID  - 1620
ER  - 

TY  - JOUR
AB  - … and BERT for various fractions of training data. From Figure 4, we can see that RAREBERT with pattern annotations delivers competitive or even better performance as BERT with twice …
AU  - Zhang, Z.
AU  - Yu, B.
AU  - Shu, X.
AU  - Mengge, X.
AU  - Liu, T.
DA  - //
N1  - Cited By (since 2021): 5
PY  - 2021
ST  - From what to why: Improving relation extraction with rationale graph
T2  - … Linguistics: ACL …
TI  - From what to why: Improving relation extraction with rationale graph
UR  - https://aclanthology.org/2021.findings-acl.8.pdf
ID  - 1671
ER  - 

TY  - JOUR
AB  - … ), we propose to create a phraselevel simplicity comparison model with a BERT siamese … For further specialization, take the medical domain as an example, only the new terms need …
AU  - Zhao, X.
AU  - Durmus, E.
AU  - Yeung, D. Y.
DA  - //
PY  - 2023
ST  - Towards Reference-free Text Simplification Evaluation with a BERT Siamese Network Architecture
T2  - … for Computational Linguistics: ACL …
TI  - Towards Reference-free Text Simplification Evaluation with a BERT Siamese Network Architecture
UR  - https://aclanthology.org/2023.findings-acl.838/
https://aclanthology.org/2023.findings-acl.838.pdf
ID  - 1595
ER  - 

TY  - JOUR
AB  - … We initialize the semantic encoder for arguments and debate topics with the BERT-base model with 110M parameters. We pad the input sentences with BERT start and end symbols (ie, …
AU  - Zhao, X.
AU  - Durmus, E.
AU  - Zhang, H.
DA  - //
N1  - Cited By (since 2021): 2
PY  - 2021
ST  - Leveraging Topic Relatedness for Argument Persuasion
T2  - … Linguistics: ACL-IJCNLP …
TI  - Leveraging Topic Relatedness for Argument Persuasion
UR  - https://aclanthology.org/2021.findings-acl.386.pdf
ID  - 1672
ER  - 

TY  - JOUR
AB  - … In contrast, the KG contains event pairs like “(I need your medical expertise, I need your help on something)”, “(you are the expert, I need answer)”, whose relation patterns support the …
AU  - Zhou, Y.
AU  - Geng, X.
AU  - Shen, T.
AU  - Pei, J.
AU  - Zhang, W.
DA  - //
N1  - Cited By (since 2021): 12
PY  - 2021
ST  - Modeling event-pair relations in external knowledge graphs for script reasoning
T2  - … Linguistics: ACL …
TI  - Modeling event-pair relations in external knowledge graphs for script reasoning
UR  - https://aclanthology.org/2021.findings-acl.403.pdf
ID  - 1748
ER  - 

TY  - JOUR
AB  - … 2016) has been widely used in medicine, public policy, and epidemiology for many years (… based on BERT do not perform as well as XLNet, revealing that it may be hard for BERT to …
AU  - Zhu, J.
AU  - Wu, S.
AU  - Zhang, X.
AU  - Hou, Y.
DA  - //
PY  - 2023
ST  - Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension
T2  - … Linguistics: ACL 2023
TI  - Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension
UR  - https://aclanthology.org/2023.findings-acl.812/
https://aclanthology.org/2023.findings-acl.812.pdf
ID  - 1707
ER  - 

TY  - JOUR
AB  - … 2019), we adopt DeBERTa, a variant of BERT in which each word is represented using two vectors encoding its content and … BMC medical informatics and decision making, 20(1):1–14. …
AU  - Zhu, L.
AU  - Zhao, R.
AU  - Pergola, G.
AU  - He, Y.
DA  - //
PY  - 2023
ST  - Disentangling Aspect and Stance via a Siamese Autoencoder for Aspect Clustering of Vaccination Opinions
T2  - … Computational Linguistics: ACL …
TI  - Disentangling Aspect and Stance via a Siamese Autoencoder for Aspect Clustering of Vaccination Opinions
UR  - https://aclanthology.org/2023.findings-acl.115/
https://aclanthology.org/2023.findings-acl.115.pdf
ID  - 1704
ER  - 

TY  - JOUR
AB  - … • LM score measure the fluency by computing the perplexity from a GPT-2 pre-trained on SAMSum and DialogSum. … Journal of chiropractic medicine, 15(2):155–163. …
AU  - Chen, J.
AU  - Yang, D.
DA  - //
PY  - 2023
ST  - Controllable Conversation Generation with Conversation Structures via Diffusion Models
T2  - … Association for Computational Linguistics: ACL …
TI  - Controllable Conversation Generation with Conversation Structures via Diffusion Models
UR  - https://aclanthology.org/2023.findings-acl.454/
https://aclanthology.org/2023.findings-acl.454.pdf
ID  - 1789
ER  - 

TY  - JOUR
AB  - … We generated GPT-3 answers for 60 QA recipe questions sourced from our custom dataset of blog recipes with one QA set per blog. Annotators were split into two groups: group-A …
AU  - Haduong, N.
AU  - Gao, A.
AU  - Smith, N. A.
DA  - //
PY  - 2023
ST  - Risks and NLP Design: A Case Study on Procedural Document QA
T2  - … for Computational Linguistics: ACL …
TI  - Risks and NLP Design: A Case Study on Procedural Document QA
UR  - https://aclanthology.org/2023.findings-acl.81/
https://aclanthology.org/2023.findings-acl.81.pdf
ID  - 1773
ER  - 

TY  - BOOK
AB  - … With the rise of large pre-trained language models (eg BERT, GPT: Devlin… NLP, it is worth noting that some topics (eg, legal, medical) are especially sensitive and that working with low-…
AU  - Howcroft, D. M.
AU  - Gkatzia, D.
M1  - Query date: 2023-07-14 12:22:00
PB  - napier-repository.worktribe.com
PY  - 2022
ST  - Most NLG is Low-Resource: here's what we can do about it
TI  - Most NLG is Low-Resource: here's what we can do about it
UR  - https://napier-repository.worktribe.com/output/3121302/most-nlg-is-low-resource-heres-what-we-can-do-about-it
https://napier-repository.worktribe.com/preview/3121301/2022.gem-1.29.pdf
ID  - 1790
ER  - 

TY  - JOUR
AB  - … GPT-2 model on the satirical headlines in our corpus (Radford et al., 2019; Wolf et al., 2019). We also fine-tuned the GPT-… “harriet hall of science based medicine reviewed the film in an …
AU  - Littman, Zhndml
DA  - //
PY  - 2020
ST  - Context-Driven Satirical Headline Generation
T2  - ACL 2020
TI  - Context-Driven Satirical Headline Generation
UR  - https://aclanthology.org/2020.figlang-1.pdf#page=54
ID  - 1760
ER  - 

TY  - JOUR
AB  - Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. …
AU  - Maheshwari, H.
AU  - Shekhar, S.
AU  - Saxena, A.
DA  - //
PY  - 2023
ST  - Open-World Factually Consistent Question Generation
T2  - … Linguistics: ACL 2023
TI  - Open-World Factually Consistent Question Generation
UR  - https://aclanthology.org/2023.findings-acl.151/
https://aclanthology.org/2023.findings-acl.151.pdf
ID  - 1792
ER  - 

TY  - JOUR
AB  - … a smart device for medical advice and receiving incorrect … Following from our method, we evaluate FARM on different GPT-… For the rationalization setting, we compare FARM to a GPT-3 …
AU  - Mei, A.
AU  - Levy, S.
AU  - Wang, W. Y.
DA  - //
PY  - 2023
ST  - Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI
T2  - … for Computational Linguistics: ACL …
TI  - Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI
UR  - https://aclanthology.org/2023.findings-acl.701/
https://aclanthology.org/2023.findings-acl.701.pdf
ID  - 1769
ER  - 

TY  - JOUR
AB  - … Abstract: Its natural to ask whether large language models like LaMDA or GPT-3 are intelligent agents. But I argue that this is the wrong question. Intelligence and agency are the wrong …
AU  - Rogers, A.
AU  - Boyd-Graber, J.
DA  - //
PY  - 2023
ST  - Findings of the Association for Computational Linguistics: ACL 2023
T2  - … Linguistics: ACL 2023
TI  - Findings of the Association for Computational Linguistics: ACL 2023
UR  - https://aclanthology.org/2023.findings-acl.0.pdf
ID  - 1803
ER  - 

TY  - JOUR
AB  - … A domain represents a certain type of application scenario, eg medical and geographical. It is difficult for approaches to understand domain knowledge properly (Gan et al.…
AU  - Xiang, Y.
AU  - Zhang, Q. W.
AU  - Zhang, X.
AU  - Liu, Z.
DA  - //
PY  - 2023
ST  - G3R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-: A Graph-Guided Generate-and-Rerank Framework for Complex and …
T2  - … Linguistics: ACL 2023
TI  - G3R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-: A Graph-Guided Generate-and-Rerank Framework for Complex and …
UR  - https://aclanthology.org/2023.findings-acl.23/
https://aclanthology.org/2023.findings-acl.23.pdf
ID  - 1778
ER  - 

TY  - JOUR
AB  - … Unlike other summarization tasks for news, Wikipedia, and medical treatment records, opinion summarization pays more attention to user opinions in product reviews, blog journals, and …
AU  - Zhang, Y.
AU  - Zhou, D.
DA  - //
PY  - 2023
ST  - Disentangling Text Representation With Counter-Template For Unsupervised Opinion Summarization
T2  - … Association for Computational Linguistics: ACL …
TI  - Disentangling Text Representation With Counter-Template For Unsupervised Opinion Summarization
UR  - https://aclanthology.org/2023.findings-acl.395/
https://aclanthology.org/2023.findings-acl.395.pdf
ID  - 1783
ER  - 

TY  - JOUR
AB  - … Feded: Federated learning via ensemble distillation for medical relation extraction. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP…
AU  - Zhang, Z.
AU  - Yang, Y.
AU  - Dai, Y.
AU  - Wang, Q.
AU  - Yu, Y.
DA  - //
PY  - 2023
ST  - FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models
T2  - … Linguistics: ACL 2023
TI  - FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models
UR  - https://aclanthology.org/2023.findings-acl.632/
https://aclanthology.org/2023.findings-acl.632.pdf
ID  - 1799
ER  - 

TY  - JOUR
AB  - … Simulated Chat (GPT-J) uses the same 6B GPT-J model as in our approach. … that GPT-J cannot work well without finetuning. We thus implemented this ablated variant with the 175B GPT…
AU  - Zheng, C.
AU  - Sabour, S.
AU  - Wen, J.
AU  - Zhang, Z.
DA  - //
N1  - Cited By (since 2023): 1
PY  - 2023
ST  - Augesc: Dialogue augmentation with large language models for emotional support conversation
T2  - … Linguistics: ACL 2023
TI  - Augesc: Dialogue augmentation with large language models for emotional support conversation
UR  - https://aclanthology.org/2023.findings-acl.99/
https://aclanthology.org/2023.findings-acl.99.pdf
ID  - 1768
ER  - 

TY  - JOUR
AB  - … critical real-world applications (eg, in the medical domain), this creates a demand for methods … model-specific methods (like methods for transformer interpretability), relying only on the …
AU  - Brinner, M.
AU  - Zarrieß, S.
DA  - //
PY  - 2023
ST  - Model Interpretability and Rationale Extraction by Input Mask Optimization
T2  - … for Computational Linguistics: ACL 2023
TI  - Model Interpretability and Rationale Extraction by Input Mask Optimization
UR  - https://aclanthology.org/2023.findings-acl.867/
https://aclanthology.org/2023.findings-acl.867.pdf
ID  - 1926
ER  - 

TY  - JOUR
AB  - … The transformer encoders results in two sequences of hidden state vectors q0,q1,...,qn and r0,r1,...,rm, where q0 and r0 are considered as the aggregate summary for the query and the …
AU  - Cai, D.
AU  - Wang, Y.
AU  - Bi, W.
AU  - Tu, Z.
AU  - Liu, X.
DA  - //
N1  - Cited By (since 2019): 61
PY  - 2019
ST  - Retrieval-guided dialogue response generation via a matching-to-generation framework
T2  - … Processing (EMNLP …
TI  - Retrieval-guided dialogue response generation via a matching-to-generation framework
UR  - https://aclanthology.org/D19-1195/
https://aclanthology.org/D19-1195.pdf
ID  - 1884
ER  - 

TY  - JOUR
AB  - … For one source of confusion identified for the Transformer, is that children’s answers sometimes … tence of neurotypical and neurodivergent children, as we collect no such medical data. …
AU  - Dijk, B. van
AU  - Spruit, M.
AU  - Duijn, M. van
DA  - //
PY  - 2023
ST  - Theory of Mind in Freely-Told Children's Narratives: A Classification Approach
T2  - … Linguistics: ACL 2023
TI  - Theory of Mind in Freely-Told Children's Narratives: A Classification Approach
UR  - https://aclanthology.org/2023.findings-acl.822/
https://aclanthology.org/2023.findings-acl.822.pdf
ID  - 1929
ER  - 

TY  - JOUR
AB  - … ) is a Transformerbased encoder-decoder model pre-trained with a new objective function by generating removed important sentences from the remaining sentences. Following existing …
AU  - Ge, Y.
AU  - Jeoung, S.
AU  - Dinh, L.
AU  - Diesner, J.
DA  - //
PY  - 2023
ST  - Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization
T2  - … Linguistics: ACL 2023
TI  - Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization
UR  - https://aclanthology.org/2023.findings-acl.877/
https://aclanthology.org/2023.findings-acl.877.pdf
ID  - 1957
ER  - 

TY  - JOUR
AB  - … MLM trains a deep bidirectional transformer by fusing its left/right contexts to enable learning of the contextual relations between words in a text. For this pretraining, an extensive …
AU  - Jin, N.
AU  - Lee, H.
DA  - //
PY  - 2022
ST  - StuBot: Learning by Teaching a Conversational Agent Through Machine Reading Comprehension
T2  - … Association for Computational Linguistics: EMNLP …
TI  - StuBot: Learning by Teaching a Conversational Agent Through Machine Reading Comprehension
UR  - https://aclanthology.org/2022.findings-emnlp.219/
https://aclanthology.org/2022.findings-emnlp.219.pdf
ID  - 1947
ER  - 

TY  - JOUR
AB  - … We also notice that "Medical Center" which directly connected to the root node in Fig1, weigh less than the key node "hospital". In OBJ attention output, event trigger "wounded" is given …
AU  - Liu, A.
AU  - Xu, N.
AU  - Liu, H.
DA  - //
N1  - Cited By (since 2021): 15
PY  - 2021
ST  - Self-attention graph residual convolutional networks for event detection with dependency relations
T2  - … for Computational Linguistics: EMNLP 2021
TI  - Self-attention graph residual convolutional networks for event detection with dependency relations
UR  - https://aclanthology.org/2021.findings-emnlp.28/
https://aclanthology.org/2021.findings-emnlp.28.pdf
ID  - 1939
ER  - 

TY  - JOUR
AB  - … The models were thus trained with a straightforward implementation of the Transformer. To … The language contains terminology specific to the medical and public health communities, …
AU  - McNamee, P.
AU  - Duh, K.
DA  - //
PY  - 2023
ST  - An Extensive Exploration of Back-Translation in 60 Languages
T2  - … for Computational Linguistics: ACL 2023
TI  - An Extensive Exploration of Back-Translation in 60 Languages
UR  - https://aclanthology.org/2023.findings-acl.518/
https://aclanthology.org/2023.findings-acl.518.pdf
ID  - 2004
ER  - 

TY  - JOUR
AB  - … , 2017), we additionally trained a transformer model on the data provided by the authors, … ), we can observe that the transformer model trained using v3 of the Sockeye tool outperforms …
AU  - Popović, M.
AU  - Arvan, M.
AU  - Parde, N.
DA  - //
PY  - 2023
ST  - Exploring Variation of Results from Different Experimental Conditions
T2  - … Linguistics: ACL 2023
TI  - Exploring Variation of Results from Different Experimental Conditions
UR  - https://aclanthology.org/2023.findings-acl.172/
https://aclanthology.org/2023.findings-acl.172.pdf
ID  - 1905
ER  - 

TY  - JOUR
AB  - … her friends are at home death, medical, care the farmers that diagnosed with alcohol death can suspend medical care police, central, bank … Transformer …
AU  - Sha, L.
DA  - //
N1  - Cited By (since 2020): 29
PY  - 2020
ST  - Gradient-guided unsupervised lexically constrained text generation
T2  - … Methods in Natural Language Processing (EMNLP)
TI  - Gradient-guided unsupervised lexically constrained text generation
UR  - https://aclanthology.org/2020.emnlp-main.701/
https://aclanthology.org/2020.emnlp-main.701.pdf
ID  - 1903
ER  - 

TY  - JOUR
AB  - … In this work, we use a transformer encoder with self-attention layers as an observation … caused muscle fracture , thanks to the handling of the local event medical team , he can still …
AU  - Srinivasan, S.
AU  - Dyer, C.
DA  - //
N1  - Cited By (since 2021): 1
PY  - 2021
ST  - Better Chinese Sentence Segmentation with Reinforcement Learning
T2  - … for Computational Linguistics: ACL …
TI  - Better Chinese Sentence Segmentation with Reinforcement Learning
UR  - https://aclanthology.org/2021.findings-acl.25.pdf
ID  - 1894
ER  - 

TY  - JOUR
AB  - … Our model does not use sensitive contexts such as legal or medical data. In addition, the dataset and common sense sources used in our experiments do not contain sensitive …
AU  - Tanaka, T.
AU  - Kimura, D.
AU  - Tatsubori, M.
DA  - //
PY  - 2022
ST  - DiffG-RL: Leveraging Difference between Environment State and Common Sense
T2  - … Linguistics: EMNLP 2022
TI  - DiffG-RL: Leveraging Difference between Environment State and Common Sense
UR  - https://aclanthology.org/2022.findings-emnlp.110/
https://aclanthology.org/2022.findings-emnlp.110.pdf
ID  - 2003
ER  - 

TY  - JOUR
AB  - … The adaptive hidden state for domain k ∈ dj in transformer layer l is computed using eq. (1) (line 8). The fusion module in transformer layer l parametrised by ψ(l) combines the adapter …
AU  - Vu, T.
AU  - Khadivi, S.
AU  - Phung, D.
AU  - Haffari, G.
DA  - //
N1  - Cited By (since 2022): 4
PY  - 2022
ST  - Domain generalisation of NMT: Fusing adapters with leave-one-domain-out training
T2  - … Linguistics: ACL 2022
TI  - Domain generalisation of NMT: Fusing adapters with leave-one-domain-out training
UR  - https://aclanthology.org/2022.findings-acl.49/
https://aclanthology.org/2022.findings-acl.49.pdf
ID  - 1862
ER  - 

TY  - JOUR
AB  - … 2020), a generative transformer based model, to directly generate the answer given the … For example, if a user is a medical doctor but did not discuss any medical information on Twitter, …
AU  - Wen, H.
AU  - Xiao, Z.
AU  - Hovy, E.
DA  - //
PY  - 2023
ST  - Towards Open-Domain Twitter User Profile Inference
T2  - … Linguistics: ACL 2023
TI  - Towards Open-Domain Twitter User Profile Inference
UR  - https://aclanthology.org/2023.findings-acl.198/
https://aclanthology.org/2023.findings-acl.198.pdf
ID  - 1950
ER  - 

TY  - JOUR
AB  - This paper describes Slav-NER: the 4th Multilingual Named Entity Challenge in Slavic languages. The tasks involve recognizing mentions of named entities in Web documents, …
AU  - Yangarber, R.
AU  - Piskorski, J.
AU  - Dmitrieva, A.
DA  - //
PY  - 2023
ST  - Slav-NER: the 4th Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic languages
T2  - Proceedings of the …
TI  - Slav-NER: the 4th Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic languages
UR  - https://helda.helsinki.fi/handle/10138/359615
https://helda.helsinki.fi/bitstream/handle/10138/359615/2023.bsnlp_1.21.pdf?sequence=1
ID  - 1952
ER  - 

TY  - JOUR
AB  - … We then use three transformer-based seq2seq models trained using a large out-ofdomain monolingual Japanese corpus to complement postpositional particles and estimate …
AU  - Yano, K.
AU  - Utsumi, A.
DA  - //
PY  - 2021
ST  - Pipeline Signed Japanese Translation Focusing on a Post-positional Particle Complement and Conjugation in a Low-resource Setting
T2  - … Association for Computational Linguistics: ACL …
TI  - Pipeline Signed Japanese Translation Focusing on a Post-positional Particle Complement and Conjugation in a Low-resource Setting
UR  - https://aclanthology.org/2021.findings-acl.178.pdf
ID  - 1873
ER  - 

TY  - JOUR
AB  - … We utilize an encoder-decoder transformer-based model to learn a mapping from user location profile strings to structured place strings. Given the multilingual nature of our dataset, we …
AU  - Zhang, J.
AU  - DeLucia, A.
AU  - Zhang, C.
DA  - //
PY  - 2023
ST  - Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning
T2  - … Linguistics: ACL 2023
TI  - Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning
UR  - https://aclanthology.org/2023.findings-acl.294/
https://aclanthology.org/2023.findings-acl.294.pdf
ID  - 1961
ER  - 

TY  - JOUR
AB  - … For example, a virtual doctor who knows how to answer the patient’s question on a complex medical … Researchers build transformer-based multi-modal fusion models and perform self-…
AU  - Zhou, M.
AU  - Fung, Y.
AU  - Chen, L.
AU  - Thomas, C.
AU  - Ji, H.
DA  - //
PY  - 2023
ST  - Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs
T2  - … Linguistics: ACL 2023
TI  - Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs
UR  - https://aclanthology.org/2023.findings-acl.85/
https://aclanthology.org/2023.findings-acl.85.pdf
ID  - 1964
ER  - 

TY  - JOUR
AB  - … Such a feature is especially desirable when handling sensitive data that involve eg personal preference, financial transactions, medical records, etc. An example of successful …
AU  - Zhu, X.
AU  - Wang, J.
AU  - Hong, Z.
AU  - Xiao, J.
DA  - //
N1  - Cited By (since 2020): 41
PY  - 2020
ST  - Empirical studies of institutional federated learning for natural language processing
T2  - … Linguistics: EMNLP 2020
TI  - Empirical studies of institutional federated learning for natural language processing
UR  - https://aclanthology.org/2020.findings-emnlp.55/
https://aclanthology.org/2020.findings-emnlp.55.pdf
ID  - 1965
ER  - 

