,author,doi,Link to paper,title,journal,year,Extracted by,Included,Reason for exclusion,Target application,Domain of automated approach,Target database,Data type,Data filter applied,Hosted Application for End-users,Alternative Approach,LLM Model,Models/Resources used with LLM,System Architecture/ Flow,Programming language,Library/Framework,Library/Framework Harmonized,Reported performance metrics,Reported performance metrics Harmonized,Source code availability,Source code link,Data used availability,Preprocessing applied,Pretraining corpus origin,Pretraining corpus size,Fine-tuning corpus data/task,New annotations,Annotations type,New dataset developed,Fine-tuning corpus size,Number of tasks/datasets for performance evaluation,Hardware used for training/validation/fine-tuning,Hardware type,Internal validity 1,Internal validity 2,External validity,External validity.1,Comment,Unnamed: 43,url,label,type,abstract,address,accession,Link,issue,keywords,language,issn,pages,volume,DA,DB,DP,ID,j2,m3,n1,ST,m1,c5,c2,c1,OP,publication_type,Unnamed: 40
219,"Li, D. and Xiong, Y. and Hu, B. and Tang, B. and Peng, W. and Chen, Q.",10.1186/s12911-021-01614-7,https://doi.org/10.1186/s12911-021-01614-7,Drug knowledge discovery via multi-task learning and pre-trained models,BMC Medical Informatics & Decision Making,2021.0,SDO,1.0,,Information Extraction,Pharma/ Drug Repurposing: Collecting mutation-disease knowledge from PubMed for drug repurposing.,PubMed,Abstracts,,no,,"BERT, NCBI BERT, ClinicalBERT, BioBERT",,"We start by splitting the PubMed abstract into sentences, tagging them as words, and extracting several features, such as POS tags. NER offsets and entity identification are then performed based on the BERT-based approach, and finally the relationship of each potential entity pair is predicted.",python,,,"Precision, Recall, F1-score","Precision, Recall, F1-Score",no,,yes,WordPiece tokenization,depends on BERT model,depends on BERT model,BC5CDR-disease; NCBI-disease; BC2GM; 2010 i2b2/VA; AGAC corpus,,,no,"# Train 4559; 5423; 12,573; 16,315 # Dev 4580; 922; 2518; – # Test 4796; 939; 5037; 27,626",4.0,,,yes,,yes,BiLSTM + CRF,,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=med20&AN=34789238https://uzb.swisscovery.slsp.ch/openurl/41SLSP_UZB/41SLSP_UZB:UZB?sid=OVID:medline&id=pmid:34789238&id=doi:10.1186%2Fs12911-021-01614-7&issn=1472-6947&isbn=&volume=21&issue=9&spage=251&pages=251&date=2021&title=BMC+Medical+Informatics+%26+Decision+Making&atitle=Drug+knowledge+discovery+via+multi-task+learning+and+pre-trained+models.&aulast=Li&pid=%3Cauthor%3ELi+D%3BXiong+Y%3BHu+B%3BTang+B%3BPeng+W%3BChen+Q%3C%2Fauthor%3E%3CAN%3E34789238%3C%2FAN%3E%3CDT%3EJournal+Article%3C%2FDT%3E,Li_2021_BMI.DM,JOUR,"BACKGROUND: Drug repurposing is to find new indications of approved drugs, which is essential for investigating new uses for approved or investigational drug efficiency. The active gene annotation corpus (named AGAC) is annotated by human experts, which was developed to support knowledge discovery for drug repurposing. The AGAC track of the BioNLP Open Shared Tasks using this corpus is organized by EMNLP-BioNLP 2019, where the ""Selective annotation"" attribution makes AGAC track more challenging than other traditional sequence labeling tasks. In this work, we show our methods for trigger word detection (Task 1) and its thematic role identification (Task 2) in the AGAC track. As a step forward to drug repurposing research, our work can also be applied to large-scale automatic extraction of medical text knowledge. METHODS: To meet the challenges of the two tasks, we consider Task 1 as the medical name entity recognition (NER), which cultivates molecular phenomena related to gene mutation. And we regard Task 2 as a relation extraction task, which captures the thematic roles between entities. In this work, we exploit pre-trained biomedical language representation models (e.g., BioBERT) in the information extraction pipeline for mutation-disease knowledge collection from PubMed. Moreover, we design the fine-tuning framework by using a multi-task learning technique and extra features. We further investigate different approaches to consolidate and transfer the knowledge from varying sources and illustrate the performance of our model on the AGAC corpus. Our approach is based on fine-tuned BERT, BioBERT, NCBI BERT, and ClinicalBERT using multi-task learning. Further experiments show the effectiveness of knowledge transformation and the ensemble integration of models of two tasks. We conduct a performance comparison of various algorithms. We also do an ablation study on the development set of Task 1 to examine the effectiveness of each component of our method. RESULTS: Compared with competitor methods, our model obtained the highest Precision (0.63), Recall (0.56), and F-score value (0.60) in Task 1, which ranks first place. It outperformed the baseline method provided by the organizers by 0.10 in F-score. The model shared the same encoding layers for the named entity recognition and relation extraction parts. And we obtained a second high F-score (0.25) in Task 2 with a simple but effective framework. CONCLUSIONS: Experimental results on the benchmark annotation of genes with active mutation-centric function changes corpus show that integrating pre-trained biomedical language representation models (i.e., BERT, NCBI BERT, ClinicalBERT, BioBERT) into a pipe of information extraction methods with multi-task learning can improve the ability to collect mutation-disease knowledge from PubMed.","Li, Dongfang. Harbin Institute of Technology (Shenzhen), Shenzhen, China. Xiong, Ying. Harbin Institute of Technology (Shenzhen), Shenzhen, China. Hu, Baotian. Harbin Institute of Technology (Shenzhen), Shenzhen, China. hubaotian@hit.edu.cn. Tang, Buzhou. Harbin Institute of Technology (Shenzhen), Shenzhen, China. Tang, Buzhou. Peng Cheng Laboratory, Shenzhen, China. Peng, Weihua. Baidu, International Technology (Shenzhen) Co., Ltd, Shenzhen, China. Chen, Qingcai. Harbin Institute of Technology (Shenzhen), Shenzhen, China. qingcai.chen@hit.edu.cn. Chen, Qingcai. Peng Cheng Laboratory, Shenzhen, China. qingcai.chen@hit.edu.cn.",34789238.0,,Suppl 9,Algorithms and Humans and Information Storage and Retrieval and Knowledge Discovery and *Natural Language Processing and *Pharmaceutical Preparations and 0 (Pharmaceutical Preparations),English,1472-6947,251.0,21.0,11 16,MEDLINE,Ovid Technologies,2897.0,BMC Med Inf Decis Mak,"Research Support, Non-U.S. Gov't","Li, Dongfang Xiong, Ying Hu, Baotian Tang, Buzhou Peng, Weihua Chen, Qingcai and ASReview_relevant",Drug knowledge discovery via multi-task learning and pre-trained models,,,,,,biomedical_journal,
265,"Martenot, V. and Masdeu, V. and Cupe, J. and Gehin, F. and Blanchon, M. and Dauriat, J. and Horst, A. and Renaudin, M. and Girard, P. and Zucker, J. D.",10.1186/s12911-022-02085-0,https://doi.org/10.1186/s12911-022-02085-0,LiSA: an assisted literature search pipeline for detecting serious adverse drug events with deep learning,BMC Medical Informatics & Decision Making,2022.0,SDO,1.0,,Information Retrieval,"Literature-Based Discovery/ Pharma/ Adverse-Drug-Events: automatically identifying relevant publications mentioning an established link between a Drug and a Serious Adverse Event (includes AE-DRUG relationship classification, NER, Seriousness classification)",PubMed,"Abstracts, Sentences",,"yes, but not public",,"BioBERT, BlueBERT, SciBERT, Bio+ClinicalBERT, BERT, PubMedBERT, UMLSBERT","regex-search, keyword search algorithm, rule-based system to calculate a ranking score","documents -> format into 3 tables: documents (metadata and full content), contents (split in different sections or paragraphs), menaing units -> keyword search with Aho-corasick algorithm for drugs and conditions-> 3 NLP tasks run in parallel -> filter and qualify relevant documents",python,"HuggingFace, PowerBI for visualization","HuggingFace, PowerBI for visualization","precision, recall, F1-score; comparison to keywords search","Precision, Recall, F1-Score, Comparison to Keywords Search",no,,partially (custom dataset not open),sentence tokenization,depends on BERT,depends on BERT,ADE-Corpus-V2 dataset; Custom-annotated PubMed sentences,"sentences labeled for three categories based on the health impact of an event: “serious”, “important medical event”, “none” ",,yes,20k sentences ; 7776 sentences,3.0,,,yes,,partially,only LLM models compared,they use PowerBI for visualisation!,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=medl&AN=36550485https://uzb.swisscovery.slsp.ch/openurl/41SLSP_UZB/41SLSP_UZB:UZB?sid=OVID:medline&id=pmid:36550485&id=doi:10.1186%2Fs12911-022-02085-0&issn=1472-6947&isbn=&volume=22&issue=1&spage=338&pages=338&date=2022&title=BMC+Medical+Informatics+%26+Decision+Making&atitle=LiSA%3A+an+assisted+literature+search+pipeline+for+detecting+serious+adverse+drug+events+with+deep+learning.&aulast=Martenot&pid=%3Cauthor%3EMartenot+V%3BMasdeu+V%3BCupe+J%3BGehin+F%3BBlanchon+M%3BDauriat+J%3BHorst+A%3BRenaudin+M%3BGirard+P%3BZucker+JD%3C%2Fauthor%3E%3CAN%3E36550485%3C%2FAN%3E%3CDT%3EJournal+Article%3C%2FDT%3E,Martenot_2022_BMI.DM,JOUR,"INTRODUCTION: Detecting safety signals attributed to a drug in scientific literature is a fundamental issue in pharmacovigilance. The constant increase in the volume of publications requires the automation of this tedious task, in order to find and extract relevant articles from the pack. This task is critical, as serious Adverse Drug Reactions (ADRs) still account for a large number of hospital admissions each year. OBJECTIVES: The aim of this study is to develop an augmented intelligence methodology for automatically identifying relevant publications mentioning an established link between a Drug and a Serious Adverse Event, according to the European Medicines Agency (EMA) definition of seriousness. METHODS: The proposed pipeline, called LiSA (for Literature Search Application), is based on three independent deep learning models supporting a precise detection of safety signals in the biomedical literature. By combining a Bidirectional Encoder Representations from Transformers (BERT) algorithms and a modular architecture, the pipeline achieves a precision of 0.81 and a recall of 0.89 at sentences level in articles extracted from PubMed (either abstract or full-text). We also measured that by using LiSA, a medical reviewer increases by a factor of 2.5 the number of relevant documents it can collect and evaluate compared to a simple keyword search. In the interest of re-usability, emphasis was placed on building a modular pipeline allowing the insertion of other NLP modules to enrich the results provided by the system, and extend it to other use cases. In addition, a lightweight visualization tool was developed to analyze and monitor safety signal results. CONCLUSIONS: Overall, the generic pipeline and the visualization tool proposed in this article allows for efficient and accurate monitoring of serious adverse drug reactions from the literature and can easily be adapted to similar pharmacovigilance use cases. To facilitate reproducibility and benefit other research studies, we also shared a first benchmark dataset for Serious Adverse Drug Events detection.","Martenot, Vincent. Quinten, 8 rue Vernier, 75017, Paris, France. v.martenot@quinten-france.com. Masdeu, Valentin. Quinten, 8 rue Vernier, 75017, Paris, France. Cupe, Jean. Quinten, 8 rue Vernier, 75017, Paris, France. Gehin, Faustine. Quinten, 8 rue Vernier, 75017, Paris, France. Blanchon, Margot. Quinten, 8 rue Vernier, 75017, Paris, France. Dauriat, Julien. Quinten, 8 rue Vernier, 75017, Paris, France. Horst, Alexander. Swiss Agency for Therapeutic Products, Swissmedic, Hallerstrasse 7, 3012, Bern, Switzerland. Renaudin, Michael. Swiss Agency for Therapeutic Products, Swissmedic, Hallerstrasse 7, 3012, Bern, Switzerland. Girard, Philippe. Swiss Agency for Therapeutic Products, Swissmedic, Hallerstrasse 7, 3012, Bern, Switzerland. Zucker, Jean-Daniel. UMMISCO, Sorbonne University, IRD, Bondy, France. Jean-Daniel.zucker@ird.fr.",36550485.0,,1.0,Humans and *Deep Learning and Reproducibility of Results and Adverse Drug Reaction Reporting Systems and Algorithms and Drug-Related Side Effects and Adverse Reactions/di [Diagnosis] and Drug-Related Side Effects and Adverse Reactions/ep [Epidemiology] and *Drug-Related Side Effects and Adverse Reactions,English,1472-6947,338.0,22.0,12 22,MEDLINE,Ovid Technologies,3308.0,BMC Med Inf Decis Mak,"Research Support, Non-U.S. Gov't","Martenot, Vincent Masdeu, Valentin Cupe, Jean Gehin, Faustine Blanchon, Margot Dauriat, Julien Horst, Alexander Renaudin, Michael Girard, Philippe Zucker, Jean-Daniel and ASReview_relevant",LiSA: an assisted literature search pipeline for detecting serious adverse drug events with deep learning,,,,,,biomedical_journal,
166,"Ji, Z. and Wei, Q. and Xu, H.",,https://doi.org/,BERT-based Ranking for Biomedical Entity Normalization,AMIA Summits on Translational Science Proceedings,2020.0,SDO,1.0,,Entity Normalization/Linking,General biomedical text mining: Alleviate the variation problem for entity linking in biomedical texts.,"Clinical texts, PubMed","Clinical notes, Abstracts",,no,,"BERT, BioBERT, ClinicalBERT",BM25,Mention -> Pre-processing -> Candidate Conpcet Ranking -> Unlinkable Mention Prediction,"Java, python",CLAMP toolkit,CLAMP toolkit,accuracy,Accuracy,no,,yes,"spelling correction, abbreviation resolution, numeric synonyms resolution, tokenization, punctuation removal, stemming, lower-casing",depends on the pre-trained model,depends on the pre-trained model,ShARe/CLEF eHealth 2013 Challenge corpus; NCBI-disease; TAC-ADR 2017,,,no,see paper table with overview,,Quadro P6000 GPU,GPU,yes,,yes,"UWM, TaggerOne, CNN-based, Xu et al.’s system, D’Souza & Ng’s system",,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=pmnm5&AN=32477646https://uzb.swisscovery.slsp.ch/openurl/41SLSP_UZB/41SLSP_UZB:UZB?sid=OVID:medline&id=pmid:32477646&id=doi:&issn=2153-4063&isbn=&volume=2020&issue=&spage=269&pages=269-277&date=2020&title=AMIA+Summits+on+Translational+Science+Proceedings&atitle=BERT-based+Ranking+for+Biomedical+Entity+Normalization.&aulast=Ji&pid=%3Cauthor%3EJi+Z%3BWei+Q%3BXu+H%3C%2Fauthor%3E%3CAN%3E32477646%3C%2FAN%3E%3CDT%3EJournal+Article%3C%2FDT%3E,Ji_2020_AMSuonTrScPr,JOUR,"Developing high-performance entity normalization algorithms that can alleviate the term variation problem is of great interest to the biomedical community. Although deep learning-based methods have been successfully applied to biomedical entity normalization, they often depend on traditional context-independent word embeddings. Bidirectional Encoder Representations from Transformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for Clinical Text Mining (ClinicalBERT) were recently introduced to pre-train contextualized word representation models using bidirectional Transformers, advancing the state-of-the-art for many natural language processing tasks. In this study, we proposed an entity normalization architecture by fine-tuning the pre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive experiments to evaluate the effectiveness of the pre-trained models for biomedical entity normalization using three different types of datasets. Our experimental results show that the best fine-tuned models consistently outperformed previous methods and advanced the state-of-the-art for biomedical entity normalization, with up to 1.17% increase in accuracy.","Ji, Zongcheng. School of Biomedical Informatics, The University of Texas Health Science Center at Houston, Houston, TX, USA. Wei, Qiang. School of Biomedical Informatics, The University of Texas Health Science Center at Houston, Houston, TX, USA. Xu, Hua. School of Biomedical Informatics, The University of Texas Health Science Center at Houston, Houston, TX, USA.",32477646.0,,,,English,2153-4063,269-277,2020.0,,MEDLINE,Ovid Technologies,2280.0,AMIA Summits Transl Sci Proc,,"Ji, Zongcheng Wei, Qiang Xu, Hua and ASReview_relevant",BERT-based Ranking for Biomedical Entity Normalization,,,,,,biomedical_journal,
132,"['Liu, Z.', 'Xiong, C.', 'Dai, Z.', 'Sun, S.', 'Sun, M.']",10.18653/v1/2020.findings-emnlp.216,https://aclanthology.org/2020.findings-emnlp.216/ https://aclanthology.org/2020.findings-emnlp.216.pdf,Adapting open domain fact extraction and verification to COVID-FACT through in-domain language modeling,Association for Computational Linguistics/ Findings,2020.0,SDO,1.0,,Fact Verification,Media for Health Care: fact extraction and verification model for COVID-19 statements.,"Scientific claims, Wikipedia",Wiki Articles,COVID-19,no,,"SciBERT, RoBERTa",TF-IDF,"given claim -> retrieve top-100 abstracts with TFIDF -> get BERT embeddings based on claim, title and abstract -> rerank abstracts",python,,"HuggingFace, PyTorch",,"Precision, Recall, F1-Score",yes,https://github.com/thunlp/KernelGAT,yes,,COVID-19 Open ResearchDataset Challenge,"86K papers before 2020, which are aboutcoronaviruses but not about COVID-19, and 54Kpapers after 2020",SCI-FACT; FEVER; MS-MARCO ,,,no,"1,409 annotated claims with 5,183 scientific articles; 185,455 annotated claims with5,416,537 Wikipedia documents. ",2.0,,,yes,,yes,TF-IDF + RoBERTa (Large) and SiBERT,,,,,,"With the epidemic of COVID-19, verifying the scientifically false online information, such as fake news and maliciously fabricated statements, has become crucial. However, the lack of training data in the scientific domain limits the performance of fact verification models. This paper proposes an in-domain language modeling method for fact extraction and verification systems. We come up with SciKGAT to combine the advantages of open-domain literature search, state-of-the-art fact verification systems and in-domain medical knowledge through language modeling. Our experiments on SCIFACT, a dataset of expert-written scientific fact verification, show that SciKGAT achieves 30% absolute improvement on precision. Our analyses show that such improvement thrives from our in-domain language model by picking up more related evidence pieces and accurate fact verification. Our codes and data are released via Github.",,,,,,,,,,,,,,,,,,,,,,,nlp_venue,
265,"['Sawhney, R.', 'Neerkaje, A. T.']",10.18653/v1/2022.acl-short.70,https://scholarcommons.sc.edu/aii_fac_pub/538/ https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=1555&context=aii_fac_pub,A risk-averse mechanism for suicidality assessment on social media,Association for Computational Linguistics,2022.0,SDO,1.0,,Text Classification,Media for Health Care: suicidal risk prediction for users in social media.,Social media,Reddit,,no,,BERT,Bi-LSTM,,,,,,"Precision, Recall, F1-Score, Robustness, Fail-Safe Rejects",no,,yes,,bert,bert,Reddit posts,,,no,posts of 500 user,1.0,,,,,yes,"Contextual CNN, SDM, ContextBERT, SISMO",,,,,,,,,,,,,,,,,,,,,,,,,,,,,nlp_venue,
